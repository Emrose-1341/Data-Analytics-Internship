{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYTD Data Validation and Cleaning - All-in-One Notebook\n",
    "\n",
    "This notebook combines configuration, S3 data loading, validation, and processing logic for NYTD Outcomes data.\n",
    "\n",
    "- **Source S3 Bucket:** `bdc-public-raw/ndacan/nytd/outcomes/`\n",
    "- **Supported Datasets:** 202, 228, 266, 297\n",
    "- **Framework:** Belmont Data Collaborative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNYTD Dataset {DATASET_NUMBER} Processing Documentation\\nCohort Year: {COHORT_YEAR}\\nProcessing Date: {DATE}\\nVersion: 1.0.0\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NYTD Dataset {DATASET_NUMBER} Processing Documentation\n",
    "Cohort Year: {COHORT_YEAR}\n",
    "Processing Date: {DATE}\n",
    "Version: 1.0.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import io\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Dict, List, Optional\n",
    "# light version tag for your cleaning code\n",
    "__version__ = \"1.0.0\"\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. NYTD Dataset Configuration\n",
    "\n",
    "\n",
    "NYTD_DATASETS = {\n",
    "    '202': {'cohort_year': 2011, 'waves': [1,2,3], 'has_fips5': False},\n",
    "    '228': {'cohort_year': 2014, 'waves': [1,2,3], 'has_fips5': False},\n",
    "    '266': {'cohort_year': 2017, 'waves': [1,2,3], 'has_fips5': True},\n",
    "    '297': {'cohort_year': 2020, 'waves': [1,2,3], 'has_fips5': True},\n",
    "}\n",
    "\n",
    "# These are the actual columns in the .tab.gz files:\n",
    "# 1) Core & Outcome columns (unchanged)\n",
    "NYTD_CORE_COLUMNS = [\n",
    "    'Wave','StFCID','StFIPS','St','RecNumbr','RepDate','DOB','Sex','AmIAKN',\n",
    "    'Asian','BlkAfrAm','HawaiiPI','White','RaceUnkn','RaceDcln',\n",
    "    'HisOrgin','OutcmRpt','OutcmDte','OutcmFCS'\n",
    "]\n",
    "\n",
    "NYTD_OUTCOME_VARIABLES = [\n",
    "    'CurrFTE','CurrPTE','EmplySklls','SocSecrty','EducAid','PubFinAs',\n",
    "    'PubFoodAs','PubHousAs','OthrFinAs','HighEdCert','CurrenRoll',\n",
    "    'CnctAdult','Homeless','SubAbuse','Incarc','Children','Marriage',\n",
    "    'Medicaid','OthrHlthIn','MedicalIn','MentlHlthIn','PrescripIn'\n",
    "]\n",
    "\n",
    "# 2) Base derived (present in every cohort)\n",
    "NYTD_BASE_DERIVED = ['Baseline','Elig19','Elig21','SampleState','InSample','Responded']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. S3 Utilities & Data Loader\n",
    "\n",
    "def __init__(self, raw_bucket: str = 'bdc-public-raw', \n",
    "                 curated_bucket: str = 'bdc-public-curated'):\n",
    "        self.raw_bucket = raw_bucket\n",
    "        self.curated_bucket = curated_bucket\n",
    "        self.s3 = boto3.client('s3')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYTDDataLoader:\n",
    "    def __init__(self, raw_bucket='bdc-public-raw', curated_bucket='bdc-public-curated'):\n",
    "        self.raw_bucket = raw_bucket\n",
    "        self.curated_bucket = curated_bucket\n",
    "        self.s3 = boto3.client('s3')\n",
    "\n",
    "    def list_s3_files(self, bucket, prefix=\"\", suffix=None):\n",
    "        \"\"\"List files in S3 bucket\"\"\"\n",
    "        try:\n",
    "            paginator = self.s3.get_paginator('list_objects_v2')\n",
    "            keys = []\n",
    "            for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "                for obj in page.get('Contents', []):\n",
    "                    key = obj['Key']\n",
    "                    if not suffix or key.lower().endswith(suffix.lower()):\n",
    "                        keys.append(key)\n",
    "            print(f\"Found {len(keys)} files in s3://{bucket}/{prefix}\")\n",
    "            return keys\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing files: {e}\")\n",
    "            return []\n",
    "\n",
    "    def load_gz_from_s3(self, bucket, key):\n",
    "        try:\n",
    "            obj = self.s3.get_object(Bucket=bucket, Key=key)\n",
    "            compressed = io.BytesIO(obj['Body'].read())\n",
    "            df = pd.read_csv(compressed, sep='\\t', compression='gzip', dtype=str)\n",
    "            print(f\"Loaded {df.shape[0]} rows, {df.shape[1]} columns from {key}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {key}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_expected_columns(self, dataset_number):\n",
    "        \"\"\"Get expected columns for dataset\"\"\"\n",
    "        if dataset_number not in NYTD_DATASETS:\n",
    "            return []\n",
    "        meta = NYTD_DATASETS[dataset_number]\n",
    "        cols = NYTD_CORE_COLUMNS + NYTD_OUTCOME_VARIABLES + NYTD_BASE_DERIVED\n",
    "        yy = str(meta['cohort_year'])[-2:]\n",
    "        cols.append(f\"FY{yy}Cohort\")\n",
    "        if meta.get('has_fips5'):\n",
    "            cols += ['Race', 'RaceEthn', 'FIPS5']\n",
    "        return cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, dataset_number):\n",
    "    \"\"\"Clean the dataset - FIXED VERSION\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    print(f\"Starting cleaning for dataset {dataset_number}\")\n",
    "    print(f\"Original shape: {df_clean.shape}\")\n",
    "    \n",
    "    # 1. Handle missing values\n",
    "    print(\"Step 1: Handling missing values...\")\n",
    "    for col in df_clean.columns:\n",
    "        if df_clean[col].isna().sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        # Numeric columns\n",
    "        if col in ['Wave', 'StFIPS', 'RecNumbr']:\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "            df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "        else:\n",
    "            # Categorical - use mode or 'Unknown'\n",
    "            mode = df_clean[col].mode()\n",
    "            fill_value = mode.iloc[0] if len(mode) > 0 else \"Unknown\"\n",
    "            df_clean[col] = df_clean[col].fillna(fill_value)\n",
    "    \n",
    "    # 2. Standardize variables\n",
    "    print(\"Step 2: Standardizing variables...\")\n",
    "    if \"Sex\" in df_clean.columns:\n",
    "        df_clean[\"Sex\"] = df_clean[\"Sex\"].str.strip().str.upper()\n",
    "    \n",
    "    if \"St\" in df_clean.columns:\n",
    "        df_clean[\"St\"] = df_clean[\"St\"].str.strip().str.upper()\n",
    "    \n",
    "    # Standardize Yes/No outcome variables\n",
    "    outcome_cols = [col for col in NYTD_OUTCOME_VARIABLES if col in df_clean.columns]\n",
    "    for col in outcome_cols:\n",
    "        df_clean[col] = df_clean[col].str.strip().str.upper()\n",
    "    \n",
    "    # 3. Validate data\n",
    "    print(\"Step 3: Validating data...\")\n",
    "    meta = NYTD_DATASETS[dataset_number]\n",
    "    \n",
    "    # Filter valid waves\n",
    "    if \"Wave\" in df_clean.columns:\n",
    "        df_clean[\"Wave\"] = pd.to_numeric(df_clean[\"Wave\"], errors='coerce')\n",
    "        valid_waves = df_clean[\"Wave\"].isin(meta[\"waves\"])\n",
    "        df_clean = df_clean[valid_waves]\n",
    "        print(f\"Filtered to valid waves {meta['waves']}: {valid_waves.sum()} records\")\n",
    "    \n",
    "    # 4. Fix date formats - THE KEY FIX\n",
    "    print(\"Step 4: Fixing date formats...\")\n",
    "    \n",
    "    # Fix RepDate - it's in YYYYMM.0 format\n",
    "    if \"RepDate\" in df_clean.columns:\n",
    "        def fix_repdate(date_val):\n",
    "            if pd.isna(date_val):\n",
    "                return pd.NaT\n",
    "            try:\n",
    "                date_str = str(date_val).replace('.0', '')\n",
    "                if len(date_str) == 6:  # YYYYMM format\n",
    "                    year = date_str[:4]\n",
    "                    month = date_str[4:6]\n",
    "                    return pd.to_datetime(f\"{year}-{month}-01\")\n",
    "                return pd.NaT\n",
    "            except:\n",
    "                return pd.NaT\n",
    "        \n",
    "        df_clean[\"RepDate\"] = df_clean[\"RepDate\"].apply(fix_repdate)\n",
    "        print(f\"Fixed RepDate format: {df_clean['RepDate'].notna().sum()} valid dates\")\n",
    "    \n",
    "    # Handle other dates\n",
    "    if \"DOB\" in df_clean.columns:\n",
    "        df_clean[\"DOB\"] = pd.to_datetime(df_clean[\"DOB\"], errors=\"coerce\")\n",
    "    \n",
    "    if \"OutcmDte\" in df_clean.columns:\n",
    "        df_clean[\"OutcmDte\"] = pd.to_datetime(df_clean[\"OutcmDte\"], errors=\"coerce\")\n",
    "    \n",
    "    # Only remove records that were originally missing RepDate\n",
    "    if \"RepDate\" in df_clean.columns:\n",
    "        before_filter = len(df_clean)\n",
    "        original_repdate_missing = df[\"RepDate\"].isna()\n",
    "        df_clean = df_clean[~original_repdate_missing]\n",
    "        print(f\"Removed {before_filter - len(df_clean)} records with originally missing RepDate\")\n",
    "    \n",
    "    # Add metadata\n",
    "    df_clean[\"processed_date\"] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    print(f\"Final shape: {df_clean.shape}\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "def save_to_s3_csv(df, bucket, key):\n",
    "    \"\"\"Save DataFrame as CSV to S3\"\"\"\n",
    "    try:\n",
    "        csv_buffer = io.StringIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "        s3.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())\n",
    "        print(f\"Saved to s3://{bucket}/{key}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to S3: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"Step 3: Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "PEEKING AT DATASET 202\n",
      "========================================\n",
      "Found 5 files in s3://bdc-public-raw/ndacan/nytd/outcomes/\n",
      "Loaded 58231 rows, 48 columns from ndacan/nytd/outcomes/202/outcomes_C14.tab.gz\n",
      "Dataset shape: (58231, 48)\n",
      "Columns: ['Wave', 'StFCID', 'StFIPS', 'St', 'RecNumbr', 'RepDate', 'DOB', 'Sex', 'AmIAKN', 'Asian', 'BlkAfrAm', 'HawaiiPI', 'White', 'RaceUnkn', 'RaceDcln', 'HisOrgin', 'OutcmRpt', 'OutcmDte', 'OutcmFCS', 'CurrFTE', 'CurrPTE', 'EmplySklls', 'SocSecrty', 'EducAid', 'PubFinAs', 'PubFoodAs', 'PubHousAs', 'OthrFinAs', 'HighEdCert', 'CurrenRoll', 'CnctAdult', 'Homeless', 'SubAbuse', 'Incarc', 'Children', 'Marriage', 'Medicaid', 'OthrHlthIn', 'MedicalIn', 'MentlHlthIn', 'PrescripIn', 'Baseline', 'FY11Cohort', 'Elig19', 'Elig21', 'SampleState', 'InSample', 'Responded']\n",
      "\n",
      "First 5 rows:\n",
      "  Wave          StFCID StFIPS  St      RecNumbr   RepDate         DOB Sex  \\\n",
      "0    1  AK450290395006      2  AK  450290395006  201103.0  1993-10-15   2   \n",
      "1    1  AK450448396586      2  AK  450448396586  201103.0  1993-12-15   2   \n",
      "2    1  AK450461296715      2  AK  450461296715  201103.0  1993-10-15   2   \n",
      "3    1  AK450540097503      2  AK  450540097503  201103.0  1993-10-15   2   \n",
      "4    1  AK450652098623      2  AK  450652098623  201103.0  1994-02-15   1   \n",
      "\n",
      "  AmIAKN Asian  ... MedicalIn MentlHlthIn PrescripIn Baseline FY11Cohort  \\\n",
      "0      0     0  ...       3.0         1.0        1.0        1          1   \n",
      "1      1     0  ...       0.0         0.0        0.0        1          1   \n",
      "2      0     0  ...       1.0         1.0        1.0        1          0   \n",
      "3      0     0  ...       1.0         1.0        1.0        1          1   \n",
      "4      0     0  ...       1.0         1.0        1.0        1          1   \n",
      "\n",
      "  Elig19 Elig21 SampleState InSample Responded  \n",
      "0      1    NaN           0      NaN         1  \n",
      "1      1    NaN           0      NaN         1  \n",
      "2      0    NaN           0      NaN         1  \n",
      "3      1    NaN           0      NaN         1  \n",
      "4      1    NaN           0      NaN         1  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "\n",
      "Data types:\n",
      "Wave        object\n",
      "StFCID      object\n",
      "StFIPS      object\n",
      "St          object\n",
      "RecNumbr    object\n",
      "RepDate     object\n",
      "DOB         object\n",
      "Sex         object\n",
      "AmIAKN      object\n",
      "Asian       object\n",
      "dtype: object\n",
      "\n",
      "üìä Dataset 202 sample:\n",
      "  Wave          StFCID StFIPS  St      RecNumbr   RepDate         DOB Sex  \\\n",
      "0    1  AK450290395006      2  AK  450290395006  201103.0  1993-10-15   2   \n",
      "1    1  AK450448396586      2  AK  450448396586  201103.0  1993-12-15   2   \n",
      "2    1  AK450461296715      2  AK  450461296715  201103.0  1993-10-15   2   \n",
      "\n",
      "  AmIAKN Asian  ... MedicalIn MentlHlthIn PrescripIn Baseline FY11Cohort  \\\n",
      "0      0     0  ...       3.0         1.0        1.0        1          1   \n",
      "1      1     0  ...       0.0         0.0        0.0        1          1   \n",
      "2      0     0  ...       1.0         1.0        1.0        1          0   \n",
      "\n",
      "  Elig19 Elig21 SampleState InSample Responded  \n",
      "0      1    NaN           0      NaN         1  \n",
      "1      1    NaN           0      NaN         1  \n",
      "2      0    NaN           0      NaN         1  \n",
      "\n",
      "[3 rows x 48 columns]\n",
      "\n",
      "========================================\n",
      "PEEKING AT DATASET 228\n",
      "========================================\n",
      "Found 5 files in s3://bdc-public-raw/ndacan/nytd/outcomes/\n",
      "Loaded 52199 rows, 49 columns from ndacan/nytd/outcomes/228/outcomes_C14.tab.gz\n",
      "Dataset shape: (52199, 49)\n",
      "Columns: ['Wave', 'StFCID', 'StFIPS', 'St', 'RecNumbr', 'RepDate', 'DOB', 'Sex', 'AmIAKN', 'Asian', 'BlkAfrAm', 'HawaiiPI', 'White', 'RaceUnkn', 'RaceDcln', 'HisOrgin', 'OutcmRpt', 'OutcmDte', 'OutcmFCS', 'CurrFTE', 'CurrPTE', 'EmplySklls', 'SocSecrty', 'EducAid', 'PubFinAs', 'PubFoodAs', 'PubHousAs', 'OthrFinAs', 'HighEdCert', 'CurrenRoll', 'CnctAdult', 'Homeless', 'SubAbuse', 'Incarc', 'Children', 'Marriage', 'Medicaid', 'OthrHlthIn', 'MedicalIn', 'MentlHlthIn', 'PrescripIn', 'Baseline', 'FY14Cohort', 'Elig19', 'Elig21', 'SampleState', 'InSample', 'Responded', 'ps_weight']\n",
      "\n",
      "First 5 rows:\n",
      "  Wave          StFCID StFIPS  St      RecNumbr RepDate         DOB Sex  \\\n",
      "0    1  AL000000007183      1  AL  000000007183  201403  1996-10-15   2   \n",
      "1    1  AL000000038434      1  AL  000000038434  201409  1997-05-15   1   \n",
      "2    1  AL000000046332      1  AL  000000046332  201403  1996-10-15   1   \n",
      "3    1  AL000000059581      1  AL  000000059581  201403  1996-11-15   2   \n",
      "4    1  AL000000121244      1  AL  000000121244  201409  1997-07-15   2   \n",
      "\n",
      "  AmIAKN Asian  ... MentlHlthIn PrescripIn Baseline FY14Cohort Elig19 Elig21  \\\n",
      "0      0     0  ...         1.0        1.0        1          0      0    NaN   \n",
      "1      0     0  ...         1.0        1.0        1          0      0    NaN   \n",
      "2      0     0  ...        88.0       88.0        1          1      1    1.0   \n",
      "3      0     0  ...        77.0       77.0        1          0      0    NaN   \n",
      "4      0     0  ...        77.0       77.0        1          0      0    NaN   \n",
      "\n",
      "  SampleState InSample Responded ps_weight  \n",
      "0         NaN      NaN         1       NaN  \n",
      "1         NaN      NaN         1       NaN  \n",
      "2         0.0      0.0         1       NaN  \n",
      "3         NaN      NaN         0       NaN  \n",
      "4         NaN      NaN         0       NaN  \n",
      "\n",
      "[5 rows x 49 columns]\n",
      "\n",
      "Data types:\n",
      "Wave        object\n",
      "StFCID      object\n",
      "StFIPS      object\n",
      "St          object\n",
      "RecNumbr    object\n",
      "RepDate     object\n",
      "DOB         object\n",
      "Sex         object\n",
      "AmIAKN      object\n",
      "Asian       object\n",
      "dtype: object\n",
      "\n",
      "üìä Dataset 228 sample:\n",
      "  Wave          StFCID StFIPS  St      RecNumbr RepDate         DOB Sex  \\\n",
      "0    1  AL000000007183      1  AL  000000007183  201403  1996-10-15   2   \n",
      "1    1  AL000000038434      1  AL  000000038434  201409  1997-05-15   1   \n",
      "2    1  AL000000046332      1  AL  000000046332  201403  1996-10-15   1   \n",
      "\n",
      "  AmIAKN Asian  ... MentlHlthIn PrescripIn Baseline FY14Cohort Elig19 Elig21  \\\n",
      "0      0     0  ...         1.0        1.0        1          0      0    NaN   \n",
      "1      0     0  ...         1.0        1.0        1          0      0    NaN   \n",
      "2      0     0  ...        88.0       88.0        1          1      1    1.0   \n",
      "\n",
      "  SampleState InSample Responded ps_weight  \n",
      "0         NaN      NaN         1       NaN  \n",
      "1         NaN      NaN         1       NaN  \n",
      "2         0.0      0.0         1       NaN  \n",
      "\n",
      "[3 rows x 49 columns]\n",
      "\n",
      "========================================\n",
      "PEEKING AT DATASET 266\n",
      "========================================\n",
      "Found 5 files in s3://bdc-public-raw/ndacan/nytd/outcomes/\n",
      "Loaded 57971 rows, 51 columns from ndacan/nytd/outcomes/266/outcomes_C17.tab.gz\n",
      "Dataset shape: (57971, 51)\n",
      "Columns: ['Wave', 'StFCID', 'State', 'St', 'RecNumbr', 'RepDate', 'DOB', 'Sex', 'AmIAKN', 'Asian', 'BlkAfrAm', 'HawaiiPI', 'White', 'RaceUnkn', 'RaceDcln', 'HisOrgin', 'OutcmRpt', 'OutcmDte', 'OutcmFCS', 'CurrFTE', 'CurrPTE', 'EmplySklls', 'SocSecrty', 'EducAid', 'PubFinAs', 'PubFoodAs', 'PubHousAs', 'OthrFinAs', 'HighEdCert', 'CurrenRoll', 'CnctAdult', 'Homeless', 'SubAbuse', 'Incarc', 'Children', 'Marriage', 'Medicaid', 'OthrHlthIn', 'MedicalIn', 'MentlHlthIn', 'PrescripIn', 'SampleState', 'InSample', 'Baseline', 'FY17Cohort', 'Elig19', 'Elig21', 'Responded', 'Race', 'RaceEthn', 'FIPS5']\n",
      "\n",
      "First 5 rows:\n",
      "  Wave          StFCID State  St      RecNumbr RepDate         DOB  Sex  \\\n",
      "0    1  AL000000109131     1  AL  000000109131  201709  2000-05-15  1.0   \n",
      "1    1  AL000000116114     1  AL  000000116114  201709  2000-05-15  2.0   \n",
      "2    1  AL000000116891     1  AL  000000116891  201709  2000-05-15  1.0   \n",
      "3    1  AL000000126073     1  AL  000000126073  201703  1999-10-15  1.0   \n",
      "4    1  AL000000154654     1  AL  000000154654  201703  1999-11-15  1.0   \n",
      "\n",
      "  AmIAKN Asian  ... SampleState InSample Baseline FY17Cohort Elig19 Elig21  \\\n",
      "0    0.0   0.0  ...         NaN      NaN      1.0        0.0    NaN    NaN   \n",
      "1    0.0   0.0  ...         NaN      NaN      1.0        1.0    NaN    NaN   \n",
      "2    0.0   0.0  ...         NaN      NaN      1.0        1.0    NaN    NaN   \n",
      "3    0.0   0.0  ...         NaN      NaN      1.0        1.0    NaN    NaN   \n",
      "4    0.0   0.0  ...         NaN      NaN      1.0        1.0    NaN    NaN   \n",
      "\n",
      "  Responded Race RaceEthn  FIPS5  \n",
      "0         0    2        2  00008  \n",
      "1         1    1        1  01097  \n",
      "2         1    1        1  00008  \n",
      "3         1    1        1  01097  \n",
      "4         1    2        2  01097  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "\n",
      "Data types:\n",
      "Wave        object\n",
      "StFCID      object\n",
      "State       object\n",
      "St          object\n",
      "RecNumbr    object\n",
      "RepDate     object\n",
      "DOB         object\n",
      "Sex         object\n",
      "AmIAKN      object\n",
      "Asian       object\n",
      "dtype: object\n",
      "\n",
      "üìä Dataset 266 sample:\n",
      "  Wave          StFCID State  St      RecNumbr RepDate         DOB  Sex  \\\n",
      "0    1  AL000000109131     1  AL  000000109131  201709  2000-05-15  1.0   \n",
      "1    1  AL000000116114     1  AL  000000116114  201709  2000-05-15  2.0   \n",
      "2    1  AL000000116891     1  AL  000000116891  201709  2000-05-15  1.0   \n",
      "\n",
      "  AmIAKN Asian  ... SampleState InSample Baseline FY17Cohort Elig19 Elig21  \\\n",
      "0    0.0   0.0  ...         NaN      NaN      1.0        0.0    NaN    NaN   \n",
      "1    0.0   0.0  ...         NaN      NaN      1.0        1.0    NaN    NaN   \n",
      "2    0.0   0.0  ...         NaN      NaN      1.0        1.0    NaN    NaN   \n",
      "\n",
      "  Responded Race RaceEthn  FIPS5  \n",
      "0         0    2        2  00008  \n",
      "1         1    1        1  01097  \n",
      "2         1    1        1  00008  \n",
      "\n",
      "[3 rows x 51 columns]\n",
      "\n",
      "========================================\n",
      "PEEKING AT DATASET 297\n",
      "========================================\n",
      "Found 5 files in s3://bdc-public-raw/ndacan/nytd/outcomes/\n",
      "Loaded 47189 rows, 51 columns from ndacan/nytd/outcomes/297/Outcomes20_w3.tab.gz\n",
      "Dataset shape: (47189, 51)\n",
      "Columns: ['Wave', 'StFCID', 'State', 'St', 'RecNumbr', 'RepDate', 'DOB', 'Sex', 'AmIAKN', 'Asian', 'BlkAfrAm', 'HawaiiPI', 'White', 'RaceUnkn', 'RaceDcln', 'HisOrgin', 'OutcmRpt', 'OutcmDte', 'OutcmFCS', 'CurrFTE', 'CurrPTE', 'EmplySklls', 'SocSecrty', 'EducAid', 'PubFinAs', 'PubFoodAs', 'PubHousAs', 'OthrFinAs', 'HighEdCert', 'CurrenRoll', 'CnctAdult', 'Homeless', 'SubAbuse', 'Incarc', 'Children', 'Marriage', 'Medicaid', 'OthrHlthIn', 'MedicalIn', 'MentlHlthIn', 'PrescripIn', 'SampleState', 'InSample', 'Baseline', 'FY20Cohort', 'Elig19', 'Elig21', 'Responded', 'Race', 'RaceEthn', 'FIPS5']\n",
      "\n",
      "First 5 rows:\n",
      "  Wave          StFCID State  St      RecNumbr   RepDate         DOB  Sex  \\\n",
      "0    1  AL000000033312     1  AL  000000033312  202003.0  2003-01-15  2.0   \n",
      "1    1  AL000000037999     1  AL  000000037999  202003.0  2002-12-15  1.0   \n",
      "2    1  AL000000061971     1  AL  000000061971  202009.0  2003-04-15  2.0   \n",
      "3    1  AL000000095219     1  AL  000000095219  202009.0  2003-07-15  2.0   \n",
      "4    1  AL000000096624     1  AL  000000096624  202003.0  2002-12-15  1.0   \n",
      "\n",
      "  AmIAKN Asian  ... SampleState InSample Baseline FY20Cohort Elig19 Elig21  \\\n",
      "0    0.0   0.0  ...         NaN      NaN      1.0          1    NaN    NaN   \n",
      "1    0.0   0.0  ...         NaN      NaN      1.0          1    NaN    NaN   \n",
      "2    0.0   0.0  ...         NaN      NaN      1.0          1    NaN    NaN   \n",
      "3    0.0   0.0  ...         NaN      NaN      1.0          1    NaN    NaN   \n",
      "4    0.0   0.0  ...         NaN      NaN      1.0          1    NaN    NaN   \n",
      "\n",
      "  Responded Race RaceEthn  FIPS5  \n",
      "0         1    2        2  00008  \n",
      "1         1    2        2  01097  \n",
      "2         1    1        1  00008  \n",
      "3         1    2        2  01097  \n",
      "4         1    2        2  01073  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "\n",
      "Data types:\n",
      "Wave        object\n",
      "StFCID      object\n",
      "State       object\n",
      "St          object\n",
      "RecNumbr    object\n",
      "RepDate     object\n",
      "DOB         object\n",
      "Sex         object\n",
      "AmIAKN      object\n",
      "Asian       object\n",
      "dtype: object\n",
      "\n",
      "üìä Dataset 297 sample:\n",
      "  Wave          StFCID State  St      RecNumbr   RepDate         DOB  Sex  \\\n",
      "0    1  AL000000033312     1  AL  000000033312  202003.0  2003-01-15  2.0   \n",
      "1    1  AL000000037999     1  AL  000000037999  202003.0  2002-12-15  1.0   \n",
      "2    1  AL000000061971     1  AL  000000061971  202009.0  2003-04-15  2.0   \n",
      "\n",
      "  AmIAKN Asian  ... SampleState InSample Baseline FY20Cohort Elig19 Elig21  \\\n",
      "0    0.0   0.0  ...         NaN      NaN      1.0          1    NaN    NaN   \n",
      "1    0.0   0.0  ...         NaN      NaN      1.0          1    NaN    NaN   \n",
      "2    0.0   0.0  ...         NaN      NaN      1.0          1    NaN    NaN   \n",
      "\n",
      "  Responded Race RaceEthn  FIPS5  \n",
      "0         1    2        2  00008  \n",
      "1         1    2        2  01097  \n",
      "2         1    1        1  00008  \n",
      "\n",
      "[3 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to just load and peek at data without processing\n",
    "def peek_dataset(dataset_number, raw_bucket='bdc-public-raw'):\n",
    "    \"\"\"Just load and show head() of a dataset\"\"\"\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"PEEKING AT DATASET {dataset_number}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Find the file\n",
    "    prefix = 'ndacan/nytd/outcomes/'\n",
    "    files = list_s3_files(raw_bucket, prefix, suffix='.tab.gz')\n",
    "    \n",
    "    file_patterns = {\n",
    "        '202': 'outcomes_C14.tab.gz',\n",
    "        '228': 'outcomes_C14.tab.gz', \n",
    "        '266': 'outcomes_C17.tab.gz',\n",
    "        '297': 'Outcomes20_w3.tab.gz'\n",
    "    }\n",
    "    \n",
    "    target_file = None\n",
    "    if dataset_number in file_patterns:\n",
    "        expected_pattern = file_patterns[dataset_number]\n",
    "        for file_key in files:\n",
    "            if f'/{dataset_number}/' in file_key and expected_pattern in file_key:\n",
    "                target_file = file_key\n",
    "                break\n",
    "    \n",
    "    if not target_file:\n",
    "        print(f\"No file found for dataset {dataset_number}\")\n",
    "        return None\n",
    "    \n",
    "    # Load and show data\n",
    "    df = load_gz_from_s3(raw_bucket, target_file)\n",
    "    if df is not None:\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(df.head())\n",
    "        print(\"\\nData types:\")\n",
    "        print(df.dtypes.head(10))\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "# Peek at each dataset\n",
    "for dataset_num in ['202', '228', '266', '297']:\n",
    "    df = peek_dataset(dataset_num)\n",
    "    if df is not None:\n",
    "        print(f\"\\nüìä Dataset {dataset_num} sample:\")\n",
    "        print(df.head(3))  # Show just 3 rows for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Step 4: Main processing function loaded\n",
      "\n",
      "Ready to process! Use: process_dataset('202') to start with dataset 202\n",
      "Available datasets: ['202', '228', '266', '297']\n"
     ]
    }
   ],
   "source": [
    "# Main processing function\n",
    "def process_dataset(dataset_number, raw_bucket='bdc-public-raw', curated_bucket='bdc-public-curated'):\n",
    "    \"\"\"Process a single dataset\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING DATASET {dataset_number}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Find the file\n",
    "    prefix = 'ndacan/nytd/outcomes/'\n",
    "    files = list_s3_files(raw_bucket, prefix, suffix='.tab.gz')\n",
    "    \n",
    "    # Map dataset numbers to their actual file patterns\n",
    "    file_patterns = {\n",
    "        '202': 'outcomes_C14.tab.gz',\n",
    "        '228': 'outcomes_C14.tab.gz', \n",
    "        '266': 'outcomes_C17.tab.gz',\n",
    "        '297': 'Outcomes20_w3.tab.gz'\n",
    "    }\n",
    "    \n",
    "    target_file = None\n",
    "    if dataset_number in file_patterns:\n",
    "        expected_pattern = file_patterns[dataset_number]\n",
    "        # Look for files that contain the dataset number in the path AND match the pattern\n",
    "        for file_key in files:\n",
    "            if f'/{dataset_number}/' in file_key and expected_pattern in file_key:\n",
    "                target_file = file_key\n",
    "                break\n",
    "    \n",
    "    if not target_file:\n",
    "        print(f\"No file found for dataset {dataset_number}\")\n",
    "        print(\"Available files:\")\n",
    "        for f in files:\n",
    "            if f.endswith('.tab.gz'):\n",
    "                print(f\"  {f}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found file: {target_file}\")\n",
    "    \n",
    "    # Load data\n",
    "    df = load_gz_from_s3(raw_bucket, target_file)\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    # Check expected columns\n",
    "    expected_cols = get_expected_columns(dataset_number)\n",
    "    missing_cols = [col for col in expected_cols if col not in df.columns]\n",
    "    extra_cols = [col for col in df.columns if col not in expected_cols]\n",
    "    \n",
    "    print(f\"Expected columns: {len(expected_cols)}\")\n",
    "    print(f\"Actual columns: {len(df.columns)}\")\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "    if extra_cols:\n",
    "        print(f\"Extra columns: {len(extra_cols)} (first 5: {extra_cols[:5]})\")\n",
    "    \n",
    "    # Analyze missing values\n",
    "    print(\"\\nMissing value analysis:\")\n",
    "    missing_report = analyze_missing_values(df)\n",
    "    print(missing_report.head(10))\n",
    "    \n",
    "    # Clean data\n",
    "    df_clean = clean_data(df, dataset_number)\n",
    "    \n",
    "    # Save to S3\n",
    "    output_key = f\"nytd/outcomes/cleaned/nytd_outcomes_{dataset_number}_cleaned.csv\"\n",
    "    success = save_to_s3_csv(df_clean, curated_bucket, output_key)\n",
    "    \n",
    "    # Save locally as backup\n",
    "    local_file = f\"nytd_outcomes_{dataset_number}_cleaned.csv\"\n",
    "    df_clean.to_csv(local_file, index=False)\n",
    "    print(f\"Also saved locally: {local_file}\")\n",
    "    \n",
    "    # Create summary\n",
    "    summary = {\n",
    "        \"dataset\": dataset_number,\n",
    "        \"cohort_year\": NYTD_DATASETS[dataset_number]['cohort_year'],\n",
    "        \"processed_date\": datetime.now().isoformat(),\n",
    "        \"original_shape\": df.shape,\n",
    "        \"cleaned_shape\": df_clean.shape,\n",
    "        \"missing_columns\": missing_cols,\n",
    "        \"records_removed\": df.shape[0] - df_clean.shape[0],\n",
    "        \"s3_saved\": success,\n",
    "        \"local_file\": local_file\n",
    "    }\n",
    "    \n",
    "    # Save summary\n",
    "    summary_file = f\"processing_summary_{dataset_number}.json\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nProcessing complete! Summary saved to {summary_file}\")\n",
    "    return summary\n",
    "\n",
    "print(\"‚úÖ Step 4: Main processing function loaded\")\n",
    "print(\"\\nReady to process! Use: process_dataset('202') to start with dataset 202\")\n",
    "print(\"Available datasets:\", list(NYTD_DATASETS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_process_dataset(dataset_number):\n",
    "    \"\"\"Simplified version to identify the bottleneck, now only TN rows.\"\"\"\n",
    "    print(f\"üîÑ Processing {dataset_number}...\")\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    print(\"  Step 1: Loading data...\")\n",
    "    prefix = 'ndacan/nytd/outcomes/'\n",
    "    files = list_s3_files('bdc-public-raw', prefix, suffix='.tab.gz')\n",
    "    \n",
    "    file_patterns = {\n",
    "        '202': 'outcomes_C14.tab.gz',\n",
    "        '228': 'outcomes_C14.tab.gz', \n",
    "        '266': 'outcomes_C17.tab.gz',\n",
    "        '297': 'Outcomes20_w3.tab.gz'\n",
    "    }\n",
    "    \n",
    "    target_file = None\n",
    "    if dataset_number in file_patterns:\n",
    "        expected_pattern = file_patterns[dataset_number]\n",
    "        for file_key in files:\n",
    "            if f'/{dataset_number}/' in file_key and expected_pattern in file_key:\n",
    "                target_file = file_key\n",
    "                break\n",
    "    \n",
    "    if not target_file:\n",
    "        print(\"  ‚ùå No file found\")\n",
    "        return None\n",
    "    \n",
    "    df = load_gz_from_s3('bdc-public-raw', target_file)\n",
    "    if df is None:\n",
    "        print(\"  ‚ùå Failed to load data\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  ‚úÖ Loaded {df.shape[0]} rows\")\n",
    "    \n",
    "    # Step 2: Basic cleaning + filter to TN only\n",
    "    print(\"  Step 2: Basic cleaning + filtering to TN...\")\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # <-- Here‚Äôs the filter:\n",
    "    df_clean = df_clean[df_clean[\"St\"] == \"TN\"]\n",
    "    print(f\"     ‚Ä¢ Kept only TN ‚Üí {df_clean.shape[0]} rows remain\")\n",
    "    \n",
    "    # Add processed date\n",
    "    df_clean[\"processed_date\"] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Step 3: Save locally\n",
    "    print(\"  Step 3: Saving...\")\n",
    "    local_file = f\"simple_{dataset_number}_cleaned.csv\"\n",
    "    df_clean.to_csv(local_file, index=False)\n",
    "    print(f\"  ‚úÖ Saved: {local_file}\")\n",
    "    \n",
    "    return {\n",
    "        \"dataset\": dataset_number,\n",
    "        \"shape\": df_clean.shape,\n",
    "        \"file\": local_file\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Saving all datasets as separate CSV files...\n",
      "\n",
      "üîÑ Processing Dataset 202...\n",
      "Found 5 files in s3://bdc-public-raw/ndacan/nytd/outcomes/\n",
      "Loaded 58231 rows, 48 columns from ndacan/nytd/outcomes/202/outcomes_C14.tab.gz\n",
      "‚úÖ Loaded 58,231 rows, 48 columns\n",
      "Starting cleaning for dataset 202\n",
      "Original shape: (58231, 48)\n",
      "Step 1: Handling missing values...\n",
      "Step 2: Standardizing variables...\n",
      "Step 3: Validating data...\n",
      "Filtered to valid waves [1, 2, 3]: 58231 records\n",
      "Step 4: Fixing date formats...\n",
      "Fixed RepDate format: 58231 valid dates\n",
      "Removed 360 records with originally missing RepDate\n",
      "Final shape: (57871, 49)\n",
      "‚úÖ Cleaned to 57,871 rows, 49 columns\n",
      "‚ùå Error processing dataset 202: name 'os' is not defined\n",
      "\n",
      "üîÑ Processing Dataset 228...\n",
      "Found 5 files in s3://bdc-public-raw/ndacan/nytd/outcomes/\n",
      "Loaded 52199 rows, 49 columns from ndacan/nytd/outcomes/228/outcomes_C14.tab.gz\n",
      "‚úÖ Loaded 52,199 rows, 49 columns\n",
      "Starting cleaning for dataset 228\n",
      "Original shape: (52199, 49)\n",
      "Step 1: Handling missing values...\n",
      "Step 2: Standardizing variables...\n",
      "Step 3: Validating data...\n",
      "Filtered to valid waves [1, 2, 3]: 52199 records\n",
      "Step 4: Fixing date formats...\n",
      "Fixed RepDate format: 51953 valid dates\n",
      "Removed 0 records with originally missing RepDate\n",
      "Final shape: (52199, 50)\n",
      "‚úÖ Cleaned to 52,199 rows, 50 columns\n",
      "‚ùå Error processing dataset 228: name 'os' is not defined\n",
      "\n",
      "üîÑ Processing Dataset 266...\n",
      "Found 5 files in s3://bdc-public-raw/ndacan/nytd/outcomes/\n",
      "Loaded 57971 rows, 51 columns from ndacan/nytd/outcomes/266/outcomes_C17.tab.gz\n",
      "‚úÖ Loaded 57,971 rows, 51 columns\n",
      "Starting cleaning for dataset 266\n",
      "Original shape: (57971, 51)\n",
      "Step 1: Handling missing values...\n",
      "Step 2: Standardizing variables...\n",
      "Step 3: Validating data...\n",
      "Filtered to valid waves [1, 2, 3]: 57971 records\n",
      "Step 4: Fixing date formats...\n",
      "Fixed RepDate format: 56076 valid dates\n",
      "Removed 36 records with originally missing RepDate\n",
      "Final shape: (57935, 52)\n",
      "‚úÖ Cleaned to 57,935 rows, 52 columns\n",
      "‚ùå Error processing dataset 266: name 'os' is not defined\n",
      "\n",
      "üîÑ Processing Dataset 297...\n",
      "Found 5 files in s3://bdc-public-raw/ndacan/nytd/outcomes/\n",
      "Loaded 47189 rows, 51 columns from ndacan/nytd/outcomes/297/Outcomes20_w3.tab.gz\n",
      "‚úÖ Loaded 47,189 rows, 51 columns\n",
      "Starting cleaning for dataset 297\n",
      "Original shape: (47189, 51)\n",
      "Step 1: Handling missing values...\n",
      "Step 2: Standardizing variables...\n",
      "Step 3: Validating data...\n",
      "Filtered to valid waves [1, 2, 3]: 47189 records\n",
      "Step 4: Fixing date formats...\n",
      "Fixed RepDate format: 47189 valid dates\n",
      "Removed 434 records with originally missing RepDate\n",
      "Final shape: (46755, 52)\n",
      "‚úÖ Cleaned to 46,755 rows, 52 columns\n",
      "‚ùå Error processing dataset 297: name 'os' is not defined\n",
      "\n",
      "============================================================\n",
      "SAVED FILES SUMMARY\n",
      "============================================================\n",
      "‚ùå No files were saved successfully\n",
      "\n",
      "==============================\n",
      "FILE VERIFICATION\n",
      "==============================\n",
      "Found 4 CSV files:\n",
      "  ‚úÖ NYTD_Dataset_202_Cleaned.csv (12,756,931 bytes)\n",
      "  ‚úÖ NYTD_Dataset_228_Cleaned.csv (12,066,674 bytes)\n",
      "  ‚úÖ NYTD_Dataset_266_Cleaned.csv (15,299,007 bytes)\n",
      "  ‚úÖ NYTD_Dataset_297_Cleaned.csv (12,233,551 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Force save each dataset as separate CSV files\n",
    "def save_all_datasets_separately():\n",
    "    \"\"\"Load and save each dataset as a separate CSV file\"\"\"\n",
    "    \n",
    "    file_patterns = {\n",
    "        '202': 'outcomes_C14.tab.gz',\n",
    "        '228': 'outcomes_C14.tab.gz', \n",
    "        '266': 'outcomes_C17.tab.gz',\n",
    "        '297': 'Outcomes20_w3.tab.gz'\n",
    "    }\n",
    "    \n",
    "    saved_files = []\n",
    "    \n",
    "    for dataset_num in ['202', '228', '266', '297']:\n",
    "        print(f\"\\nüîÑ Processing Dataset {dataset_num}...\")\n",
    "        \n",
    "        try:\n",
    "            # Find and load the file\n",
    "            prefix = 'ndacan/nytd/outcomes/'\n",
    "            files = list_s3_files('bdc-public-raw', prefix, suffix='.tab.gz')\n",
    "            \n",
    "            target_file = None\n",
    "            expected_pattern = file_patterns[dataset_num]\n",
    "            for file_key in files:\n",
    "                if f'/{dataset_num}/' in file_key and expected_pattern in file_key:\n",
    "                    target_file = file_key\n",
    "                    break\n",
    "            \n",
    "            if not target_file:\n",
    "                print(f\"‚ùå No file found for dataset {dataset_num}\")\n",
    "                continue\n",
    "            \n",
    "            # Load data\n",
    "            df = load_gz_from_s3('bdc-public-raw', target_file)\n",
    "            if df is None:\n",
    "                print(f\"‚ùå Failed to load dataset {dataset_num}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"‚úÖ Loaded {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "            \n",
    "            # Clean the data\n",
    "            df_clean = clean_data(df, dataset_num)\n",
    "            print(f\"‚úÖ Cleaned to {df_clean.shape[0]:,} rows, {df_clean.shape[1]} columns\")\n",
    "            \n",
    "            # Save as separate CSV files\n",
    "            csv_filename = f\"NYTD_Dataset_{dataset_num}_Cleaned.csv\"\n",
    "            df_clean.to_csv(csv_filename, index=False)\n",
    "            \n",
    "            file_size = os.path.getsize(csv_filename)\n",
    "            print(f\"‚úÖ Saved: {csv_filename} ({file_size:,} bytes)\")\n",
    "            \n",
    "            saved_files.append({\n",
    "                'dataset': dataset_num,\n",
    "                'filename': csv_filename,\n",
    "                'rows': df_clean.shape[0],\n",
    "                'columns': df_clean.shape[1],\n",
    "                'file_size_bytes': file_size\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing dataset {dataset_num}: {e}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "# Run the function to save all files\n",
    "print(\"üöÄ Saving all datasets as separate CSV files...\")\n",
    "saved_files = save_all_datasets_separately()\n",
    "\n",
    "# Show summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SAVED FILES SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if saved_files:\n",
    "    summary_df = pd.DataFrame(saved_files)\n",
    "    print(summary_df)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully saved {len(saved_files)} datasets!\")\n",
    "    print(\"\\nFiles created:\")\n",
    "    for file_info in saved_files:\n",
    "        print(f\"  üìÑ {file_info['filename']} - {file_info['rows']:,} rows\")\n",
    "else:\n",
    "    print(\"‚ùå No files were saved successfully\")\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"\\n{'='*30}\")\n",
    "print(\"FILE VERIFICATION\")\n",
    "print(f\"{'='*30}\")\n",
    "\n",
    "import os\n",
    "csv_files = [f for f in os.listdir('.') if f.startswith('NYTD_Dataset_') and f.endswith('_Cleaned.csv')]\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for f in csv_files:\n",
    "    size = os.path.getsize(f)\n",
    "    print(f\"  ‚úÖ {f} ({size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emros\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['Overall_Food_Insecurity_Rate_1.0' 'Overall_Food_Insecurity_Rate_2.0'\n",
      " 'Overall_Food_Insecurity_Rate_3.0'\n",
      " 'n_of_Food_Insecure_Persons_Overall_1.0'\n",
      " 'n_of_Food_Insecure_Persons_Overall_2.0'\n",
      " 'n_of_Food_Insecure_Persons_Overall_3.0'\n",
      " 'Food_Insecurity_Rate_among_Black_Persons_(all_ethnicities)_1.0'\n",
      " 'Food_Insecurity_Rate_among_Black_Persons_(all_ethnicities)_2.0'\n",
      " 'Food_Insecurity_Rate_among_Black_Persons_(all_ethnicities)_3.0'\n",
      " 'Food_Insecurity_Rate_among_Hispanic_Persons_(any_race)_1.0'\n",
      " 'Food_Insecurity_Rate_among_Hispanic_Persons_(any_race)_2.0'\n",
      " 'Food_Insecurity_Rate_among_Hispanic_Persons_(any_race)_3.0'\n",
      " 'Food_Insecurity_Rate_among_White,_non-Hispanic_Persons_1.0'\n",
      " 'Food_Insecurity_Rate_among_White,_non-Hispanic_Persons_2.0'\n",
      " 'Food_Insecurity_Rate_among_White,_non-Hispanic_Persons_3.0'\n",
      " 'SNAP_Threshold_1.0' 'SNAP_Threshold_2.0' 'SNAP_Threshold_3.0'\n",
      " 'pct_FI_‚â§_SNAP_Threshold_1.0' 'pct_FI_‚â§_SNAP_Threshold_2.0'\n",
      " 'pct_FI_‚â§_SNAP_Threshold_3.0' 'pct_FI_>_SNAP_Threshold_1.0'\n",
      " 'pct_FI_>_SNAP_Threshold_2.0' 'pct_FI_>_SNAP_Threshold_3.0'\n",
      " 'Child_Food_Insecurity_Rate_1.0' 'Child_Food_Insecurity_Rate_2.0'\n",
      " 'Child_Food_Insecurity_Rate_3.0' 'n_of_Food_Insecure_Children_1.0'\n",
      " 'n_of_Food_Insecure_Children_2.0' 'n_of_Food_Insecure_Children_3.0'\n",
      " 'pct_food_insecure_children_in_HH_w__HH_incomes_below_185_FPL_1.0'\n",
      " 'pct_food_insecure_children_in_HH_w__HH_incomes_below_185_FPL_2.0'\n",
      " 'pct_food_insecure_children_in_HH_w__HH_incomes_below_185_FPL_3.0'\n",
      " 'pct_food_insecure_children_in_HH_w__HH_incomes_above_185_FPL_1.0'\n",
      " 'pct_food_insecure_children_in_HH_w__HH_incomes_above_185_FPL_2.0'\n",
      " 'pct_food_insecure_children_in_HH_w__HH_incomes_above_185_FPL_3.0'\n",
      " 'Cost_Per_Meal_1.0' 'Cost_Per_Meal_2.0' 'Cost_Per_Meal_3.0'\n",
      " 'Weighted_weekly_$_needed_by_FI_1.0' 'Weighted_weekly_$_needed_by_FI_2.0'\n",
      " 'Weighted_weekly_$_needed_by_FI_3.0'\n",
      " 'Weighted_Annual_Food_Budget_Shortfall_1.0'\n",
      " 'Weighted_Annual_Food_Budget_Shortfall_2.0'\n",
      " 'Weighted_Annual_Food_Budget_Shortfall_3.0']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      5\u001b[39m pipeline = make_pipeline(\n\u001b[32m      6\u001b[39m     SimpleImputer(strategy=\u001b[33m\"\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m\"\u001b[39m),            \u001b[38;5;66;03m# fill numeric NaNs with median\u001b[39;00m\n\u001b[32m      7\u001b[39m     LogisticRegression(penalty=\u001b[33m\"\u001b[39m\u001b[33ml1\u001b[39m\u001b[33m\"\u001b[39m, solver=\u001b[33m\"\u001b[39m\u001b[33msaga\u001b[39m\u001b[33m\"\u001b[39m, max_iter=\u001b[32m10000\u001b[39m)\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Fit on training data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     14\u001b[39m y_pred = pipeline.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\pipeline.py:661\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    656\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    657\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    658\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    659\u001b[39m             all_params=params,\n\u001b[32m    660\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1327\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1325\u001b[39m classes_ = \u001b[38;5;28mself\u001b[39m.classes_\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_classes < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis solver needs samples of at least 2 classes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1329\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m in the data, but the data contains only one\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1330\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % classes_[\u001b[32m0\u001b[39m]\n\u001b[32m   1331\u001b[39m     )\n\u001b[32m   1333\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.classes_) == \u001b[32m2\u001b[39m:\n\u001b[32m   1334\u001b[39m     n_classes = \u001b[32m1\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Build a pipeline: first impute, then fit logistic regression\n",
    "pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),            # fill numeric NaNs with median\n",
    "    LogisticRegression(penalty=\"l1\", solver=\"saga\", max_iter=10000)\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Extract coefficients (after imputation, pipeline.named_steps[\"logisticregression\"].coef_)\n",
    "lr = pipeline.named_steps[\"logisticregression\"]\n",
    "coef = pd.Series(lr.coef_[0], index=X.columns)\n",
    "top = coef.abs().sort_values(ascending=False).head(10)\n",
    "print(\"Top 10 predictors of harder life in Wave 3:\")\n",
    "print(top.to_frame(\"|coef|\").join(coef.to_frame(\"coef\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 135)) while a minimum of 1 is required by SimpleImputer.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      3\u001b[39m y_train_clean = y_train.loc[X_train_clean.index]\n\u001b[32m      5\u001b[39m pipeline = make_pipeline(\n\u001b[32m      6\u001b[39m     SimpleImputer(strategy=\u001b[33m\"\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      7\u001b[39m     LogisticRegression(penalty=\u001b[33m\"\u001b[39m\u001b[33ml1\u001b[39m\u001b[33m\"\u001b[39m, solver=\u001b[33m\"\u001b[39m\u001b[33msaga\u001b[39m\u001b[33m\"\u001b[39m, max_iter=\u001b[32m10000\u001b[39m)\n\u001b[32m      8\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_clean\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\pipeline.py:653\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    647\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    648\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m     )\n\u001b[32m    652\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\pipeline.py:587\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    580\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    581\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    582\u001b[39m     step_idx=step_idx,\n\u001b[32m    583\u001b[39m     step_params=routed_params[name],\n\u001b[32m    584\u001b[39m     all_params=raw_params,\n\u001b[32m    585\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\pipeline.py:1539\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1538\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1539\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1540\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1541\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1542\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1543\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:895\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    892\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, **fit_params).transform(X)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    894\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\impute\\_base.py:436\u001b[39m, in \u001b[36mSimpleImputer.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    420\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the imputer on `X`.\u001b[39;00m\n\u001b[32m    421\u001b[39m \n\u001b[32m    422\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    434\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m     \u001b[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[39;00m\n\u001b[32m    439\u001b[39m     \u001b[38;5;66;03m# otherwise\u001b[39;00m\n\u001b[32m    440\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\impute\\_base.py:363\u001b[39m, in \u001b[36mSimpleImputer._validate_input\u001b[39m\u001b[34m(self, X, in_fit)\u001b[39m\n\u001b[32m    361\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ve\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m in_fit:\n\u001b[32m    366\u001b[39m     \u001b[38;5;66;03m# Use the dtype seen in `fit` for non-`fit` conversion\u001b[39;00m\n\u001b[32m    367\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_dtype = X.dtype\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\impute\\_base.py:344\u001b[39m, in \u001b[36mSimpleImputer._validate_input\u001b[39m\u001b[34m(self, X, in_fit)\u001b[39m\n\u001b[32m    341\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcould not convert\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:1128\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1126\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m   1127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1128\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1129\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1130\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1131\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m   1132\u001b[39m         )\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   1135\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 135)) while a minimum of 1 is required by SimpleImputer."
     ]
    }
   ],
   "source": [
    "# Option 1: Drop rows with any NaNs in X\n",
    "X_train_clean = X_train.dropna()\n",
    "y_train_clean = y_train.loc[X_train_clean.index]\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    LogisticRegression(penalty=\"l1\", solver=\"saga\", max_iter=10000)\n",
    ")\n",
    "pipeline.fit(X_train_clean, y_train_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# 10. Fit a logistic regression with L1 penalty\u001b[39;00m\n\u001b[32m     80\u001b[39m clf = LogisticRegression(penalty=\u001b[33m\"\u001b[39m\u001b[33ml1\u001b[39m\u001b[33m\"\u001b[39m, solver=\u001b[33m\"\u001b[39m\u001b[33msaga\u001b[39m\u001b[33m\"\u001b[39m, max_iter=\u001b[32m10000\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# 11. Evaluate\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, clf.predict(X_test)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1239\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     _dtype = [np.float64, np.float32]\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mliblinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaga\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1248\u001b[39m check_classification_targets(y)\n\u001b[32m   1249\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = np.unique(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2971\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2969\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:1368\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1363\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1364\u001b[39m     )\n\u001b[32m   1366\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1385\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1387\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Load (low_memory=False to silence mixed‚Äêtype warnings)\n",
    "df = pd.read_csv(\n",
    "    \"NYTD_S3_Integrated_Long_Format.csv\",\n",
    "    parse_dates=[\"RepDate\",\"DOB\",\"OutcmDte\",\"processed_date\"],\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# 2. Keep only Tennessee\n",
    "df = df[df[\"St\"] == \"TN\"]\n",
    "\n",
    "# 3. Dedupe so pivot won‚Äôt error\n",
    "df = df.drop_duplicates(subset=[\"StFCID\",\"Wave\"], keep=\"first\")\n",
    "\n",
    "# 4. Pivot to wide format\n",
    "df_feat = df.pivot(\n",
    "    index=\"StFCID\",\n",
    "    columns=\"Wave\",\n",
    "    # include all your FI + NYTD flags here\n",
    "    values=[\n",
    "        # FOOD INSECURITY METRICS\n",
    "        \"Overall Food Insecurity Rate\",\n",
    "        \"# of Food Insecure Persons Overall\",\n",
    "        \"Food Insecurity Rate among Black Persons (all ethnicities)\",\n",
    "        \"Food Insecurity Rate among Hispanic Persons (any race)\",\n",
    "        \"Food Insecurity Rate among White, non-Hispanic Persons\",\n",
    "        \"SNAP Threshold\",\n",
    "        \"% FI ‚â§ SNAP Threshold\",\n",
    "        \"% FI > SNAP Threshold\",\n",
    "        \"Child Food Insecurity Rate\",\n",
    "        \"# of Food Insecure Children\",\n",
    "        \"% food insecure children in HH w/ HH incomes below 185 FPL\",\n",
    "        \"% food insecure children in HH w/ HH incomes above 185 FPL\",\n",
    "        \"Cost Per Meal\",\n",
    "        \"Weighted weekly $ needed by FI\",\n",
    "        \"Weighted Annual Food Budget Shortfall\",\n",
    "        # NYTD FLAGS\n",
    "        \"Sex\",\n",
    "        \"AmIAKN\",\"Asian\",\"BlkAfrAm\",\"HawaiiPI\",\"White\",\"RaceUnkn\",\"RaceDcln\",\"HisOrgin\",\n",
    "        \"EmplySklls\",\"SocSecrty\",\"EducAid\",\"PubFinAs\",\"PubFoodAs\",\"PubHousAs\",\"OthrFinAs\",\n",
    "        \"HighEdCert\",\"CurrenRoll\",\"CnctAdult\",\"Homeless\",\"SubAbuse\",\"Incarc\",\"Children\",\n",
    "        \"Marriage\",\"Medicaid\",\"OthrHlthIn\",\"MedicalIn\",\"MentlHlthIn\",\"PrescripIn\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 5. Flatten column names: e.g. \"Homeless_1\", \"Overall Food Insecurity Rate_2\", etc.\n",
    "df_feat.columns = [f\"{col.replace(' ', '_').replace('%','pct').replace('#','n').replace('/','_')}_{wave}\"\n",
    "                   for col, wave in df_feat.columns]\n",
    "\n",
    "# 6a. Ensure the ‚Äú_3‚Äù columns exist, even if all zeros\n",
    "for base in [\"Homeless\", \"Incarc\", \"SubAbuse\"]:\n",
    "    col3 = f\"{base}_3\"\n",
    "    if col3 not in df_feat.columns:\n",
    "        df_feat[col3] = 0\n",
    "\n",
    "# 6b. Now build the target safely\n",
    "df_feat[\"harder3\"] = (\n",
    "    (df_feat[\"Homeless_3\"] == 1) |\n",
    "    (df_feat[\"Incarc_3\"]   == 1) |\n",
    "    (df_feat[\"SubAbuse_3\"] == 1)\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "# 7. Keep only those with complete Wave 1 & 2 data\n",
    "mask = df_feat[[c for c in df_feat.columns if c.endswith(\"_1\") or c.endswith(\"_2\")]].notnull().all(axis=1)\n",
    "df_model = df_feat[mask].dropna(subset=[\"harder3\"])\n",
    "\n",
    "# 8. Split into X & y\n",
    "X = df_model.drop(columns=[\"harder3\"])\n",
    "y = df_model[\"harder3\"]\n",
    "\n",
    "# 9. Train‚Äêtest split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# 10. Fit a logistic regression with L1 penalty\n",
    "clf = LogisticRegression(penalty=\"l1\", solver=\"saga\", max_iter=10000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 11. Evaluate\n",
    "print(classification_report(y_test, clf.predict(X_test)))\n",
    "\n",
    "# 12. Show top 10 predictors by absolute coefficient\n",
    "import numpy as np\n",
    "coef = pd.Series(clf.coef_[0], index=X.columns)\n",
    "top = coef.abs().sort_values(ascending=False).head(10)\n",
    "print(\"Top 10 predictors of harder life in Wave 3:\")\n",
    "print(top.to_frame(\"|coef|\").join(coef.to_frame(\"coef\")))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
