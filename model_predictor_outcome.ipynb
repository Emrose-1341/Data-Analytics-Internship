{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62e1079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import boto3\n",
    "import io\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import chi2_contingency, fisher_exact\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import itertools\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784200e",
   "metadata": {},
   "source": [
    "## S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265f793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────── Settings ─────────────\n",
    "BUCKET       = 'bdc-public-curated'\n",
    "PREFIX       = 'ndacan/nytd/outcomes/waves_processed/'\n",
    "\n",
    "# ───────────── Initialize S3 client & list files ─────────────\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63cd884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AFCARS_TN_processed(in).csv as a DataFrame\n",
    "afcars_tn_df = pd.read_csv('AFCARS_TN_processed(in).csv', dtype=str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498e2e4f",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed811f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_read_csv(key):\n",
    "    \"\"\"Load a CSV from S3 into a pandas DataFrame.\"\"\"\n",
    "    obj = s3.get_object(Bucket=BUCKET, Key=key)\n",
    "    return pd.read_csv(io.BytesIO(obj['Body'].read()), dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd4ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID and metadata columns\n",
    "ID_COLS   = ['StFIPS', 'RepDate',]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2dc9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_keys(df, wave_tag):\n",
    "    # 1) rename StFIPS_wX → StFIPS, etc.\n",
    "    for col in ID_COLS:\n",
    "        suff = f\"{col}_{wave_tag}\"\n",
    "        if suff in df.columns:\n",
    "            df.rename(columns={suff: col}, inplace=True)\n",
    "    # 2) trim whitespace & unify types\n",
    "    df[ID_COLS] = df[ID_COLS].astype(str).apply(lambda s: s.str.strip())\n",
    "    # 3) parse dates\n",
    "    df['RepDate'] = pd.to_datetime(df['RepDate'], errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b731d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave shapes: (3068, 38) (1362, 38) (950, 38)\n"
     ]
    }
   ],
   "source": [
    "# ───────────── 1) LOAD & NORMALIZE WAVES ─────────────\n",
    "wave1 = s3_read_csv(f\"{PREFIX}wave1.csv\")\n",
    "wave2 = s3_read_csv(f\"{PREFIX}wave2.csv\")\n",
    "wave3 = s3_read_csv(f\"{PREFIX}wave3.csv\")\n",
    "all_wave = s3_read_csv(f\"{PREFIX}cleaned_all_waves.csv\")\n",
    "wave1 = normalize_keys(wave1, \"w1\")\n",
    "wave2 = normalize_keys(wave2, \"w2\")\n",
    "wave3 = normalize_keys(wave3, \"w3\")\n",
    "\n",
    "print(\"Wave shapes:\", wave1.shape, wave2.shape, wave3.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ed16a",
   "metadata": {},
   "source": [
    "## Wave 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e10aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Drop any column suffixed with '_w2' or '_w3'\n",
    "wave1 = all_wave.drop(\n",
    "    columns=[c for c in all_wave.columns if c.endswith(('_w23'))]\n",
    ").copy()\n",
    "\n",
    "# 3) Optionally strip off the '_w1' suffix so your columns are the “base” names\n",
    "wave1.columns = [\n",
    "    re.sub(r'_w1$', '', c)\n",
    "    for c in wave1.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63cd07ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 42)\n"
     ]
    }
   ],
   "source": [
    "# print the (rows, columns) tuple\n",
    "print(wave1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ead98",
   "metadata": {},
   "source": [
    "## Wave 2-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7cdc9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Drop any column suffixed with '_w2' or '_w3'\n",
    "wave23 = all_wave.drop(\n",
    "    columns=[c for c in all_wave.columns if c.endswith(('_w1'))]\n",
    ").copy()\n",
    "\n",
    "# 3) Optionally strip off the '_w1' suffix so your columns are the “base” names\n",
    "wave23.columns = [\n",
    "    re.sub(r'_w1$', '', c)\n",
    "    for c in wave23.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "711d5234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 41)\n"
     ]
    }
   ],
   "source": [
    "# print the (rows, columns) tuple\n",
    "print(wave23.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c6c5b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors: ['CurrFTE', 'CurrPTE', 'EmplySklls', 'SocSecrty', 'EducAid', 'PubFinAs', 'PubFoodAs', 'PubHousAs', 'OthrFinAs', 'Medicaid', 'OthrHlthIn', 'MedicalIn', 'MentlHlthIn']\n",
      "Outcomes: ['Homeless', 'SubAbuse', 'Incarc', 'Children', 'Marriage', 'CnctAdult']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Define predictors and outcomes\n",
    "OUTCOMES = [\n",
    "    'Homeless', 'SubAbuse', 'Incarc',\n",
    "    'Children', 'Marriage', 'CnctAdult'\n",
    "]\n",
    "\n",
    "PREDICTORS = [\n",
    "    'CurrFTE', 'CurrPTE', 'EmplySklls', 'SocSecrty', 'EducAid',\n",
    "    'PubFinAs', 'PubFoodAs', 'PubHousAs', 'OthrFinAs',\n",
    "    'Medicaid', 'OthrHlthIn', 'MedicalIn', 'MentlHlthIn'\n",
    "]\n",
    "\n",
    "print(f\"Predictors: {PREDICTORS}\")\n",
    "print(f\"Outcomes: {OUTCOMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc4659",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13630872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave1 predictors shape: (720, 14)\n",
      "Wave23 outcomes shape: (720, 7)\n",
      "Wave23 outcome columns found: ['Homeless_w23', 'SubAbuse_w23', 'Incarc_w23', 'Children_w23', 'Marriage_w23', 'CnctAdult_w23']\n",
      "Longitudinal merged shape: (720, 20)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Create longitudinal dataset and overall analysis - FIXED\n",
    "def create_longitudinal_data(wave1_df, wave23_df):\n",
    "    \"\"\"Merge wave1 predictors with wave23 outcomes by StFCID\"\"\"\n",
    "    \n",
    "    # Get predictors from wave1 (clean names)\n",
    "    wave1_predictors_df = wave1_df[['StFCID'] + [col for col in PREDICTORS if col in wave1_df.columns]].copy()\n",
    "    \n",
    "    # Get outcomes from wave23 (with suffixes)\n",
    "    wave23_outcome_cols = ['StFCID']\n",
    "    for outcome in OUTCOMES:\n",
    "        # Add both _w2 and _w3 versions if they exist\n",
    "        wave23_outcome_cols.extend([col for col in wave23_df.columns if col.startswith(outcome + '_w')])\n",
    "    \n",
    "    wave23_outcomes_df = wave23_df[wave23_outcome_cols].copy()\n",
    "    \n",
    "    # Merge on StFCID\n",
    "    longitudinal_df = pd.merge(wave1_predictors_df, wave23_outcomes_df, on='StFCID', how='inner')\n",
    "    \n",
    "    print(f\"Wave1 predictors shape: {wave1_predictors_df.shape}\")\n",
    "    print(f\"Wave23 outcomes shape: {wave23_outcomes_df.shape}\")\n",
    "    print(f\"Wave23 outcome columns found: {[col for col in wave23_outcome_cols if col != 'StFCID']}\")\n",
    "    print(f\"Longitudinal merged shape: {longitudinal_df.shape}\")\n",
    "    \n",
    "    return longitudinal_df\n",
    "\n",
    "# Create longitudinal dataset\n",
    "longitudinal_data = create_longitudinal_data(wave1, wave23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f149594",
   "metadata": {},
   "source": [
    "# Whole Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f16bbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking StFCID column in both datasets...\n",
      "==================================================\n",
      "Wave1 shape: (720, 42)\n",
      "Wave23 shape: (720, 41)\n",
      "\n",
      "StFCID in wave1: True\n",
      "StFCID in wave23: True\n",
      "\n",
      "Unique StFCID values:\n",
      "  Wave1: 720 unique IDs\n",
      "  Wave23: 720 unique IDs\n",
      "  Matching between both waves: 720 IDs\n",
      "\n",
      "Sample matching StFCID values:\n",
      "['TNOOOONMNKJMJM', 'TNOOOOOKLONFKG', 'TNOOOOOHIJKKNM', 'TNOOOONOGGFHJK', 'TNµ®œ¿Š¤\\xad©þ±ü£', 'TNOOOOMGNKLFGG', 'TNOOOOLJOHNFML', 'TNOOOOMKLMKFJN', 'TNOOOOOKMGNOKI', 'TNOOOOMKGKFLKL']\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Check StFCID columns and basic info\n",
    "print(\"Checking StFCID column in both datasets...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Wave1 shape: {wave1.shape}\")\n",
    "print(f\"Wave23 shape: {wave23.shape}\")\n",
    "\n",
    "# Check if StFCID exists\n",
    "print(f\"\\nStFCID in wave1: {'StFCID' in wave1.columns}\")\n",
    "print(f\"StFCID in wave23: {'StFCID' in wave23.columns}\")\n",
    "\n",
    "# Show unique StFCID counts\n",
    "wave1_ids = wave1['StFCID'].dropna().unique()\n",
    "wave23_ids = wave23['StFCID'].dropna().unique()\n",
    "\n",
    "print(f\"\\nUnique StFCID values:\")\n",
    "print(f\"  Wave1: {len(wave1_ids):,} unique IDs\")\n",
    "print(f\"  Wave23: {len(wave23_ids):,} unique IDs\")\n",
    "\n",
    "# Find matching IDs\n",
    "matching_ids = set(wave1_ids).intersection(set(wave23_ids))\n",
    "print(f\"  Matching between both waves: {len(matching_ids):,} IDs\")\n",
    "\n",
    "# Sample of matching IDs\n",
    "print(f\"\\nSample matching StFCID values:\")\n",
    "print(list(matching_ids)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c88a69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating combo dataset with matching StFCIDs...\n",
      "==================================================\n",
      "Combo dataset created!\n",
      "  Original Wave1: 720 records\n",
      "  Original Wave23: 720 records\n",
      "  Combo dataset: 720 records\n",
      "  Retention rate: 100.0%\n",
      "\n",
      "Combo dataset shape: (720, 82)\n",
      "Columns: 82 total\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Create combo dataset with matching StFCIDs\n",
    "print(\"\\nCreating combo dataset with matching StFCIDs...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Simple inner join on StFCID - only keep records that exist in both datasets\n",
    "combo_dataset = pd.merge(\n",
    "    wave1,\n",
    "    wave23,\n",
    "    on='StFCID',\n",
    "    how='inner',\n",
    "    suffixes=('_w1', '_w23')\n",
    ")\n",
    "\n",
    "print(f\"Combo dataset created!\")\n",
    "print(f\"  Original Wave1: {len(wave1):,} records\")\n",
    "print(f\"  Original Wave23: {len(wave23):,} records\") \n",
    "print(f\"  Combo dataset: {len(combo_dataset):,} records\")\n",
    "print(f\"  Retention rate: {len(combo_dataset)/min(len(wave1), len(wave23))*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nCombo dataset shape: {combo_dataset.shape}\")\n",
    "print(f\"Columns: {len(combo_dataset.columns)} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d3ff2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # write the first 5 rows (head) to CSV, without the DataFrame index\n",
    "combo_dataset.to_csv('combo_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cd3e70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing combo dataset structure...\n",
      "==================================================\n",
      "Column breakdown:\n",
      "  Wave1 columns (no suffix): 37\n",
      "  Actual _w1 suffixed columns: 4\n",
      "  Wave23 columns (_w23): 40\n",
      "  Shared/ID columns: 1\n",
      "\n",
      "Shared columns: ['StFCID']\n",
      "\n",
      "Sample Wave1 columns (no suffix): ['RepDate', 'OutcmRpt', 'OutcmDte', 'OutcmFCS', 'CurrFTE', 'CurrPTE', 'EmplySklls', 'SocSecrty', 'EducAid', 'PubFinAs']\n",
      "Sample Wave23 columns: ['StFIPS_w23', 'St_w23', 'Sex_w23', 'Cohort_w23', 'Elig19_w23', 'PubFoodAs_w23', 'SubAbuse_w23', 'Assoc_Degree_w23', 'Responded_w23', 'HS_or_GED_w23']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Analyze the combo dataset structure - FIXED \n",
    "print(\"\\nAnalyzing combo dataset structure...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count columns by source\n",
    "actual_w1_cols = [col for col in combo_dataset.columns if col.endswith('_w1')]\n",
    "no_suffix_cols = [col for col in combo_dataset.columns if not col.endswith(('_w1', '_w23'))]\n",
    "w23_cols = [col for col in combo_dataset.columns if col.endswith('_w23')]\n",
    "\n",
    "# Wave1 columns are the no_suffix_cols (these ARE the w1 data)\n",
    "w1_cols = [col for col in no_suffix_cols if col != 'StFCID']  # This is the fix!\n",
    "shared_cols = ['StFCID'] if 'StFCID' in combo_dataset.columns else []\n",
    "\n",
    "print(f\"Column breakdown:\")\n",
    "print(f\"  Wave1 columns (no suffix): {len(w1_cols)}\")\n",
    "print(f\"  Actual _w1 suffixed columns: {len(actual_w1_cols)}\")\n",
    "print(f\"  Wave23 columns (_w23): {len(w23_cols)}\")\n",
    "print(f\"  Shared/ID columns: {len(shared_cols)}\")\n",
    "\n",
    "print(f\"\\nShared columns: {shared_cols}\")\n",
    "print(f\"\\nSample Wave1 columns (no suffix): {w1_cols[:10]}\")\n",
    "print(f\"Sample Wave23 columns: {w23_cols[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93ab52ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for predictor and outcome variables...\n",
      "==================================================\n",
      "PREDICTORS in combo dataset:\n",
      "  ✓ CurrFTE: found in Wave1 (no suffix)\n",
      "  ✓ CurrPTE: found in Wave1 (no suffix)\n",
      "  ✓ EmplySklls: found in Wave1 (no suffix)\n",
      "  ✓ SocSecrty: found in Wave1 (no suffix)\n",
      "  ✓ EducAid: found in Wave1 (no suffix)\n",
      "  ✓ PubFinAs: found in Wave1 (no suffix)\n",
      "  ✓ PubFoodAs: found in Wave1 (no suffix)\n",
      "  ✓ PubHousAs: found in Wave1 (no suffix)\n",
      "  ✓ OthrFinAs: found in Wave1 (no suffix)\n",
      "  ✓ Medicaid: found in Wave1 (no suffix)\n",
      "  ✓ OthrHlthIn: found in Wave1 (no suffix)\n",
      "  ✓ MedicalIn: found in Wave1 (no suffix)\n",
      "  ✓ MentlHlthIn: found in Wave1 (no suffix)\n",
      "\n",
      "Found 13/13 predictors\n",
      "\n",
      "OUTCOMES in combo dataset:\n",
      "  ✓ Homeless: Wave1, Wave23\n",
      "  ✓ SubAbuse: Wave1, Wave23\n",
      "  ✓ Incarc: Wave1, Wave23\n",
      "  ✓ Children: Wave1, Wave23\n",
      "  ✓ Marriage: Wave1, Wave23\n",
      "  ✓ CnctAdult: Wave1, Wave23\n",
      "\n",
      "Found 6/6 outcomes\n",
      "\n",
      "FINAL COLUMN MAPPING:\n",
      "  Predictor columns (Wave1): ['CurrFTE', 'CurrPTE', 'EmplySklls', 'SocSecrty', 'EducAid', 'PubFinAs', 'PubFoodAs', 'PubHousAs', 'OthrFinAs', 'Medicaid', 'OthrHlthIn', 'MedicalIn', 'MentlHlthIn']\n",
      "  Outcome columns (Wave23): ['Homeless_w23', 'SubAbuse_w23', 'Incarc_w23', 'Children_w23', 'Marriage_w23', 'CnctAdult_w23']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Check for our specific variables - FIXED to look for exact predictor names\n",
    "print(\"\\nChecking for predictor and outcome variables...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check predictors - look for exact names (like 'CurrFTE') \n",
    "print(\"PREDICTORS in combo dataset:\")\n",
    "found_predictors = []\n",
    "for pred in PREDICTORS:\n",
    "    # For predictors: look for exact name in w1_cols (no suffix columns)\n",
    "    exact_match = pred in w1_cols\n",
    "    \n",
    "    if exact_match:\n",
    "        found_predictors.append(pred)\n",
    "        print(f\"  ✓ {pred}: found in Wave1 (no suffix)\")\n",
    "    else:\n",
    "        print(f\"  ✗ {pred}: not found\")\n",
    "\n",
    "print(f\"\\nFound {len(found_predictors)}/{len(PREDICTORS)} predictors\")\n",
    "\n",
    "# Check outcomes in Wave23 columns\n",
    "print(\"\\nOUTCOMES in combo dataset:\")\n",
    "found_outcomes = []\n",
    "for outcome in OUTCOMES:\n",
    "    # Check in w23 columns first, then no suffix\n",
    "    w23_match = f\"{outcome}_w23\" in w23_cols\n",
    "    no_suffix_match = outcome in w1_cols\n",
    "    \n",
    "    if w23_match or no_suffix_match:\n",
    "        found_outcomes.append(outcome)\n",
    "        match_info = []\n",
    "        if no_suffix_match: match_info.append(\"Wave1\")\n",
    "        if w23_match: match_info.append(\"Wave23\")\n",
    "        print(f\"  ✓ {outcome}: {', '.join(match_info)}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {outcome}: not found\")\n",
    "\n",
    "print(f\"\\nFound {len(found_outcomes)}/{len(OUTCOMES)} outcomes\")\n",
    "\n",
    "# Create final column lists\n",
    "predictor_columns = [pred for pred in PREDICTORS if pred in w1_cols]\n",
    "outcome_columns = [f\"{outcome}_w23\" for outcome in OUTCOMES if f\"{outcome}_w23\" in w23_cols]\n",
    "\n",
    "print(f\"\\nFINAL COLUMN MAPPING:\")\n",
    "print(f\"  Predictor columns (Wave1): {predictor_columns}\")\n",
    "print(f\"  Outcome columns (Wave23): {outcome_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "800ad135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for predictor and outcome variables...\n",
      "==================================================\n",
      "PREDICTORS in combo dataset:\n",
      "  ✓ CurrFTE: exact, w23\n",
      "  ✓ CurrPTE: exact, w23\n",
      "  ✓ EmplySklls: exact, w23\n",
      "  ✓ SocSecrty: exact, w23\n",
      "  ✓ EducAid: exact, w23\n",
      "  ✓ PubFinAs: exact, w23\n",
      "  ✓ PubFoodAs: exact, w23\n",
      "  ✓ PubHousAs: exact, w23\n",
      "  ✓ OthrFinAs: exact, w23\n",
      "  ✓ Medicaid: exact, w23\n",
      "  ✓ OthrHlthIn: exact, w23\n",
      "  ✓ MedicalIn: exact, w23\n",
      "  ✓ MentlHlthIn: exact, w23\n",
      "\n",
      "Found 13/13 predictors\n",
      "\n",
      "OUTCOMES in combo dataset:\n",
      "  ✓ Homeless: exact, w23\n",
      "  ✓ SubAbuse: exact, w23\n",
      "  ✓ Incarc: exact, w23\n",
      "  ✓ Children: exact, w23\n",
      "  ✓ Marriage: exact, w23\n",
      "  ✓ CnctAdult: exact, w23\n",
      "\n",
      "Found 6/6 outcomes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 4: Check for our specific variables\n",
    "print(\"\\nChecking for predictor and outcome variables...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check predictors in Wave1 columns\n",
    "print(\"PREDICTORS in combo dataset:\")\n",
    "found_predictors = []\n",
    "for pred in PREDICTORS:\n",
    "    # Check if predictor exists in any form\n",
    "    exact_match = pred in combo_dataset.columns\n",
    "    w1_match = f\"{pred}_w1\" in combo_dataset.columns\n",
    "    w23_match = f\"{pred}_w23\" in combo_dataset.columns\n",
    "    \n",
    "    if exact_match or w1_match or w23_match:\n",
    "        found_predictors.append(pred)\n",
    "        match_info = []\n",
    "        if exact_match: match_info.append(\"exact\")\n",
    "        if w1_match: match_info.append(\"w1\")\n",
    "        if w23_match: match_info.append(\"w23\")\n",
    "        print(f\"  ✓ {pred}: {', '.join(match_info)}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {pred}: not found\")\n",
    "\n",
    "print(f\"\\nFound {len(found_predictors)}/{len(PREDICTORS)} predictors\")\n",
    "\n",
    "# Check outcomes in Wave23 columns\n",
    "print(\"\\nOUTCOMES in combo dataset:\")\n",
    "found_outcomes = []\n",
    "for outcome in OUTCOMES:\n",
    "    # Check if outcome exists in any form\n",
    "    exact_match = outcome in combo_dataset.columns\n",
    "    w1_match = f\"{outcome}_w1\" in combo_dataset.columns\n",
    "    w23_match = f\"{outcome}_w23\" in combo_dataset.columns\n",
    "    \n",
    "    if exact_match or w1_match or w23_match:\n",
    "        found_outcomes.append(outcome)\n",
    "        match_info = []\n",
    "        if exact_match: match_info.append(\"exact\")\n",
    "        if w1_match: match_info.append(\"w1\")\n",
    "        if w23_match: match_info.append(\"w23\")\n",
    "        print(f\"  ✓ {outcome}: {', '.join(match_info)}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {outcome}: not found\")\n",
    "\n",
    "print(f\"\\nFound {len(found_outcomes)}/{len(OUTCOMES)} outcomes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "822edd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating analysis-ready dataset...\n",
      "==================================================\n",
      "Analysis variables selected:\n",
      "  Predictor columns: 13\n",
      "    ['CurrFTE', 'CurrPTE', 'EmplySklls', 'SocSecrty', 'EducAid', 'PubFinAs', 'PubFoodAs', 'PubHousAs', 'OthrFinAs', 'Medicaid', 'OthrHlthIn', 'MedicalIn', 'MentlHlthIn']\n",
      "  Outcome columns: 6\n",
      "    ['Homeless_w23', 'SubAbuse_w23', 'Incarc_w23', 'Children_w23', 'Marriage_w23', 'CnctAdult_w23']\n",
      "\n",
      "Analysis dataset shape: (720, 20)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Create analysis-ready dataset\n",
    "print(\"\\nCreating analysis-ready dataset...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Build predictor column list (use exact names from Wave1 - no suffix)\n",
    "predictor_columns = []\n",
    "for pred in PREDICTORS:\n",
    "    if pred in w1_cols:  # Look for exact name like 'CurrFTE'\n",
    "        predictor_columns.append(pred)\n",
    "\n",
    "# Build outcome column list (prefer Wave23 _w23 versions)\n",
    "outcome_columns = []\n",
    "for outcome in OUTCOMES:\n",
    "    if f\"{outcome}_w23\" in w23_cols:\n",
    "        outcome_columns.append(f\"{outcome}_w23\")\n",
    "    elif f\"{outcome}_w2\" in combo_dataset.columns:\n",
    "        outcome_columns.append(f\"{outcome}_w2\")\n",
    "    elif f\"{outcome}_w3\" in combo_dataset.columns:\n",
    "        outcome_columns.append(f\"{outcome}_w3\")\n",
    "    elif outcome in w1_cols:\n",
    "        outcome_columns.append(outcome)\n",
    "\n",
    "print(f\"Analysis variables selected:\")\n",
    "print(f\"  Predictor columns: {len(predictor_columns)}\")\n",
    "print(f\"    {predictor_columns}\")\n",
    "print(f\"  Outcome columns: {len(outcome_columns)}\")\n",
    "print(f\"    {outcome_columns}\")\n",
    "\n",
    "# Create analysis dataset\n",
    "analysis_columns = ['StFCID'] + predictor_columns + outcome_columns\n",
    "analysis_dataset = combo_dataset[analysis_columns].copy()\n",
    "\n",
    "print(f\"\\nAnalysis dataset shape: {analysis_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c518f9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StFCID</th>\n",
       "      <th>CurrFTE</th>\n",
       "      <th>CurrPTE</th>\n",
       "      <th>EmplySklls</th>\n",
       "      <th>SocSecrty</th>\n",
       "      <th>EducAid</th>\n",
       "      <th>PubFinAs</th>\n",
       "      <th>PubFoodAs</th>\n",
       "      <th>PubHousAs</th>\n",
       "      <th>OthrFinAs</th>\n",
       "      <th>Medicaid</th>\n",
       "      <th>OthrHlthIn</th>\n",
       "      <th>MedicalIn</th>\n",
       "      <th>MentlHlthIn</th>\n",
       "      <th>Homeless_w23</th>\n",
       "      <th>SubAbuse_w23</th>\n",
       "      <th>Incarc_w23</th>\n",
       "      <th>Children_w23</th>\n",
       "      <th>Marriage_w23</th>\n",
       "      <th>CnctAdult_w23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TNµ®œ¿ˆ©¬«÷½û¥</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TNµ®œ¿ˆ®¦¥þ¸ý¤</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TNµ®œ¿ˆ®©¯ô¸ù¤</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TNµ®œ¿ˆ®«¨ôºü</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TNµ®œ¿ˆ®«®ð¸ø¬</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           StFCID CurrFTE CurrPTE EmplySklls SocSecrty EducAid PubFinAs  \\\n",
       "0  TNµ®œ¿ˆ©¬«÷½û¥     0.0     0.0        0.0       0.0     0.0      0.0   \n",
       "1  TNµ®œ¿ˆ®¦¥þ¸ý¤     0.0     0.0        0.0       0.0     0.0      0.0   \n",
       "2  TNµ®œ¿ˆ®©¯ô¸ù¤     0.0     0.0        0.0       1.0     NaN      0.0   \n",
       "3  TNµ®œ¿ˆ®«¨ôºü      0.0     0.0        0.0       0.0     NaN      0.0   \n",
       "4  TNµ®œ¿ˆ®«®ð¸ø¬     0.0     0.0        0.0       0.0     0.0      0.0   \n",
       "\n",
       "  PubFoodAs PubHousAs OthrFinAs Medicaid OthrHlthIn MedicalIn MentlHlthIn  \\\n",
       "0       0.0       0.0       0.0      0.0        0.0       NaN         NaN   \n",
       "1       0.0       0.0       0.0      0.0        0.0       NaN         NaN   \n",
       "2       0.0       0.0       0.0      1.0        0.0       NaN         NaN   \n",
       "3       0.0       0.0       0.0      1.0        1.0       1.0         1.0   \n",
       "4       0.0       0.0       0.0      1.0        0.0       NaN         NaN   \n",
       "\n",
       "  Homeless_w23 SubAbuse_w23 Incarc_w23 Children_w23 Marriage_w23 CnctAdult_w23  \n",
       "0            0            0          0            1            1             1  \n",
       "1            0            0          0            1            1             1  \n",
       "2            0            1          1            1            1             1  \n",
       "3            0            1          0            0          NaN             1  \n",
       "4            1            0          0            1            1             1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c01fa59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking data quality...\n",
      "==================================================\n",
      "Step 1: Converting any inappropriate 0s back to NaN...\n",
      "\n",
      "Step 2: Missing data summary (NaN values only):\n",
      "  CurrFTE: 10 missing (1.4%) - preserved as NaN\n",
      "  CurrPTE: 9 missing (1.2%) - preserved as NaN\n",
      "  EmplySklls: 13 missing (1.8%) - preserved as NaN\n",
      "  SocSecrty: 12 missing (1.7%) - preserved as NaN\n",
      "  EducAid: 71 missing (9.9%) - preserved as NaN\n",
      "  PubFinAs: 598 missing (83.1%) - preserved as NaN\n",
      "  PubFoodAs: 599 missing (83.2%) - preserved as NaN\n",
      "  PubHousAs: 599 missing (83.2%) - preserved as NaN\n",
      "  OthrFinAs: 6 missing (0.8%) - preserved as NaN\n",
      "  Medicaid: 28 missing (3.9%) - preserved as NaN\n",
      "  OthrHlthIn: 89 missing (12.4%) - preserved as NaN\n",
      "  MedicalIn: 678 missing (94.2%) - preserved as NaN\n",
      "  MentlHlthIn: 680 missing (94.4%) - preserved as NaN\n",
      "  Homeless_w23: 1 missing (0.1%) - preserved as NaN\n",
      "  SubAbuse_w23: 3 missing (0.4%) - preserved as NaN\n",
      "  Incarc_w23: 2 missing (0.3%) - preserved as NaN\n",
      "  Children_w23: 8 missing (1.1%) - preserved as NaN\n",
      "  Marriage_w23: 531 missing (73.8%) - preserved as NaN\n",
      "  CnctAdult_w23: 1 missing (0.1%) - preserved as NaN\n",
      "  Total NaN values: 3,938 (preserved as blank cells)\n",
      "\n",
      "Step 3: Complete cases analysis:\n",
      "  Total records: 720\n",
      "  Complete cases (no NaN): 0\n",
      "  Records with missing data: 720\n",
      "  Retention rate: 0.0%\n",
      "\n",
      "Step 4: Sample of analysis dataset (NaN preserved):\n",
      "           StFCID CurrFTE CurrPTE EmplySklls SocSecrty EducAid PubFinAs  \\\n",
      "0  TNµ®œ¿ˆ©¬«÷½û¥     0.0     0.0        0.0       0.0     0.0      0.0   \n",
      "1  TNµ®œ¿ˆ®¦¥þ¸ý¤     0.0     0.0        0.0       0.0     0.0      0.0   \n",
      "2  TNµ®œ¿ˆ®©¯ô¸ù¤     0.0     0.0        0.0       1.0     NaN      0.0   \n",
      "3  TNµ®œ¿ˆ®«¨ôºü      0.0     0.0        0.0       0.0     NaN      0.0   \n",
      "4  TNµ®œ¿ˆ®«®ð¸ø¬     0.0     0.0        0.0       0.0     0.0      0.0   \n",
      "\n",
      "  PubFoodAs PubHousAs OthrFinAs Medicaid OthrHlthIn MedicalIn MentlHlthIn  \\\n",
      "0       0.0       0.0       0.0      0.0        0.0       NaN         NaN   \n",
      "1       0.0       0.0       0.0      0.0        0.0       NaN         NaN   \n",
      "2       0.0       0.0       0.0      1.0        0.0       NaN         NaN   \n",
      "3       0.0       0.0       0.0      1.0        1.0       1.0         1.0   \n",
      "4       0.0       0.0       0.0      1.0        0.0       NaN         NaN   \n",
      "\n",
      "  Homeless_w23 SubAbuse_w23 Incarc_w23 Children_w23 Marriage_w23 CnctAdult_w23  \n",
      "0            0            0          0            1            1             1  \n",
      "1            0            0          0            1            1             1  \n",
      "2            0            1          1            1            1             1  \n",
      "3            0            1          0            0          NaN             1  \n",
      "4            1            0          0            1            1             1  \n",
      "\n",
      "Step 5: Data type verification:\n",
      "  CurrFTE: object, 10 NaN values\n",
      "  CurrPTE: object, 9 NaN values\n",
      "  EmplySklls: object, 13 NaN values\n",
      "  SocSecrty: object, 12 NaN values\n",
      "  EducAid: object, 71 NaN values\n",
      "\n",
      "✅ Data quality check complete!\n",
      "✅ All missing values preserved as NaN (blank cells)\n",
      "✅ No inappropriate conversion to 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Check data quality and missing values - preserve NaN as blank cells\n",
    "print(\"\\nChecking data quality...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# First, ensure any 0s that should be NaN are converted back to NaN\n",
    "print(\"Step 1: Converting any inappropriate 0s back to NaN...\")\n",
    "for col in predictor_columns + outcome_columns:\n",
    "    if col in analysis_dataset.columns:\n",
    "        # Check if column has suspicious patterns (many 0s that might be missing data)\n",
    "        zero_count = (analysis_dataset[col] == 0).sum()\n",
    "        total_count = len(analysis_dataset)\n",
    "        if zero_count > 0:\n",
    "            print(f\"  {col}: {zero_count} zeros found (keeping as-is for now)\")\n",
    "\n",
    "# Missing data analysis - count actual NaN values\n",
    "print(\"\\nStep 2: Missing data summary (NaN values only):\")\n",
    "missing_data = analysis_dataset.isnull().sum()\n",
    "missing_pct = (missing_data / len(analysis_dataset) * 100).round(1)\n",
    "\n",
    "total_missing = 0\n",
    "for col in predictor_columns + outcome_columns:\n",
    "    if missing_data[col] > 0:\n",
    "        total_missing += missing_data[col]\n",
    "        print(f\"  {col}: {missing_data[col]:,} missing ({missing_pct[col]}%) - preserved as NaN\")\n",
    "\n",
    "if total_missing == 0:\n",
    "    print(\"  ✅ No missing values found - all data complete\")\n",
    "else:\n",
    "    print(f\"  Total NaN values: {total_missing:,} (preserved as blank cells)\")\n",
    "\n",
    "# Complete cases analysis - only count rows with ALL data present\n",
    "modeling_columns = predictor_columns + outcome_columns\n",
    "complete_cases = analysis_dataset.dropna(subset=modeling_columns)\n",
    "\n",
    "print(f\"\\nStep 3: Complete cases analysis:\")\n",
    "print(f\"  Total records: {len(analysis_dataset):,}\")\n",
    "print(f\"  Complete cases (no NaN): {len(complete_cases):,}\")\n",
    "print(f\"  Records with missing data: {len(analysis_dataset) - len(complete_cases):,}\")\n",
    "print(f\"  Retention rate: {len(complete_cases)/len(analysis_dataset)*100:.1f}%\")\n",
    "\n",
    "# Show sample data with NaN preservation\n",
    "print(f\"\\nStep 4: Sample of analysis dataset (NaN preserved):\")\n",
    "sample_data = analysis_dataset.head()\n",
    "print(sample_data)\n",
    "\n",
    "# Verify data types - ensure nothing was inappropriately converted\n",
    "print(f\"\\nStep 5: Data type verification:\")\n",
    "for col in (predictor_columns + outcome_columns)[:5]:  # Show first 5 columns\n",
    "    if col in analysis_dataset.columns:\n",
    "        dtype = analysis_dataset[col].dtype\n",
    "        nan_count = analysis_dataset[col].isnull().sum()\n",
    "        print(f\"  {col}: {dtype}, {nan_count} NaN values\")\n",
    "\n",
    "print(f\"\\n✅ Data quality check complete!\")\n",
    "print(f\"✅ All missing values preserved as NaN (blank cells)\")\n",
    "print(f\"✅ No inappropriate conversion to 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "551c4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the first 5 rows (head) to CSV, without the DataFrame index\n",
    "analysis_dataset.to_csv('analysis_dataset_head.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4516d57",
   "metadata": {},
   "source": [
    "# Cleaning & Grouping for Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c570135",
   "metadata": {},
   "source": [
    "##### Preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c99a39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing combo_dataset - keeping both _w1 and _w23 columns...\n",
      "============================================================\n",
      "Combo dataset info:\n",
      "  Shape: (720, 82)\n",
      "  Total columns: 82\n",
      "  Records: 720\n",
      "\n",
      "Column breakdown:\n",
      "  Wave1 columns (_w1): 4\n",
      "  Wave23 columns (_w23): 40\n",
      "  Shared columns (no suffix): 38\n",
      "\n",
      "Shared columns: ['StFCID', 'RepDate', 'OutcmRpt', 'OutcmDte', 'OutcmFCS', 'CurrFTE', 'CurrPTE', 'EmplySklls', 'SocSecrty', 'EducAid', 'PubFinAs', 'PubFoodAs', 'PubHousAs', 'OthrFinAs', 'HighEdCert', 'CurrenRoll', 'CnctAdult', 'Homeless', 'SubAbuse', 'Incarc', 'Children', 'Marriage', 'Medicaid', 'OthrHlthIn', 'MedicalIn', 'MentlHlthIn', 'PrescripIn', 'Baseline', 'Elig19', 'Elig21', 'Responded', 'Race', 'HS_or_GED', 'Voc_Certificate', 'Voc_License', 'Assoc_Degree', 'Bach_Degree', 'Higher_Degree']\n",
      "\n",
      "Sample Wave1 columns: ['StFIPS_w1', 'St_w1', 'Sex_w1', 'Cohort_w1']\n",
      "Sample Wave23 columns: ['StFIPS_w23', 'St_w23', 'Sex_w23', 'Cohort_w23', 'Elig19_w23']\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Analyze combo_dataset structure and keep both wave columns\n",
    "print(\"Processing combo_dataset - keeping both _w1 and _w23 columns...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check the combo_dataset structure\n",
    "print(f\"Combo dataset info:\")\n",
    "print(f\"  Shape: {combo_dataset.shape}\")\n",
    "print(f\"  Total columns: {len(combo_dataset.columns)}\")\n",
    "print(f\"  Records: {len(combo_dataset):,}\")\n",
    "\n",
    "# Identify column types by suffix\n",
    "w1_columns = [col for col in combo_dataset.columns if col.endswith('_w1')]\n",
    "w23_columns = [col for col in combo_dataset.columns if col.endswith('_w23')]\n",
    "shared_columns = [col for col in combo_dataset.columns if not col.endswith(('_w1', '_w23'))]\n",
    "\n",
    "print(f\"\\nColumn breakdown:\")\n",
    "print(f\"  Wave1 columns (_w1): {len(w1_columns)}\")\n",
    "print(f\"  Wave23 columns (_w23): {len(w23_columns)}\")\n",
    "print(f\"  Shared columns (no suffix): {len(shared_columns)}\")\n",
    "\n",
    "print(f\"\\nShared columns: {shared_columns}\")\n",
    "\n",
    "# Show sample of paired columns\n",
    "print(f\"\\nSample Wave1 columns: {w1_columns[:5]}\")\n",
    "print(f\"Sample Wave23 columns: {w23_columns[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d19872ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding matching column pairs between waves...\n",
      "==================================================\n",
      "Variable analysis:\n",
      "  Paired variables (both waves): 4\n",
      "  Wave1 only variables: 0\n",
      "  Wave23 only variables: 36\n",
      "\n",
      "Paired variables (first 10): ['StFIPS', 'St', 'Sex', 'Cohort']\n",
      "Wave23 only (first 5): ['EmplySklls', 'Elig19', 'Assoc_Degree', 'OutcmRpt', 'Elig21']\n",
      "\n",
      "Example paired columns:\n",
      "  StFIPS: StFIPS_w1 & StFIPS_w23\n",
      "  St: St_w1 & St_w23\n",
      "  Sex: Sex_w1 & Sex_w23\n",
      "  Cohort: Cohort_w1 & Cohort_w23\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Find matching column pairs between waves - FIXED for consistent _w1 naming\n",
    "print(\"\\nFinding matching column pairs between waves...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get base names for each wave (remove suffixes)\n",
    "w1_base_names = [col.replace('_w1', '') for col in w1_columns]\n",
    "w23_base_names = [col.replace('_w23', '') for col in w23_columns]\n",
    "\n",
    "# Find which variables exist in both waves\n",
    "paired_variables = []\n",
    "w1_only_variables = []\n",
    "w23_only_variables = []\n",
    "\n",
    "for base_name in set(w1_base_names + w23_base_names):\n",
    "    w1_col = f\"{base_name}_w1\"  # Now ALL Wave1 columns have _w1\n",
    "    w23_col = f\"{base_name}_w23\"\n",
    "    \n",
    "    has_w1 = w1_col in combo_dataset.columns\n",
    "    has_w23 = w23_col in combo_dataset.columns\n",
    "    \n",
    "    if has_w1 and has_w23:\n",
    "        paired_variables.append(base_name)\n",
    "    elif has_w1:\n",
    "        w1_only_variables.append(base_name)\n",
    "    elif has_w23:\n",
    "        w23_only_variables.append(base_name)\n",
    "\n",
    "print(f\"Variable analysis:\")\n",
    "print(f\"  Paired variables (both waves): {len(paired_variables)}\")\n",
    "print(f\"  Wave1 only variables: {len(w1_only_variables)}\")\n",
    "print(f\"  Wave23 only variables: {len(w23_only_variables)}\")\n",
    "\n",
    "print(f\"\\nPaired variables (first 10): {paired_variables[:10]}\")\n",
    "if w1_only_variables:\n",
    "    print(f\"Wave1 only (first 5): {w1_only_variables[:5]}\")\n",
    "if w23_only_variables:\n",
    "    print(f\"Wave23 only (first 5): {w23_only_variables[:5]}\")\n",
    "\n",
    "# Show example of paired columns\n",
    "print(f\"\\nExample paired columns:\")\n",
    "for base_name in paired_variables[:5]:\n",
    "    w1_name = f\"{base_name}_w1\"\n",
    "    w23_name = f\"{base_name}_w23\"\n",
    "    print(f\"  {base_name}: {w1_name} & {w23_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8d1a436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating comprehensive clean dataset...\n",
      "==================================================\n",
      "Starting comprehensive cleaning:\n",
      "  Original shape: (720, 82)\n",
      "\n",
      "Step 1: Text cleaning (preserving NaN)...\n",
      "Step 2: Converting missing indicators to NaN...\n",
      "  Converted 0 missing indicators to NaN\n",
      "Step 3: Binary encoding (preserving NaN)...\n",
      "  Applied binary encoding to 34 columns\n",
      "Step 4: Converting to numeric (preserving NaN)...\n",
      "\n",
      "Comprehensive cleaning complete!\n",
      "  Final shape: (720, 82)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create comprehensive clean dataset preserving both waves\n",
    "print(\"\\nCreating comprehensive clean dataset...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Start with the complete combo_dataset\n",
    "comprehensive_clean = combo_dataset.copy()\n",
    "\n",
    "print(f\"Starting comprehensive cleaning:\")\n",
    "print(f\"  Original shape: {comprehensive_clean.shape}\")\n",
    "\n",
    "# Get all analysis columns (both waves)\n",
    "all_analysis_columns = w1_columns + w23_columns\n",
    "\n",
    "# Clean text data but preserve NaN\n",
    "print(f\"\\nStep 1: Text cleaning (preserving NaN)...\")\n",
    "for col in all_analysis_columns:\n",
    "    if col in comprehensive_clean.columns:\n",
    "        # Only clean non-null values\n",
    "        mask = comprehensive_clean[col].notna()\n",
    "        if mask.any():\n",
    "            comprehensive_clean.loc[mask, col] = comprehensive_clean.loc[mask, col].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Handle missing indicators\n",
    "print(f\"Step 2: Converting missing indicators to NaN...\")\n",
    "missing_indicators = ['NAN', 'NONE', 'NULL', '', 'MISSING', 'UNKNOWN', 'N/A', 'NA', 'nan']\n",
    "\n",
    "missing_converted = 0\n",
    "for col in all_analysis_columns:\n",
    "    if col in comprehensive_clean.columns:\n",
    "        before_count = comprehensive_clean[col].isin(missing_indicators).sum()\n",
    "        comprehensive_clean[col] = comprehensive_clean[col].replace(missing_indicators, np.nan)\n",
    "        missing_converted += before_count\n",
    "\n",
    "print(f\"  Converted {missing_converted:,} missing indicators to NaN\")\n",
    "\n",
    "# Binary encoding\n",
    "print(f\"Step 3: Binary encoding (preserving NaN)...\")\n",
    "binary_mapping = {\n",
    "    'YES': 1, 'Y': 1, '1': 1, 'TRUE': 1, 'T': 1,\n",
    "    'NO': 0, 'N': 0, '0': 0, 'FALSE': 0, 'F': 0\n",
    "}\n",
    "\n",
    "encoded_columns = 0\n",
    "for col in all_analysis_columns:\n",
    "    if col in comprehensive_clean.columns:\n",
    "        unique_vals = comprehensive_clean[col].dropna().unique()\n",
    "        if len(unique_vals) <= 5:  # Likely binary\n",
    "            before_encoding = comprehensive_clean[col].notna().sum()\n",
    "            comprehensive_clean[col] = comprehensive_clean[col].map(binary_mapping)\n",
    "            after_encoding = comprehensive_clean[col].notna().sum()\n",
    "            \n",
    "            if after_encoding > 0:\n",
    "                encoded_columns += 1\n",
    "\n",
    "print(f\"  Applied binary encoding to {encoded_columns} columns\")\n",
    "\n",
    "# Convert to numeric\n",
    "print(f\"Step 4: Converting to numeric (preserving NaN)...\")\n",
    "conversion_nans = 0\n",
    "for col in all_analysis_columns:\n",
    "    if col in comprehensive_clean.columns:\n",
    "        original_nulls = comprehensive_clean[col].isnull().sum()\n",
    "        comprehensive_clean[col] = pd.to_numeric(comprehensive_clean[col], errors='coerce')\n",
    "        new_nulls = comprehensive_clean[col].isnull().sum()\n",
    "        conversion_nans += (new_nulls - original_nulls)\n",
    "\n",
    "if conversion_nans > 0:\n",
    "    print(f\"  {conversion_nans:,} values became NaN during numeric conversion\")\n",
    "\n",
    "print(f\"\\nComprehensive cleaning complete!\")\n",
    "print(f\"  Final shape: {comprehensive_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "30a5f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV file\n",
    "comprehensive_clean.to_csv('comprehensive_clean_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08223984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating specific analysis datasets...\n",
      "==================================================\n",
      "Longitudinal analysis dataset:\n",
      "  Shape: (720, 7)\n",
      "  Wave1 predictors: 0\n",
      "  Wave23 outcomes: 6\n",
      "  Predictors: []\n",
      "  Outcomes: ['Homeless_w23', 'SubAbuse_w23', 'Incarc_w23', 'Children_w23', 'Marriage_w23', 'CnctAdult_w23']\n",
      "\n",
      "Complete cases longitudinal:\n",
      "  Shape: (188, 7)\n",
      "  Retention rate: 26.1%\n",
      "\n",
      "Paired variables analysis dataset:\n",
      "  Shape: (720, 9)\n",
      "  Paired variables: 4\n",
      "  Total paired columns: 8\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create specific analysis datasets\n",
    "print(\"\\nCreating specific analysis datasets...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dataset 1: Longitudinal analysis (Wave1 predictors -> Wave23 outcomes)\n",
    "longitudinal_predictors = [f\"{pred}_w1\" for pred in PREDICTORS if f\"{pred}_w1\" in comprehensive_clean.columns]\n",
    "longitudinal_outcomes = []\n",
    "\n",
    "for outcome in OUTCOMES:\n",
    "    if f\"{outcome}_w23\" in comprehensive_clean.columns:\n",
    "        longitudinal_outcomes.append(f\"{outcome}_w23\")\n",
    "\n",
    "longitudinal_analysis = comprehensive_clean[['StFCID'] + longitudinal_predictors + longitudinal_outcomes].copy()\n",
    "\n",
    "print(f\"Longitudinal analysis dataset:\")\n",
    "print(f\"  Shape: {longitudinal_analysis.shape}\")\n",
    "print(f\"  Wave1 predictors: {len(longitudinal_predictors)}\")\n",
    "print(f\"  Wave23 outcomes: {len(longitudinal_outcomes)}\")\n",
    "print(f\"  Predictors: {longitudinal_predictors}\")\n",
    "print(f\"  Outcomes: {longitudinal_outcomes}\")\n",
    "\n",
    "# Dataset 2: Complete cases for modeling\n",
    "complete_cases_longitudinal = longitudinal_analysis.dropna(subset=longitudinal_predictors + longitudinal_outcomes)\n",
    "\n",
    "print(f\"\\nComplete cases longitudinal:\")\n",
    "print(f\"  Shape: {complete_cases_longitudinal.shape}\")\n",
    "print(f\"  Retention rate: {len(complete_cases_longitudinal)/len(comprehensive_clean)*100:.1f}%\")\n",
    "\n",
    "# Dataset 3: All paired variables for comparison analysis\n",
    "paired_analysis_columns = ['StFCID']\n",
    "for var in paired_variables:\n",
    "    w1_col = f\"{var}_w1\"\n",
    "    w23_col = f\"{var}_w23\"\n",
    "    if w1_col in comprehensive_clean.columns and w23_col in comprehensive_clean.columns:\n",
    "        paired_analysis_columns.extend([w1_col, w23_col])\n",
    "\n",
    "paired_analysis = comprehensive_clean[paired_analysis_columns].copy()\n",
    "\n",
    "print(f\"\\nPaired variables analysis dataset:\")\n",
    "print(f\"  Shape: {paired_analysis.shape}\")\n",
    "print(f\"  Paired variables: {len(paired_variables)}\")\n",
    "print(f\"  Total paired columns: {len(paired_analysis_columns)-1}\")  # Subtract StFCID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8b54d1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating specific analysis datasets...\n",
      "==================================================\n",
      "Longitudinal analysis dataset:\n",
      "  Shape: (720, 20)\n",
      "  Wave1 predictors: 13\n",
      "  Wave23 outcomes: 6\n",
      "  Predictors: ['CurrFTE_w1', 'CurrPTE_w1', 'EmplySklls_w1', 'SocSecrty_w1', 'EducAid_w1', 'PubFinAs_w1', 'PubFoodAs_w1', 'PubHousAs_w1', 'OthrFinAs_w1', 'Medicaid_w1', 'OthrHlthIn_w1', 'MedicalIn_w1', 'MentlHlthIn_w1']\n",
      "  Outcomes: ['Homeless_w23', 'SubAbuse_w23', 'Incarc_w23', 'Children_w23', 'Marriage_w23', 'CnctAdult_w23']\n",
      "\n",
      "Missing data patterns in longitudinal dataset:\n",
      "Predictors with missing data:\n",
      "  CurrFTE_w1: 720 (100.0%)\n",
      "  CurrPTE_w1: 720 (100.0%)\n",
      "  EmplySklls_w1: 720 (100.0%)\n",
      "  SocSecrty_w1: 720 (100.0%)\n",
      "  EducAid_w1: 720 (100.0%)\n",
      "  PubFinAs_w1: 720 (100.0%)\n",
      "  PubFoodAs_w1: 720 (100.0%)\n",
      "  PubHousAs_w1: 720 (100.0%)\n",
      "  OthrFinAs_w1: 720 (100.0%)\n",
      "  Medicaid_w1: 720 (100.0%)\n",
      "  OthrHlthIn_w1: 720 (100.0%)\n",
      "  MedicalIn_w1: 720 (100.0%)\n",
      "  MentlHlthIn_w1: 720 (100.0%)\n",
      "Outcomes with missing data:\n",
      "  Homeless_w23: 1 (0.1%)\n",
      "  SubAbuse_w23: 3 (0.4%)\n",
      "  Incarc_w23: 2 (0.3%)\n",
      "  Children_w23: 8 (1.1%)\n",
      "  Marriage_w23: 531 (73.8%)\n",
      "  CnctAdult_w23: 1 (0.1%)\n",
      "\n",
      "Complete cases longitudinal:\n",
      "  Shape: (0, 20)\n",
      "  Retention rate: 0.0%\n",
      "  ⚠️  WARNING: Low retention rate (0.0%) - consider multiple imputation\n",
      "\n",
      "Paired variables analysis dataset:\n",
      "  Shape: (720, 81)\n",
      "  Valid paired variables: 40\n",
      "  Total paired columns: 80\n",
      "\n",
      "Paired variable completeness:\n",
      "  OthrFinAs: 0/720 (0.0%)\n",
      "  CurrenRoll: 0/720 (0.0%)\n",
      "  EmplySklls: 0/720 (0.0%)\n",
      "  Marriage: 0/720 (0.0%)\n",
      "  Baseline: 0/720 (0.0%)\n",
      "  Sex: 0/720 (0.0%)\n",
      "  St: 0/720 (0.0%)\n",
      "  Children: 0/720 (0.0%)\n",
      "  Homeless: 0/720 (0.0%)\n",
      "  Medicaid: 0/720 (0.0%)\n",
      "  MedicalIn: 0/720 (0.0%)\n",
      "  PubFinAs: 0/720 (0.0%)\n",
      "  Cohort: 0/720 (0.0%)\n",
      "  EducAid: 0/720 (0.0%)\n",
      "  CnctAdult: 0/720 (0.0%)\n",
      "  Bach_Degree: 720/720 (100.0%)\n",
      "  Elig19: 0/720 (0.0%)\n",
      "  MentlHlthIn: 0/720 (0.0%)\n",
      "  CurrPTE: 0/720 (0.0%)\n",
      "  HS_or_GED: 720/720 (100.0%)\n",
      "  PubHousAs: 0/720 (0.0%)\n",
      "  OutcmRpt: 720/720 (100.0%)\n",
      "  OutcmFCS: 0/720 (0.0%)\n",
      "  SocSecrty: 0/720 (0.0%)\n",
      "  CurrFTE: 0/720 (0.0%)\n",
      "  PrescripIn: 0/720 (0.0%)\n",
      "  Voc_License: 720/720 (100.0%)\n",
      "  PubFoodAs: 0/720 (0.0%)\n",
      "  Elig21: 0/720 (0.0%)\n",
      "  Incarc: 0/720 (0.0%)\n",
      "  OutcmDte: 0/720 (0.0%)\n",
      "  Assoc_Degree: 720/720 (100.0%)\n",
      "  OthrHlthIn: 0/720 (0.0%)\n",
      "  Voc_Certificate: 720/720 (100.0%)\n",
      "  Responded: 720/720 (100.0%)\n",
      "  SubAbuse: 0/720 (0.0%)\n",
      "  HighEdCert: 0/720 (0.0%)\n",
      "  StFIPS: 0/720 (0.0%)\n",
      "  Higher_Degree: 720/720 (100.0%)\n",
      "  RepDate: 0/720 (0.0%)\n",
      "\n",
      "Wave 23 cross-sectional dataset:\n",
      "  Shape: (720, 41)\n",
      "  Wave 23 variables: 40\n",
      "\n",
      "Creating change score dataset...\n",
      "Change score dataset:\n",
      "  Shape: (720, 41)\n",
      "  Change variables created: 40\n",
      "\n",
      "==================================================\n",
      "DATASET SUMMARY:\n",
      "==================================================\n",
      "1. Longitudinal prediction: 720 cases, 13 predictors → 6 outcomes\n",
      "2. Complete cases: 0 cases (0.0% retention)\n",
      "3. Paired variables: 720 cases, 40 variable pairs\n",
      "4. Wave 23 cross-sectional: 720 cases, 40 variables\n",
      "5. Change scores: 720 cases, 40 change variables\n",
      "\n",
      "All datasets stored in 'datasets' dictionary\n",
      "Access with: datasets['longitudinal'], datasets['complete_longitudinal'], etc.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create specific analysis datasets with enhanced quality checks\n",
    "print(\"\\nCreating specific analysis datasets...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dataset 1: Longitudinal analysis (Wave1 predictors -> Wave23 outcomes)\n",
    "longitudinal_predictors = [f\"{pred}_w1\" for pred in PREDICTORS if f\"{pred}_w1\" in comprehensive_clean.columns]\n",
    "longitudinal_outcomes = []\n",
    "for outcome in OUTCOMES:\n",
    "    if f\"{outcome}_w23\" in comprehensive_clean.columns:\n",
    "        longitudinal_outcomes.append(f\"{outcome}_w23\")\n",
    "\n",
    "longitudinal_analysis = comprehensive_clean[['StFCID'] + longitudinal_predictors + longitudinal_outcomes].copy()\n",
    "\n",
    "print(f\"Longitudinal analysis dataset:\")\n",
    "print(f\"  Shape: {longitudinal_analysis.shape}\")\n",
    "print(f\"  Wave1 predictors: {len(longitudinal_predictors)}\")\n",
    "print(f\"  Wave23 outcomes: {len(longitudinal_outcomes)}\")\n",
    "print(f\"  Predictors: {longitudinal_predictors}\")\n",
    "print(f\"  Outcomes: {longitudinal_outcomes}\")\n",
    "\n",
    "# Enhanced missing data analysis\n",
    "print(f\"\\nMissing data patterns in longitudinal dataset:\")\n",
    "predictor_missing = longitudinal_analysis[longitudinal_predictors].isnull().sum()\n",
    "outcome_missing = longitudinal_analysis[longitudinal_outcomes].isnull().sum()\n",
    "\n",
    "print(f\"Predictors with missing data:\")\n",
    "for pred, missing in predictor_missing[predictor_missing > 0].items():\n",
    "    print(f\"  {pred}: {missing} ({missing/len(longitudinal_analysis)*100:.1f}%)\")\n",
    "\n",
    "print(f\"Outcomes with missing data:\")\n",
    "for outcome, missing in outcome_missing[outcome_missing > 0].items():\n",
    "    print(f\"  {outcome}: {missing} ({missing/len(longitudinal_analysis)*100:.1f}%)\")\n",
    "\n",
    "# Dataset 2: Complete cases for modeling\n",
    "complete_cases_longitudinal = longitudinal_analysis.dropna(subset=longitudinal_predictors + longitudinal_outcomes)\n",
    "print(f\"\\nComplete cases longitudinal:\")\n",
    "print(f\"  Shape: {complete_cases_longitudinal.shape}\")\n",
    "print(f\"  Retention rate: {len(complete_cases_longitudinal)/len(comprehensive_clean)*100:.1f}%\")\n",
    "\n",
    "# Check if retention rate is concerning\n",
    "retention_rate = len(complete_cases_longitudinal)/len(comprehensive_clean)*100\n",
    "if retention_rate < 50:\n",
    "    print(f\"  ⚠️  WARNING: Low retention rate ({retention_rate:.1f}%) - consider multiple imputation\")\n",
    "elif retention_rate < 70:\n",
    "    print(f\"  ⚠️  CAUTION: Moderate retention rate ({retention_rate:.1f}%) - check for systematic missingness\")\n",
    "else:\n",
    "    print(f\"  ✓ Good retention rate ({retention_rate:.1f}%)\")\n",
    "\n",
    "# Dataset 3: All paired variables for comparison analysis\n",
    "paired_analysis_columns = ['StFCID']\n",
    "valid_paired_vars = []\n",
    "\n",
    "for var in paired_variables:\n",
    "    w1_col = f\"{var}_w1\"\n",
    "    w23_col = f\"{var}_w23\"\n",
    "    if w1_col in comprehensive_clean.columns and w23_col in comprehensive_clean.columns:\n",
    "        paired_analysis_columns.extend([w1_col, w23_col])\n",
    "        valid_paired_vars.append(var)\n",
    "\n",
    "paired_analysis = comprehensive_clean[paired_analysis_columns].copy()\n",
    "\n",
    "print(f\"\\nPaired variables analysis dataset:\")\n",
    "print(f\"  Shape: {paired_analysis.shape}\")\n",
    "print(f\"  Valid paired variables: {len(valid_paired_vars)}\")\n",
    "print(f\"  Total paired columns: {len(paired_analysis_columns)-1}\")  # Subtract StFCID\n",
    "\n",
    "# Calculate completeness for each paired variable\n",
    "print(f\"\\nPaired variable completeness:\")\n",
    "for var in valid_paired_vars:\n",
    "    w1_col = f\"{var}_w1\"\n",
    "    w23_col = f\"{var}_w23\"\n",
    "    both_complete = paired_analysis[[w1_col, w23_col]].dropna().shape[0]\n",
    "    completeness = both_complete / len(paired_analysis) * 100\n",
    "    print(f\"  {var}: {both_complete}/{len(paired_analysis)} ({completeness:.1f}%)\")\n",
    "\n",
    "# Dataset 4: Additional - Cross-sectional Wave 23 for current status analysis\n",
    "wave23_columns = ['StFCID'] + [col for col in comprehensive_clean.columns if col.endswith('_w23')]\n",
    "wave23_analysis = comprehensive_clean[wave23_columns].copy()\n",
    "print(f\"\\nWave 23 cross-sectional dataset:\")\n",
    "print(f\"  Shape: {wave23_analysis.shape}\")\n",
    "print(f\"  Wave 23 variables: {len(wave23_columns)-1}\")\n",
    "\n",
    "# Dataset 5: Additional - Change score dataset\n",
    "print(f\"\\nCreating change score dataset...\")\n",
    "change_analysis = comprehensive_clean[['StFCID']].copy()\n",
    "change_vars_created = 0\n",
    "\n",
    "for var in valid_paired_vars:\n",
    "    w1_col = f\"{var}_w1\"\n",
    "    w23_col = f\"{var}_w23\"\n",
    "    change_col = f\"{var}_change\"\n",
    "    \n",
    "    # Create change score (Wave23 - Wave1)\n",
    "    change_analysis[change_col] = comprehensive_clean[w23_col] - comprehensive_clean[w1_col]\n",
    "    change_vars_created += 1\n",
    "\n",
    "print(f\"Change score dataset:\")\n",
    "print(f\"  Shape: {change_analysis.shape}\")\n",
    "print(f\"  Change variables created: {change_vars_created}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"DATASET SUMMARY:\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"1. Longitudinal prediction: {longitudinal_analysis.shape[0]} cases, {len(longitudinal_predictors)} predictors → {len(longitudinal_outcomes)} outcomes\")\n",
    "print(f\"2. Complete cases: {complete_cases_longitudinal.shape[0]} cases ({retention_rate:.1f}% retention)\")\n",
    "print(f\"3. Paired variables: {paired_analysis.shape[0]} cases, {len(valid_paired_vars)} variable pairs\")\n",
    "print(f\"4. Wave 23 cross-sectional: {wave23_analysis.shape[0]} cases, {len(wave23_columns)-1} variables\")\n",
    "print(f\"5. Change scores: {change_analysis.shape[0]} cases, {change_vars_created} change variables\")\n",
    "\n",
    "# Store datasets in a dictionary for easy access\n",
    "datasets = {\n",
    "    'longitudinal': longitudinal_analysis,\n",
    "    'complete_longitudinal': complete_cases_longitudinal,\n",
    "    'paired': paired_analysis,\n",
    "    'wave23': wave23_analysis,\n",
    "    'change': change_analysis\n",
    "}\n",
    "\n",
    "print(f\"\\nAll datasets stored in 'datasets' dictionary\")\n",
    "print(f\"Access with: datasets['longitudinal'], datasets['complete_longitudinal'], etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe0240",
   "metadata": {},
   "source": [
    "# Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7d2c377f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing predictor → outcome relationships...\n",
      "============================================================\n",
      "Complete cases dataset has 0 cases\n",
      "⚠️  Using pairwise deletion instead of complete cases due to extensive missing data\n",
      "Analyzing 720 cases (pairwise deletion: True)\n",
      "Testing 13 predictors against 6 outcomes\n",
      "Total tests: 78\n",
      "\n",
      "MISSING DATA DIAGNOSTIC:\n",
      "========================================\n",
      "Wave 1 predictors missing data:\n",
      "  CurrFTE: 720/720 (100.0%)\n",
      "  CurrPTE: 720/720 (100.0%)\n",
      "  EmplySklls: 720/720 (100.0%)\n",
      "  SocSecrty: 720/720 (100.0%)\n",
      "  EducAid: 720/720 (100.0%)\n",
      "  PubFinAs: 720/720 (100.0%)\n",
      "  PubFoodAs: 720/720 (100.0%)\n",
      "  PubHousAs: 720/720 (100.0%)\n",
      "  OthrFinAs: 720/720 (100.0%)\n",
      "  Medicaid: 720/720 (100.0%)\n",
      "  OthrHlthIn: 720/720 (100.0%)\n",
      "  MedicalIn: 720/720 (100.0%)\n",
      "  MentlHlthIn: 720/720 (100.0%)\n",
      "\n",
      "Wave 23 outcomes missing data:\n",
      "  Homeless: 1/720 (0.1%)\n",
      "  SubAbuse: 3/720 (0.4%)\n",
      "  Incarc: 2/720 (0.3%)\n",
      "  Children: 8/720 (1.1%)\n",
      "  Marriage: 531/720 (73.8%)\n",
      "  CnctAdult: 1/720 (0.1%)\n",
      "\n",
      "PAIRWISE DATA AVAILABILITY (sample sizes for each test):\n",
      "============================================================\n",
      "\n",
      "Pairwise sample size summary:\n",
      "  Range: 0 to 0\n",
      "  Mean: 0.0\n",
      "  Pairs with N ≥ 20: 0\n",
      "  Pairs with N ≥ 50: 0\n",
      "\n",
      "Running statistical tests...\n",
      "Progress: ██████████████████████████ Complete!\n",
      "\n",
      "============================================================\n",
      "DATA AVAILABILITY:\n",
      "============================================================\n",
      "Total predictor-outcome pairs: 78\n",
      "Pairs with insufficient data (N < 20): 78\n",
      "Pairs with sufficient data: 0\n",
      "\n",
      "============================================================\n",
      "RESULTS SUMMARY:\n",
      "============================================================\n",
      "Total valid tests conducted: 78\n",
      "Significant relationships (p < 0.05): 0\n",
      "Significance rate: 0.0%\n",
      "Bonferroni corrected significant (α = 0.000641): 0\n",
      "FDR corrected significant (α = 0.05): 0\n",
      "\n",
      "Results stored in 'results_summary' dictionary\n",
      "Access with: results_summary['significant_results'], etc.\n",
      "\n",
      "No significant relationships found to save.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Find significant predictor → outcome relationships\n",
    "print(\"\\nAnalyzing predictor → outcome relationships...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check complete cases first, but use pairwise deletion if needed\n",
    "complete_cases_data = datasets['complete_longitudinal'].copy()\n",
    "print(f\"Complete cases dataset has {complete_cases_data.shape[0]} cases\")\n",
    "\n",
    "if complete_cases_data.shape[0] < 50:  # Not enough complete cases\n",
    "    print(\"⚠️  Using pairwise deletion instead of complete cases due to extensive missing data\")\n",
    "    analysis_data = datasets['longitudinal'].copy()\n",
    "    use_pairwise = True\n",
    "else:\n",
    "    analysis_data = complete_cases_data\n",
    "    use_pairwise = False\n",
    "\n",
    "print(f\"Analyzing {analysis_data.shape[0]} cases (pairwise deletion: {use_pairwise})\")\n",
    "\n",
    "# Separate predictors and outcomes\n",
    "predictor_cols = [col for col in analysis_data.columns if col.endswith('_w1') and col != 'StFCID']\n",
    "outcome_cols = [col for col in analysis_data.columns if col.endswith('_w23')]\n",
    "\n",
    "print(f\"Testing {len(predictor_cols)} predictors against {len(outcome_cols)} outcomes\")\n",
    "print(f\"Total tests: {len(predictor_cols) * len(outcome_cols)}\")\n",
    "\n",
    "# Diagnostic: Check missing data patterns\n",
    "print(f\"\\nMISSING DATA DIAGNOSTIC:\")\n",
    "print(f\"=\"*40)\n",
    "print(f\"Wave 1 predictors missing data:\")\n",
    "for col in predictor_cols:\n",
    "    missing = analysis_data[col].isnull().sum()\n",
    "    total = len(analysis_data)\n",
    "    print(f\"  {col.replace('_w1', '')}: {missing}/{total} ({missing/total*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nWave 23 outcomes missing data:\")\n",
    "for col in outcome_cols:\n",
    "    missing = analysis_data[col].isnull().sum()\n",
    "    total = len(analysis_data)\n",
    "    print(f\"  {col.replace('_w23', '')}: {missing}/{total} ({missing/total*100:.1f}%)\")\n",
    "\n",
    "# Check pairwise availability\n",
    "print(f\"\\nPAIRWISE DATA AVAILABILITY (sample sizes for each test):\")\n",
    "print(f\"=\"*60)\n",
    "pairwise_counts = []\n",
    "for predictor in predictor_cols:\n",
    "    for outcome in outcome_cols:\n",
    "        valid_pairs = (~analysis_data[predictor].isnull() & ~analysis_data[outcome].isnull()).sum()\n",
    "        pairwise_counts.append(valid_pairs)\n",
    "        if valid_pairs >= 20:  # Only show pairs with sufficient data\n",
    "            pred_short = predictor.replace('_w1', '')\n",
    "            outcome_short = outcome.replace('_w23', '')\n",
    "            print(f\"  {pred_short} → {outcome_short}: N = {valid_pairs}\")\n",
    "\n",
    "if pairwise_counts:\n",
    "    print(f\"\\nPairwise sample size summary:\")\n",
    "    print(f\"  Range: {min(pairwise_counts)} to {max(pairwise_counts)}\")\n",
    "    print(f\"  Mean: {np.mean(pairwise_counts):.1f}\")\n",
    "    print(f\"  Pairs with N ≥ 20: {sum(1 for n in pairwise_counts if n >= 20)}\")\n",
    "    print(f\"  Pairs with N ≥ 50: {sum(1 for n in pairwise_counts if n >= 50)}\")\n",
    "else:\n",
    "    print(\"  No valid pairs found!\")\n",
    "\n",
    "# Storage for results\n",
    "significant_relationships = []\n",
    "all_relationships = []\n",
    "\n",
    "# Function to determine variable type and choose appropriate test\n",
    "def analyze_relationship(predictor_data, outcome_data, pred_name, outcome_name):\n",
    "    \"\"\"Analyze relationship between predictor and outcome variables\"\"\"\n",
    "    \n",
    "    # Remove any remaining NaN values (pairwise deletion)\n",
    "    valid_indices = ~(pd.isna(predictor_data) | pd.isna(outcome_data))\n",
    "    pred_clean = predictor_data[valid_indices]\n",
    "    outcome_clean = outcome_data[valid_indices]\n",
    "    \n",
    "    if len(pred_clean) < 20:  # Need minimum sample size for meaningful analysis\n",
    "        return {\n",
    "            'predictor': pred_name,\n",
    "            'outcome': outcome_name,\n",
    "            'test': 'Insufficient data',\n",
    "            'statistic': np.nan,\n",
    "            'p_value': 1.0,\n",
    "            'effect_size': 0,\n",
    "            'n_cases': len(pred_clean),\n",
    "            'pred_unique_values': 0,\n",
    "            'outcome_unique_values': 0,\n",
    "            'insufficient_n': True\n",
    "        }\n",
    "    \n",
    "    # Determine if variables are continuous or categorical\n",
    "    pred_unique = len(pred_clean.unique())\n",
    "    outcome_unique = len(outcome_clean.unique())\n",
    "    \n",
    "    # Choose appropriate statistical test\n",
    "    if pred_unique <= 5 and outcome_unique <= 5:\n",
    "        # Both categorical - Chi-square test\n",
    "        try:\n",
    "            contingency_table = pd.crosstab(pred_clean, outcome_clean)\n",
    "            if contingency_table.size > 1:\n",
    "                chi2, p_value = stats.chi2_contingency(contingency_table)[:2]\n",
    "                test_used = \"Chi-square\"\n",
    "                statistic = chi2\n",
    "                effect_size = np.sqrt(chi2 / (len(pred_clean) * (min(contingency_table.shape) - 1)))  # Cramér's V\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "    elif pred_unique <= 5 and outcome_unique > 5:\n",
    "        # Predictor categorical, outcome continuous - ANOVA/t-test\n",
    "        try:\n",
    "            groups = [outcome_clean[pred_clean == group] for group in pred_clean.unique()]\n",
    "            groups = [g for g in groups if len(g) > 0]  # Remove empty groups\n",
    "            \n",
    "            if len(groups) == 2:\n",
    "                # t-test\n",
    "                statistic, p_value = stats.ttest_ind(groups[0], groups[1])\n",
    "                test_used = \"t-test\"\n",
    "                # Cohen's d effect size\n",
    "                pooled_std = np.sqrt((groups[0].var() + groups[1].var()) / 2)\n",
    "                effect_size = abs((groups[0].mean() - groups[1].mean()) / pooled_std) if pooled_std > 0 else 0\n",
    "            elif len(groups) > 2:\n",
    "                # One-way ANOVA\n",
    "                statistic, p_value = stats.f_oneway(*groups)\n",
    "                test_used = \"ANOVA\"\n",
    "                # Eta-squared effect size\n",
    "                overall_mean = outcome_clean.mean()\n",
    "                ss_between = sum(len(g) * (g.mean() - overall_mean)**2 for g in groups)\n",
    "                ss_total = sum((outcome_clean - overall_mean)**2)\n",
    "                effect_size = ss_between / ss_total if ss_total > 0 else 0\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "    elif pred_unique > 5 and outcome_unique <= 5:\n",
    "        # Predictor continuous, outcome categorical - reverse ANOVA\n",
    "        try:\n",
    "            groups = [pred_clean[outcome_clean == group] for group in outcome_clean.unique()]\n",
    "            groups = [g for g in groups if len(g) > 0]\n",
    "            \n",
    "            if len(groups) == 2:\n",
    "                statistic, p_value = stats.ttest_ind(groups[0], groups[1])\n",
    "                test_used = \"t-test (reversed)\"\n",
    "                pooled_std = np.sqrt((groups[0].var() + groups[1].var()) / 2)\n",
    "                effect_size = abs((groups[0].mean() - groups[1].mean()) / pooled_std) if pooled_std > 0 else 0\n",
    "            elif len(groups) > 2:\n",
    "                statistic, p_value = stats.f_oneway(*groups)\n",
    "                test_used = \"ANOVA (reversed)\"\n",
    "                overall_mean = pred_clean.mean()\n",
    "                ss_between = sum(len(g) * (g.mean() - overall_mean)**2 for g in groups)\n",
    "                ss_total = sum((pred_clean - overall_mean)**2)\n",
    "                effect_size = ss_between / ss_total if ss_total > 0 else 0\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "    else:\n",
    "        # Both continuous - Pearson correlation (with Spearman as backup)\n",
    "        try:\n",
    "            # Try Pearson first\n",
    "            r_pearson, p_pearson = pearsonr(pred_clean, outcome_clean)\n",
    "            \n",
    "            # Also calculate Spearman for robustness\n",
    "            r_spearman, p_spearman = spearmanr(pred_clean, outcome_clean)\n",
    "            \n",
    "            # Use Pearson if assumptions are reasonable, otherwise Spearman\n",
    "            if abs(r_pearson) > abs(r_spearman):\n",
    "                statistic, p_value = r_pearson, p_pearson\n",
    "                test_used = \"Pearson correlation\"\n",
    "            else:\n",
    "                statistic, p_value = r_spearman, p_spearman\n",
    "                test_used = \"Spearman correlation\"\n",
    "                \n",
    "            effect_size = abs(statistic)  # Correlation coefficient is the effect size\n",
    "            \n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    return {\n",
    "        'predictor': pred_name,\n",
    "        'outcome': outcome_name,\n",
    "        'test': test_used,\n",
    "        'statistic': statistic,\n",
    "        'p_value': p_value,\n",
    "        'effect_size': effect_size,\n",
    "        'n_cases': len(pred_clean),\n",
    "        'pred_unique_values': pred_unique,\n",
    "        'outcome_unique_values': outcome_unique\n",
    "    }\n",
    "\n",
    "# Run analysis for all predictor-outcome pairs\n",
    "print(\"\\nRunning statistical tests...\")\n",
    "print(\"Progress: \", end=\"\", flush=True)\n",
    "\n",
    "total_tests = len(predictor_cols) * len(outcome_cols)\n",
    "completed_tests = 0\n",
    "\n",
    "for i, predictor in enumerate(predictor_cols):\n",
    "    for j, outcome in enumerate(outcome_cols):\n",
    "        \n",
    "        # Progress indicator\n",
    "        if completed_tests % max(1, total_tests // 20) == 0:\n",
    "            print(\"█\", end=\"\", flush=True)\n",
    "        \n",
    "        result = analyze_relationship(\n",
    "            analysis_data[predictor], \n",
    "            analysis_data[outcome], \n",
    "            predictor, \n",
    "            outcome\n",
    "        )\n",
    "        \n",
    "        if result is not None:\n",
    "            all_relationships.append(result)\n",
    "            \n",
    "            # Store significant relationships (p < 0.05) with sufficient data\n",
    "            if result['p_value'] < 0.05 and not result.get('insufficient_n', False):\n",
    "                significant_relationships.append(result)\n",
    "        \n",
    "        completed_tests += 1\n",
    "\n",
    "print(f\" Complete!\")\n",
    "\n",
    "# Convert to DataFrames for easier analysis\n",
    "all_results_df = pd.DataFrame(all_relationships)\n",
    "significant_results_df = pd.DataFrame(significant_relationships)\n",
    "\n",
    "# Check data availability\n",
    "if len(all_relationships) > 0:\n",
    "    insufficient_data_count = sum(1 for r in all_relationships if r.get('insufficient_n', False))\n",
    "    valid_tests = len(all_relationships) - insufficient_data_count\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"DATA AVAILABILITY:\")\n",
    "    print(f\"=\"*60)\n",
    "    print(f\"Total predictor-outcome pairs: {len(predictor_cols) * len(outcome_cols)}\")\n",
    "    print(f\"Pairs with insufficient data (N < 20): {insufficient_data_count}\")\n",
    "    print(f\"Pairs with sufficient data: {valid_tests}\")\n",
    "    \n",
    "    if valid_tests > 0:\n",
    "        # Show sample sizes for valid tests\n",
    "        valid_relationships = [r for r in all_relationships if not r.get('insufficient_n', False)]\n",
    "        sample_sizes = [r['n_cases'] for r in valid_relationships]\n",
    "        print(f\"Sample size range: {min(sample_sizes)} to {max(sample_sizes)} cases\")\n",
    "        print(f\"Mean sample size: {np.mean(sample_sizes):.1f}\")\n",
    "    \n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"RESULTS SUMMARY:\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Total valid tests conducted: {len(all_relationships)}\")\n",
    "print(f\"Significant relationships (p < 0.05): {len(significant_relationships)}\")\n",
    "print(f\"Significance rate: {len(significant_relationships)/len(all_relationships)*100:.1f}%\")\n",
    "\n",
    "# Apply multiple comparison correction (Bonferroni)\n",
    "if len(all_relationships) > 0:\n",
    "    alpha_bonferroni = 0.05 / len(all_relationships)\n",
    "    bonferroni_significant = [r for r in significant_relationships if r['p_value'] < alpha_bonferroni]\n",
    "    print(f\"Bonferroni corrected significant (α = {alpha_bonferroni:.6f}): {len(bonferroni_significant)}\")\n",
    "    \n",
    "    # Also try False Discovery Rate (FDR) correction - less conservative\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    if len(all_relationships) > 0:\n",
    "        p_values = [r['p_value'] for r in all_relationships]\n",
    "        fdr_rejected, fdr_p_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "        fdr_significant_count = sum(fdr_rejected)\n",
    "        print(f\"FDR corrected significant (α = 0.05): {fdr_significant_count}\")\n",
    "\n",
    "# Display top significant relationships\n",
    "if len(significant_relationships) > 0:\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"TOP SIGNIFICANT RELATIONSHIPS:\")\n",
    "    print(f\"=\"*60)\n",
    "    \n",
    "    # Sort by p-value\n",
    "    sorted_significant = sorted(significant_relationships, key=lambda x: x['p_value'])\n",
    "    \n",
    "    for i, result in enumerate(sorted_significant[:15]):  # Show top 15\n",
    "        pred_clean = result['predictor'].replace('_w1', '')\n",
    "        outcome_clean = result['outcome'].replace('_w23', '')\n",
    "        \n",
    "        print(f\"{i+1:2d}. {pred_clean} → {outcome_clean}\")\n",
    "        print(f\"    {result['test']}: {result['statistic']:.4f}, p = {result['p_value']:.6f}\")\n",
    "        print(f\"    Effect size: {result['effect_size']:.4f}, N = {result['n_cases']}\")\n",
    "        \n",
    "        # Add interpretation for effect size\n",
    "        if 'correlation' in result['test'].lower():\n",
    "            if result['effect_size'] >= 0.5:\n",
    "                size_interpretation = \"(large effect)\"\n",
    "            elif result['effect_size'] >= 0.3:\n",
    "                size_interpretation = \"(medium effect)\"\n",
    "            elif result['effect_size'] >= 0.1:\n",
    "                size_interpretation = \"(small effect)\"\n",
    "            else:\n",
    "                size_interpretation = \"(very small effect)\"\n",
    "        else:\n",
    "            if result['effect_size'] >= 0.8:\n",
    "                size_interpretation = \"(large effect)\"\n",
    "            elif result['effect_size'] >= 0.5:\n",
    "                size_interpretation = \"(medium effect)\"\n",
    "            elif result['effect_size'] >= 0.2:\n",
    "                size_interpretation = \"(small effect)\"\n",
    "            else:\n",
    "                size_interpretation = \"(very small effect)\"\n",
    "        \n",
    "        print(f\"    {size_interpretation}\")\n",
    "        print()\n",
    "\n",
    "# Summary by test type\n",
    "if len(significant_relationships) > 0:\n",
    "    print(f\"BREAKDOWN BY TEST TYPE:\")\n",
    "    print(f\"=\"*30)\n",
    "    test_counts = {}\n",
    "    for result in significant_relationships:\n",
    "        test_type = result['test']\n",
    "        test_counts[test_type] = test_counts.get(test_type, 0) + 1\n",
    "    \n",
    "    for test_type, count in sorted(test_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{test_type}: {count}\")\n",
    "\n",
    "# Store results for future use\n",
    "results_summary = {\n",
    "    'all_results': all_results_df,\n",
    "    'significant_results': significant_results_df,\n",
    "    'bonferroni_significant': bonferroni_significant if len(all_relationships) > 0 else [],\n",
    "    'alpha_bonferroni': alpha_bonferroni if len(all_relationships) > 0 else None,\n",
    "    'fdr_significant_count': fdr_significant_count if len(all_relationships) > 0 else 0\n",
    "}\n",
    "\n",
    "print(f\"\\nResults stored in 'results_summary' dictionary\")\n",
    "print(f\"Access with: results_summary['significant_results'], etc.\")\n",
    "\n",
    "# Save significant results to CSV for external analysis\n",
    "if len(significant_relationships) > 0:\n",
    "    significant_results_df.to_csv('significant_predictor_outcome_relationships.csv', index=False)\n",
    "    print(f\"\\nSignificant results saved to 'significant_predictor_outcome_relationships.csv'\")\n",
    "else:\n",
    "    print(f\"\\nNo significant relationships found to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21ee55",
   "metadata": {},
   "source": [
    "# Visual Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc817b3",
   "metadata": {},
   "source": [
    "#### Outcome = 1 for significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "182e2438",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'longitudinal_sig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30272\\3543222103.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;31m# Create the visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[0mcreate_relationship_plots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlongitudinal_sig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlongitudinal_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'longitudinal_sig' is not defined"
     ]
    }
   ],
   "source": [
    "def create_relationship_plots(significant_df, data_df, max_plots=12):\n",
    "    \"\"\"Create bar plots showing how Wave 1 predictors affect Wave 2-3 outcome probabilities\"\"\"\n",
    "    \n",
    "    if len(significant_df) == 0:\n",
    "        print(\"No significant relationships to plot\")\n",
    "        return\n",
    "    \n",
    "    # Take the most significant relationships\n",
    "    top_relationships = significant_df.nsmallest(max_plots, 'P_Value')\n",
    "    \n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_plots = len(top_relationships)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_plots + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(top_relationships.iterrows()):\n",
    "        predictor = row['Predictor']\n",
    "        outcome = row['Outcome']\n",
    "        p_value = row['P_Value']\n",
    "        odds_ratio = row['Odds_Ratio']\n",
    "        \n",
    "        # Calculate subplot position\n",
    "        row_idx = idx // n_cols\n",
    "        col_idx = idx % n_cols\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        \n",
    "        # Get clean data for this relationship\n",
    "        plot_data = data_df[[predictor, outcome]].dropna()\n",
    "        \n",
    "        if len(plot_data) > 0:\n",
    "            # Calculate outcome probabilities by predictor value\n",
    "            prob_table = plot_data.groupby(predictor)[outcome].agg(['mean', 'count']).reset_index()\n",
    "            prob_table.columns = [predictor, 'outcome_probability', 'sample_size']\n",
    "            \n",
    "            # Create labels\n",
    "            labels = [f\"{predictor}=0\\n(n={prob_table[prob_table[predictor]==0]['sample_size'].iloc[0] if 0 in prob_table[predictor].values else 0})\",\n",
    "                     f\"{predictor}=1\\n(n={prob_table[prob_table[predictor]==1]['sample_size'].iloc[0] if 1 in prob_table[predictor].values else 0})\"]\n",
    "            \n",
    "            # Get probabilities\n",
    "            prob_0 = prob_table[prob_table[predictor]==0]['outcome_probability'].iloc[0] if 0 in prob_table[predictor].values else 0\n",
    "            prob_1 = prob_table[prob_table[predictor]==1]['outcome_probability'].iloc[0] if 1 in prob_table[predictor].values else 0\n",
    "            \n",
    "            probabilities = [prob_0, prob_1]\n",
    "            \n",
    "            # Create bar plot\n",
    "            bars = ax.bar([0, 1], probabilities, color=['lightblue', 'orange'], alpha=0.7)\n",
    "            \n",
    "            # Add percentage labels on bars\n",
    "            for i, (bar, prob) in enumerate(zip(bars, probabilities)):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{prob:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            # Clean outcome name for title\n",
    "            clean_outcome = outcome.replace('_w23', '')\n",
    "            ax.set_title(f'{predictor} → {clean_outcome}\\nOR={odds_ratio:.2f}, p={p_value:.3f}', fontsize=10)\n",
    "            ax.set_ylabel(f'Probability of {clean_outcome}=1')\n",
    "            ax.set_xticks([0, 1])\n",
    "            ax.set_xticklabels(labels)\n",
    "            ax.set_ylim(0, max(probabilities) * 1.2)\n",
    "            \n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(n_plots, n_rows * n_cols):\n",
    "        row_idx = idx // n_cols\n",
    "        col_idx = idx % n_cols\n",
    "        axes[row_idx, col_idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the visualization\n",
    "create_relationship_plots(longitudinal_sig, longitudinal_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f72769",
   "metadata": {},
   "source": [
    "#### Outcome = 0 for significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "68e87891",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'longitudinal_sig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30272\\3940696503.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;31m# Create the visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[0mcreate_relationship_plots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlongitudinal_sig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlongitudinal_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'longitudinal_sig' is not defined"
     ]
    }
   ],
   "source": [
    "def create_relationship_plots(significant_df, data_df, max_plots=12):\n",
    "    \"\"\"Create bar plots showing how Wave 1 predictors affect Wave 2-3 outcome probabilities\"\"\"\n",
    "    \n",
    "    if len(significant_df) == 0:\n",
    "        print(\"No significant relationships to plot\")\n",
    "        return\n",
    "    \n",
    "    # Take the most significant relationships\n",
    "    top_relationships = significant_df.nsmallest(max_plots, 'P_Value')\n",
    "    \n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_plots = len(top_relationships)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_plots + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(top_relationships.iterrows()):\n",
    "        predictor = row['Predictor']\n",
    "        outcome = row['Outcome']\n",
    "        p_value = row['P_Value']\n",
    "        odds_ratio = row['Odds_Ratio']\n",
    "        \n",
    "        # Calculate subplot position\n",
    "        row_idx = idx // n_cols\n",
    "        col_idx = idx % n_cols\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        \n",
    "        # Get clean data for this relationship\n",
    "        plot_data = data_df[[predictor, outcome]].dropna()\n",
    "        \n",
    "        if len(plot_data) > 0:\n",
    "            # Calculate outcome probabilities by predictor value\n",
    "            prob_table = plot_data.groupby(predictor)[outcome].agg(['mean', 'count']).reset_index()\n",
    "            prob_table.columns = [predictor, 'outcome_probability', 'sample_size']\n",
    "            \n",
    "            # Create labels\n",
    "            labels = [f\"{predictor}=0\\n(n={prob_table[prob_table[predictor]==0]['sample_size'].iloc[0] if 0 in prob_table[predictor].values else 0})\",\n",
    "                     f\"{predictor}=1\\n(n={prob_table[prob_table[predictor]==1]['sample_size'].iloc[0] if 1 in prob_table[predictor].values else 0})\"]\n",
    "            \n",
    "            # Get probabilities for outcome=0 (1 - mean gives probability of 0)\n",
    "            prob_0 = 1 - prob_table[prob_table[predictor]==0]['outcome_probability'].iloc[0] if 0 in prob_table[predictor].values else 1\n",
    "            prob_1 = 1 - prob_table[prob_table[predictor]==1]['outcome_probability'].iloc[0] if 1 in prob_table[predictor].values else 1\n",
    "            \n",
    "            probabilities = [prob_0, prob_1]\n",
    "            \n",
    "            # Create bar plot\n",
    "            bars = ax.bar([0, 1], probabilities, color=['lightblue', 'orange'], alpha=0.7)\n",
    "            \n",
    "            # Add percentage labels on bars\n",
    "            for i, (bar, prob) in enumerate(zip(bars, probabilities)):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{prob:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            # Clean outcome name for title\n",
    "            clean_outcome = outcome.replace('_w23', '')\n",
    "            ax.set_title(f'{predictor} → {clean_outcome}\\nOR={odds_ratio:.2f}, p={p_value:.3f}', fontsize=10)\n",
    "            ax.set_ylabel(f'Probability of {clean_outcome}=0')\n",
    "            ax.set_xticks([0, 1])\n",
    "            ax.set_xticklabels(labels)\n",
    "            ax.set_ylim(0, max(probabilities) * 1.2)\n",
    "            \n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(n_plots, n_rows * n_cols):\n",
    "        row_idx = idx // n_cols\n",
    "        col_idx = idx % n_cols\n",
    "        axes[row_idx, col_idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the visualization\n",
    "create_relationship_plots(longitudinal_sig, longitudinal_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33209846",
   "metadata": {},
   "source": [
    "#### Visual of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e45236f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'longitudinal_sig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30272\\3250627142.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;31m# Create the visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m \u001b[0mcreate_relationship_plots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlongitudinal_sig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlongitudinal_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'longitudinal_sig' is not defined"
     ]
    }
   ],
   "source": [
    "def create_relationship_plots(significant_df, data_df, max_plots=12):\n",
    "    \"\"\"Create bar plots showing how Wave 1 predictors affect Wave 2-3 outcome probabilities\"\"\"\n",
    "    \n",
    "    if len(significant_df) == 0:\n",
    "        print(\"No significant relationships to plot\")\n",
    "        return\n",
    "    \n",
    "    # Take the most significant relationships\n",
    "    top_relationships = significant_df.nsmallest(max_plots, 'P_Value')\n",
    "    \n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_plots = len(top_relationships)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_plots + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(top_relationships.iterrows()):\n",
    "        predictor = row['Predictor']\n",
    "        outcome = row['Outcome']\n",
    "        p_value = row['P_Value']\n",
    "        odds_ratio = row['Odds_Ratio']\n",
    "        \n",
    "        # Calculate subplot position\n",
    "        row_idx = idx // n_cols\n",
    "        col_idx = idx % n_cols\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        \n",
    "        # Get clean data for this relationship\n",
    "        plot_data = data_df[[predictor, outcome]].dropna()\n",
    "        \n",
    "        if len(plot_data) > 0:\n",
    "            # Calculate outcome probabilities by predictor value\n",
    "            prob_table = plot_data.groupby(predictor)[outcome].agg(['mean', 'count']).reset_index()\n",
    "            prob_table.columns = [predictor, 'outcome_probability', 'sample_size']\n",
    "            \n",
    "            # Get probabilities for outcome=0 and outcome=1\n",
    "            prob_0_when_pred_0 = prob_table[prob_table[predictor]==0]['outcome_probability'].iloc[0] if 0 in prob_table[predictor].values else 0\n",
    "            prob_1_when_pred_0 = 1 - prob_0_when_pred_0\n",
    "            \n",
    "            prob_0_when_pred_1 = prob_table[prob_table[predictor]==1]['outcome_probability'].iloc[0] if 1 in prob_table[predictor].values else 0\n",
    "            prob_1_when_pred_1 = 1 - prob_0_when_pred_1\n",
    "            \n",
    "            # Sample sizes\n",
    "            n_pred_0 = prob_table[prob_table[predictor]==0]['sample_size'].iloc[0] if 0 in prob_table[predictor].values else 0\n",
    "            n_pred_1 = prob_table[prob_table[predictor]==1]['sample_size'].iloc[0] if 1 in prob_table[predictor].values else 0\n",
    "            \n",
    "            # Set up grouped bars\n",
    "            x_positions = [0, 1]  # predictor = 0, predictor = 1\n",
    "            width = 0.35\n",
    "            \n",
    "            # Probabilities for outcome=0 and outcome=1\n",
    "            prob_outcome_0 = [1 - prob_0_when_pred_0, 1 - prob_0_when_pred_1]  # P(outcome=0)\n",
    "            prob_outcome_1 = [prob_0_when_pred_0, prob_0_when_pred_1]          # P(outcome=1)\n",
    "            \n",
    "            # Create grouped bars\n",
    "            bars1 = ax.bar([x - width/2 for x in x_positions], prob_outcome_0, width, \n",
    "                          label='Outcome=0', color='lightcoral', alpha=0.7)\n",
    "            bars2 = ax.bar([x + width/2 for x in x_positions], prob_outcome_1, width,\n",
    "                          label='Outcome=1', color='lightblue', alpha=0.7)\n",
    "            \n",
    "            # Add percentage labels on bars\n",
    "            for bars, probs in [(bars1, prob_outcome_0), (bars2, prob_outcome_1)]:\n",
    "                for bar, prob in zip(bars, probs):\n",
    "                    height = bar.get_height()\n",
    "                    if height > 0.05:  # Only show label if bar is tall enough\n",
    "                        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                               f'{prob:.1%}', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
    "            \n",
    "            # Clean outcome name for title\n",
    "            clean_outcome = outcome.replace('_w23', '')\n",
    "            ax.set_title(f'{predictor} → {clean_outcome}\\nOR={odds_ratio:.2f}, p={p_value:.3f}', fontsize=10)\n",
    "            ax.set_ylabel(f'Probability')\n",
    "            ax.set_xticks(x_positions)\n",
    "            ax.set_xticklabels([f'{predictor}=0\\n(n={n_pred_0})', f'{predictor}=1\\n(n={n_pred_1})'])\n",
    "            ax.set_ylim(0, 1.1)\n",
    "            ax.legend(fontsize=8)\n",
    "            \n",
    "            # Add grid for easier reading\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(n_plots, n_rows * n_cols):\n",
    "        row_idx = idx // n_cols\n",
    "        col_idx = idx % n_cols\n",
    "        axes[row_idx, col_idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the visualization\n",
    "create_relationship_plots(longitudinal_sig, longitudinal_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598f175",
   "metadata": {},
   "source": [
    "#### Re-run stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f366a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Step 1 - Binary Pattern Analysis for 1's and 0's Relationships\n",
    "def analyze_binary_patterns(data, predictors, outcomes, analysis_name):\n",
    "    \"\"\"\n",
    "    Analyze specific patterns of binary relationships:\n",
    "    - Predictor=1 & Outcome=1\n",
    "    - Predictor=1 & Outcome=0  \n",
    "    - Predictor=0 & Outcome=1\n",
    "    - Predictor=0 & Outcome=0\n",
    "    \"\"\"\n",
    "    \n",
    "    pattern_results = []\n",
    "    \n",
    "    print(f\"\\n=== Binary Pattern Analysis - {analysis_name} ===\")\n",
    "    \n",
    "    for predictor in predictors:\n",
    "        for outcome in outcomes:\n",
    "            if predictor not in data.columns or outcome not in data.columns:\n",
    "                continue\n",
    "                \n",
    "            # Create subset with no missing values\n",
    "            subset = data[[predictor, outcome]].dropna()\n",
    "            \n",
    "            if len(subset) < 10:\n",
    "                continue\n",
    "                \n",
    "            # Ensure both variables are binary (0 or 1)\n",
    "            if set(subset[predictor].unique()) - {0, 1} or set(subset[outcome].unique()) - {0, 1}:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Create crosstab\n",
    "                crosstab = pd.crosstab(subset[predictor], subset[outcome], margins=True)\n",
    "                \n",
    "                if crosstab.shape[0] < 3 or crosstab.shape[1] < 3:  # Need 2x2 + margins\n",
    "                    continue\n",
    "                \n",
    "                # Calculate specific pattern counts and percentages\n",
    "                total_n = len(subset)\n",
    "                \n",
    "                # Pattern counts\n",
    "                pred0_out0 = len(subset[(subset[predictor] == 0) & (subset[outcome] == 0)])\n",
    "                pred0_out1 = len(subset[(subset[predictor] == 0) & (subset[outcome] == 1)])\n",
    "                pred1_out0 = len(subset[(subset[predictor] == 1) & (subset[outcome] == 0)])\n",
    "                pred1_out1 = len(subset[(subset[predictor] == 1) & (subset[outcome] == 1)])\n",
    "                \n",
    "                # Pattern percentages\n",
    "                pred0_out0_pct = (pred0_out0 / total_n) * 100 if total_n > 0 else 0\n",
    "                pred0_out1_pct = (pred0_out1 / total_n) * 100 if total_n > 0 else 0\n",
    "                pred1_out0_pct = (pred1_out0 / total_n) * 100 if total_n > 0 else 0\n",
    "                pred1_out1_pct = (pred1_out1 / total_n) * 100 if total_n > 0 else 0\n",
    "                \n",
    "                # Conditional probabilities\n",
    "                pred0_total = pred0_out0 + pred0_out1\n",
    "                pred1_total = pred1_out0 + pred1_out1\n",
    "                \n",
    "                prob_out1_given_pred0 = pred0_out1 / pred0_total if pred0_total > 0 else 0\n",
    "                prob_out1_given_pred1 = pred1_out1 / pred1_total if pred1_total > 0 else 0\n",
    "                prob_out0_given_pred0 = pred0_out0 / pred0_total if pred0_total > 0 else 0\n",
    "                prob_out0_given_pred1 = pred1_out0 / pred1_total if pred1_total > 0 else 0\n",
    "                \n",
    "                # Chi-square test\n",
    "                chi2, p_value, dof, expected = chi2_contingency(crosstab.iloc[:-1, :-1])\n",
    "                \n",
    "                # Determine dominant pattern\n",
    "                max_pattern_pct = max(pred0_out0_pct, pred0_out1_pct, pred1_out0_pct, pred1_out1_pct)\n",
    "                if max_pattern_pct == pred1_out1_pct:\n",
    "                    dominant_pattern = \"Pred=1 & Out=1\"\n",
    "                elif max_pattern_pct == pred1_out0_pct:\n",
    "                    dominant_pattern = \"Pred=1 & Out=0\"\n",
    "                elif max_pattern_pct == pred0_out1_pct:\n",
    "                    dominant_pattern = \"Pred=0 & Out=1\"\n",
    "                else:\n",
    "                    dominant_pattern = \"Pred=0 & Out=0\"\n",
    "                \n",
    "                pattern_results.append({\n",
    "                    'Analysis': analysis_name,\n",
    "                    'Predictor': predictor,\n",
    "                    'Outcome': outcome,\n",
    "                    'Total_N': total_n,\n",
    "                    'Pred0_Out0_Count': pred0_out0,\n",
    "                    'Pred0_Out0_Pct': round(pred0_out0_pct, 2),\n",
    "                    'Pred0_Out1_Count': pred0_out1,\n",
    "                    'Pred0_Out1_Pct': round(pred0_out1_pct, 2),\n",
    "                    'Pred1_Out0_Count': pred1_out0,\n",
    "                    'Pred1_Out0_Pct': round(pred1_out0_pct, 2),\n",
    "                    'Pred1_Out1_Count': pred1_out1,\n",
    "                    'Pred1_Out1_Pct': round(pred1_out1_pct, 2),\n",
    "                    'Prob_Out1_Given_Pred0': round(prob_out1_given_pred0, 3),\n",
    "                    'Prob_Out1_Given_Pred1': round(prob_out1_given_pred1, 3),\n",
    "                    'Prob_Out0_Given_Pred0': round(prob_out0_given_pred0, 3),\n",
    "                    'Prob_Out0_Given_Pred1': round(prob_out0_given_pred1, 3),\n",
    "                    'Dominant_Pattern': dominant_pattern,\n",
    "                    'Chi2_P_Value': round(p_value, 4),\n",
    "                    'Significant': p_value < 0.05\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in pattern analysis {predictor} -> {outcome}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(pattern_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run binary pattern analysis on longitudinal data\n",
    "longitudinal_patterns = analyze_binary_patterns(longitudinal_clean, long_predictors, long_outcomes, \"Longitudinal_Patterns\")\n",
    "\n",
    "# Filter for significant patterns\n",
    "significant_patterns = longitudinal_patterns[longitudinal_patterns['Significant'] == True].copy()\n",
    "significant_patterns = significant_patterns.sort_values('Chi2_P_Value')\n",
    "\n",
    "print(f\"\\n=== SIGNIFICANT BINARY PATTERNS (p < 0.05) ===\")\n",
    "print(f\"Found {len(significant_patterns)} significant binary patterns\")\n",
    "\n",
    "if len(significant_patterns) > 0:\n",
    "    pattern_display_cols = ['Predictor', 'Outcome', 'Dominant_Pattern', 'Total_N', \n",
    "                           'Pred1_Out1_Pct', 'Pred1_Out0_Pct', 'Pred0_Out1_Pct', 'Pred0_Out0_Pct',\n",
    "                           'Prob_Out1_Given_Pred1', 'Prob_Out1_Given_Pred0', 'Chi2_P_Value']\n",
    "    print(\"\\nSignificant Binary Patterns:\")\n",
    "    print(significant_patterns[pattern_display_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ea788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "longitudinal_patterns.to_csv('binary_pattern_analysis.csv', index=False)\n",
    "significant_patterns.to_csv('significant_binary_patterns.csv', index=False)\n",
    "print(f\"\\nPattern analysis saved to CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45754d9",
   "metadata": {},
   "source": [
    "#### Higher level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Step 2 - Wave1 Predictors Causing Higher Levels in Wave23 Outcomes\n",
    "def analyze_wave1_to_wave23_increases(longitudinal_data, wave1_predictors, wave23_outcomes):\n",
    "    \"\"\"\n",
    "    Analyze how Wave1 predictors lead to higher levels in Wave23 outcomes\n",
    "    Focus on increases/improvements from Wave1 to Wave23\n",
    "    \"\"\"\n",
    "    \n",
    "    increase_results = []\n",
    "    \n",
    "    print(f\"\\n=== Wave1 → Wave23 Higher Level Analysis ===\")\n",
    "    \n",
    "    for predictor in wave1_predictors:\n",
    "        for outcome in wave23_outcomes:\n",
    "            if predictor not in longitudinal_data.columns or outcome not in longitudinal_data.columns:\n",
    "                continue\n",
    "                \n",
    "            # Create subset with no missing values\n",
    "            subset = longitudinal_data[[predictor, outcome]].dropna()\n",
    "            \n",
    "            if len(subset) < 10:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Group by predictor and calculate outcome statistics\n",
    "                grouped_stats = subset.groupby(predictor)[outcome].agg(['count', 'mean', 'std', 'median']).reset_index()\n",
    "                \n",
    "                if len(grouped_stats) < 2:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate effect sizes and statistical tests\n",
    "                pred_0_group = subset[subset[predictor] == 0][outcome]\n",
    "                pred_1_group = subset[subset[predictor] == 1][outcome]\n",
    "                \n",
    "                if len(pred_0_group) < 5 or len(pred_1_group) < 5:\n",
    "                    continue\n",
    "                \n",
    "                # Mean difference (higher is better)\n",
    "                mean_diff = pred_1_group.mean() - pred_0_group.mean()\n",
    "                \n",
    "                # Percentage increase\n",
    "                pct_increase = ((pred_1_group.mean() - pred_0_group.mean()) / pred_0_group.mean() * 100) if pred_0_group.mean() != 0 else 0\n",
    "                \n",
    "                # T-test for mean differences\n",
    "                from scipy.stats import ttest_ind\n",
    "                t_stat, t_p_value = ttest_ind(pred_1_group, pred_0_group)\n",
    "                \n",
    "                # Mann-Whitney U test (non-parametric)\n",
    "                from scipy.stats import mannwhitneyu\n",
    "                try:\n",
    "                    u_stat, u_p_value = mannwhitneyu(pred_1_group, pred_0_group, alternative='two-sided')\n",
    "                except:\n",
    "                    u_p_value = np.nan\n",
    "                \n",
    "                # Effect size (Cohen's d)\n",
    "                pooled_std = np.sqrt(((len(pred_0_group) - 1) * pred_0_group.std()**2 + \n",
    "                                    (len(pred_1_group) - 1) * pred_1_group.std()**2) / \n",
    "                                   (len(pred_0_group) + len(pred_1_group) - 2))\n",
    "                cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0\n",
    "                \n",
    "                # Determine effect magnitude\n",
    "                if abs(cohens_d) < 0.2:\n",
    "                    effect_magnitude = \"Small\"\n",
    "                elif abs(cohens_d) < 0.5:\n",
    "                    effect_magnitude = \"Small-Medium\"\n",
    "                elif abs(cohens_d) < 0.8:\n",
    "                    effect_magnitude = \"Medium-Large\"\n",
    "                else:\n",
    "                    effect_magnitude = \"Large\"\n",
    "                \n",
    "                # Positive effect indicator\n",
    "                positive_effect = mean_diff > 0\n",
    "                \n",
    "                increase_results.append({\n",
    "                    'Predictor_Wave1': predictor,\n",
    "                    'Outcome_Wave23': outcome,\n",
    "                    'Total_N': len(subset),\n",
    "                    'Pred0_N': len(pred_0_group),\n",
    "                    'Pred1_N': len(pred_1_group),\n",
    "                    'Pred0_Mean': round(pred_0_group.mean(), 3),\n",
    "                    'Pred1_Mean': round(pred_1_group.mean(), 3),\n",
    "                    'Mean_Difference': round(mean_diff, 3),\n",
    "                    'Percent_Increase': round(pct_increase, 2),\n",
    "                    'Cohens_D': round(cohens_d, 3),\n",
    "                    'Effect_Magnitude': effect_magnitude,\n",
    "                    'Positive_Effect': positive_effect,\n",
    "                    'T_Test_P_Value': round(t_p_value, 4),\n",
    "                    'Mann_Whitney_P_Value': round(u_p_value, 4) if not np.isnan(u_p_value) else np.nan,\n",
    "                    'T_Test_Significant': t_p_value < 0.05,\n",
    "                    'MW_Test_Significant': u_p_value < 0.05 if not np.isnan(u_p_value) else False,\n",
    "                    'Overall_Significant': t_p_value < 0.05 or (u_p_value < 0.05 if not np.isnan(u_p_value) else False)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in increase analysis {predictor} -> {outcome}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(increase_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad50aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Wave1 → Wave23 increase analysis\n",
    "wave1_to_wave23_increases = analyze_wave1_to_wave23_increases(longitudinal_clean, long_predictors, long_outcomes)\n",
    "\n",
    "# Filter for significant increases\n",
    "significant_increases = wave1_to_wave23_increases[wave1_to_wave23_increases['Overall_Significant'] == True].copy()\n",
    "\n",
    "# Sort by effect size (largest positive effects first)\n",
    "significant_increases = significant_increases.sort_values(['Positive_Effect', 'Cohens_D'], ascending=[False, False])\n",
    "\n",
    "print(f\"\\n=== SIGNIFICANT WAVE1 → WAVE23 INCREASES ===\")\n",
    "print(f\"Found {len(significant_increases)} significant relationships\")\n",
    "\n",
    "if len(significant_increases) > 0:\n",
    "    increase_display_cols = ['Predictor_Wave1', 'Outcome_Wave23', 'Mean_Difference', 'Percent_Increase', \n",
    "                            'Cohens_D', 'Effect_Magnitude', 'Positive_Effect', 'T_Test_P_Value', 'Total_N']\n",
    "    print(\"\\nSignificant Wave1 → Wave23 Increases:\")\n",
    "    print(significant_increases[increase_display_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405973ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "wave1_to_wave23_increases.to_csv('wave1_to_wave23_increases.csv', index=False)\n",
    "significant_increases.to_csv('significant_wave1_to_wave23_increases.csv', index=False)\n",
    "print(f\"\\nWave1 → Wave23 analysis saved to CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556015d4",
   "metadata": {},
   "source": [
    "### Multiple way predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e8fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Step 3 - REWRITTEN Multiple Predictor Analysis\n",
    "def multiple_predictor_analysis(data, predictors, outcomes, analysis_name, max_predictors=5):\n",
    "    \"\"\"\n",
    "    Rewritten multiple predictor analysis with robust error handling\n",
    "    \"\"\"\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # Initialize results list\n",
    "    multi_results = []\n",
    "    \n",
    "    print(f\"\\n=== Multiple Predictor Analysis - {analysis_name} ===\")\n",
    "    print(f\"Testing {len(predictors)} predictors against {len(outcomes)} outcomes\")\n",
    "    \n",
    "    # Process each outcome\n",
    "    for outcome_idx, outcome in enumerate(outcomes):\n",
    "        if outcome not in data.columns:\n",
    "            print(f\"Outcome {outcome} not found in data columns\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n[{outcome_idx+1}/{len(outcomes)}] Analyzing outcome: {outcome}\")\n",
    "        \n",
    "        # Get available predictors (excluding the outcome itself)\n",
    "        available_predictors = [p for p in predictors if p in data.columns and p != outcome]\n",
    "        \n",
    "        if len(available_predictors) < 2:\n",
    "            print(f\"  Not enough predictors available: {len(available_predictors)}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Available predictors: {len(available_predictors)}\")\n",
    "        \n",
    "        # Create clean dataset\n",
    "        required_cols = available_predictors + [outcome]\n",
    "        subset = data[required_cols].dropna()\n",
    "        \n",
    "        if len(subset) < 30:\n",
    "            print(f\"  Insufficient data after cleaning: {len(subset)} rows\")\n",
    "            continue\n",
    "            \n",
    "        # Check outcome distribution\n",
    "        outcome_counts = subset[outcome].value_counts()\n",
    "        if len(outcome_counts) != 2:\n",
    "            print(f\"  Non-binary outcome: {outcome_counts.to_dict()}\")\n",
    "            continue\n",
    "            \n",
    "        if min(outcome_counts) < 10:\n",
    "            print(f\"  Insufficient minority class: {outcome_counts.to_dict()}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Clean dataset: {len(subset)} rows, outcome distribution: {outcome_counts.to_dict()}\")\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = subset[available_predictors]\n",
    "        y = subset[outcome]\n",
    "        \n",
    "        # Standardize features\n",
    "        try:\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in scaling: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        # === 1. FULL MODEL WITH ALL PREDICTORS ===\n",
    "        if len(available_predictors) <= max_predictors:\n",
    "            try:\n",
    "                print(f\"    Testing full model with {len(available_predictors)} predictors\")\n",
    "                \n",
    "                # Fit logistic regression\n",
    "                lr_full = LogisticRegression(random_state=42, max_iter=1000)\n",
    "                lr_full.fit(X_scaled, y)\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores_auc = cross_val_score(lr_full, X_scaled, y, cv=3, scoring='roc_auc')\n",
    "                cv_scores_acc = cross_val_score(lr_full, X_scaled, y, cv=3, scoring='accuracy')\n",
    "                \n",
    "                # Feature importance\n",
    "                coefficients = lr_full.coef_[0]\n",
    "                odds_ratios = np.exp(coefficients)\n",
    "                \n",
    "                # Sort features by absolute coefficient\n",
    "                feature_importance = list(zip(available_predictors, coefficients, odds_ratios))\n",
    "                feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "                \n",
    "                # Get performance metrics\n",
    "                y_pred_proba = lr_full.predict_proba(X_scaled)[:, 1]\n",
    "                auc_score = roc_auc_score(y, y_pred_proba)\n",
    "                \n",
    "                # Store result\n",
    "                result_entry = {\n",
    "                    'Analysis': analysis_name,\n",
    "                    'Outcome': outcome,\n",
    "                    'Model_Type': 'Full_Multiple_Logistic',\n",
    "                    'Num_Predictors': len(available_predictors),\n",
    "                    'Predictor_List': ', '.join(available_predictors),\n",
    "                    'Sample_Size': len(subset),\n",
    "                    'AUC_Score': round(auc_score, 4),\n",
    "                    'CV_AUC_Mean': round(cv_scores_auc.mean(), 4),\n",
    "                    'CV_AUC_Std': round(cv_scores_auc.std(), 4),\n",
    "                    'CV_Accuracy_Mean': round(cv_scores_acc.mean(), 4),\n",
    "                    'Model_Performance': 'Excellent' if auc_score > 0.8 else 'Good' if auc_score > 0.7 else 'Fair' if auc_score > 0.6 else 'Poor'\n",
    "                }\n",
    "                \n",
    "                # Add top 3 predictors\n",
    "                for i in range(min(3, len(feature_importance))):\n",
    "                    pred_name, coef, odds_ratio = feature_importance[i]\n",
    "                    result_entry[f'Top_Predictor_{i+1}'] = pred_name\n",
    "                    result_entry[f'Top_Predictor_{i+1}_Coef'] = round(coef, 4)\n",
    "                    result_entry[f'Top_Predictor_{i+1}_OR'] = round(odds_ratio, 4)\n",
    "                \n",
    "                # Fill empty slots if fewer than 3 predictors\n",
    "                for i in range(len(feature_importance), 3):\n",
    "                    result_entry[f'Top_Predictor_{i+1}'] = ''\n",
    "                    result_entry[f'Top_Predictor_{i+1}_Coef'] = ''\n",
    "                    result_entry[f'Top_Predictor_{i+1}_OR'] = ''\n",
    "                \n",
    "                multi_results.append(result_entry)\n",
    "                print(f\"    Full model AUC: {auc_score:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error in full model: {str(e)}\")\n",
    "        \n",
    "        # === 2. BEST 2-PREDICTOR COMBINATION ===\n",
    "        if len(available_predictors) >= 2:\n",
    "            try:\n",
    "                print(f\"    Testing 2-predictor combinations\")\n",
    "                \n",
    "                best_auc = 0\n",
    "                best_combo = None\n",
    "                best_model = None\n",
    "                \n",
    "                # Test all 2-predictor combinations\n",
    "                for combo in itertools.combinations(available_predictors, 2):\n",
    "                    try:\n",
    "                        combo_indices = [available_predictors.index(pred) for pred in combo]\n",
    "                        X_combo = X_scaled[:, combo_indices]\n",
    "                        \n",
    "                        lr_combo = LogisticRegression(random_state=42, max_iter=1000)\n",
    "                        lr_combo.fit(X_combo, y)\n",
    "                        \n",
    "                        y_pred_proba_combo = lr_combo.predict_proba(X_combo)[:, 1]\n",
    "                        auc_combo = roc_auc_score(y, y_pred_proba_combo)\n",
    "                        \n",
    "                        if auc_combo > best_auc:\n",
    "                            best_auc = auc_combo\n",
    "                            best_combo = combo\n",
    "                            best_model = lr_combo\n",
    "                            \n",
    "                    except Exception:\n",
    "                        continue\n",
    "                \n",
    "                if best_combo and best_model:\n",
    "                    # Cross-validation for best combo\n",
    "                    combo_indices = [available_predictors.index(pred) for pred in best_combo]\n",
    "                    X_best_combo = X_scaled[:, combo_indices]\n",
    "                    \n",
    "                    cv_scores_auc_combo = cross_val_score(best_model, X_best_combo, y, cv=3, scoring='roc_auc')\n",
    "                    cv_scores_acc_combo = cross_val_score(best_model, X_best_combo, y, cv=3, scoring='accuracy')\n",
    "                    \n",
    "                    result_entry = {\n",
    "                        'Analysis': analysis_name,\n",
    "                        'Outcome': outcome,\n",
    "                        'Model_Type': 'Best_2_Predictor_Combo',\n",
    "                        'Num_Predictors': 2,\n",
    "                        'Predictor_List': ', '.join(best_combo),\n",
    "                        'Sample_Size': len(subset),\n",
    "                        'AUC_Score': round(best_auc, 4),\n",
    "                        'CV_AUC_Mean': round(cv_scores_auc_combo.mean(), 4),\n",
    "                        'CV_AUC_Std': round(cv_scores_auc_combo.std(), 4),\n",
    "                        'CV_Accuracy_Mean': round(cv_scores_acc_combo.mean(), 4),\n",
    "                        'Model_Performance': 'Excellent' if best_auc > 0.8 else 'Good' if best_auc > 0.7 else 'Fair' if best_auc > 0.6 else 'Poor',\n",
    "                        'Top_Predictor_1': best_combo[0],\n",
    "                        'Top_Predictor_1_Coef': round(best_model.coef_[0][0], 4),\n",
    "                        'Top_Predictor_1_OR': round(np.exp(best_model.coef_[0][0]), 4),\n",
    "                        'Top_Predictor_2': best_combo[1],\n",
    "                        'Top_Predictor_2_Coef': round(best_model.coef_[0][1], 4),\n",
    "                        'Top_Predictor_2_OR': round(np.exp(best_model.coef_[0][1]), 4),\n",
    "                        'Top_Predictor_3': '',\n",
    "                        'Top_Predictor_3_Coef': '',\n",
    "                        'Top_Predictor_3_OR': ''\n",
    "                    }\n",
    "                    \n",
    "                    multi_results.append(result_entry)\n",
    "                    print(f\"    Best 2-predictor combo AUC: {best_auc:.4f} ({best_combo})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error in 2-predictor analysis: {str(e)}\")\n",
    "        \n",
    "        # === 3. BEST 3-PREDICTOR COMBINATION ===\n",
    "        if len(available_predictors) >= 3:\n",
    "            try:\n",
    "                print(f\"    Testing 3-predictor combinations\")\n",
    "                \n",
    "                best_auc_3 = 0\n",
    "                best_combo_3 = None\n",
    "                best_model_3 = None\n",
    "                \n",
    "                # Limit combinations to avoid excessive computation\n",
    "                all_3_combos = list(itertools.combinations(available_predictors, 3))\n",
    "                if len(all_3_combos) > 20:\n",
    "                    import random\n",
    "                    random.seed(42)\n",
    "                    test_combos = random.sample(all_3_combos, 20)\n",
    "                else:\n",
    "                    test_combos = all_3_combos\n",
    "                \n",
    "                for combo in test_combos:\n",
    "                    try:\n",
    "                        combo_indices = [available_predictors.index(pred) for pred in combo]\n",
    "                        X_combo = X_scaled[:, combo_indices]\n",
    "                        \n",
    "                        lr_combo = LogisticRegression(random_state=42, max_iter=1000)\n",
    "                        lr_combo.fit(X_combo, y)\n",
    "                        \n",
    "                        y_pred_proba_combo = lr_combo.predict_proba(X_combo)[:, 1]\n",
    "                        auc_combo = roc_auc_score(y, y_pred_proba_combo)\n",
    "                        \n",
    "                        if auc_combo > best_auc_3:\n",
    "                            best_auc_3 = auc_combo\n",
    "                            best_combo_3 = combo\n",
    "                            best_model_3 = lr_combo\n",
    "                            \n",
    "                    except Exception:\n",
    "                        continue\n",
    "                \n",
    "                if best_combo_3 and best_model_3:\n",
    "                    combo_indices = [available_predictors.index(pred) for pred in best_combo_3]\n",
    "                    X_best_combo_3 = X_scaled[:, combo_indices]\n",
    "                    \n",
    "                    cv_scores_auc_combo3 = cross_val_score(best_model_3, X_best_combo_3, y, cv=3, scoring='roc_auc')\n",
    "                    cv_scores_acc_combo3 = cross_val_score(best_model_3, X_best_combo_3, y, cv=3, scoring='accuracy')\n",
    "                    \n",
    "                    result_entry = {\n",
    "                        'Analysis': analysis_name,\n",
    "                        'Outcome': outcome,\n",
    "                        'Model_Type': 'Best_3_Predictor_Combo',\n",
    "                        'Num_Predictors': 3,\n",
    "                        'Predictor_List': ', '.join(best_combo_3),\n",
    "                        'Sample_Size': len(subset),\n",
    "                        'AUC_Score': round(best_auc_3, 4),\n",
    "                        'CV_AUC_Mean': round(cv_scores_auc_combo3.mean(), 4),\n",
    "                        'CV_AUC_Std': round(cv_scores_auc_combo3.std(), 4),\n",
    "                        'CV_Accuracy_Mean': round(cv_scores_acc_combo3.mean(), 4),\n",
    "                        'Model_Performance': 'Excellent' if best_auc_3 > 0.8 else 'Good' if best_auc_3 > 0.7 else 'Fair' if best_auc_3 > 0.6 else 'Poor',\n",
    "                        'Top_Predictor_1': best_combo_3[0],\n",
    "                        'Top_Predictor_1_Coef': round(best_model_3.coef_[0][0], 4),\n",
    "                        'Top_Predictor_1_OR': round(np.exp(best_model_3.coef_[0][0]), 4),\n",
    "                        'Top_Predictor_2': best_combo_3[1],\n",
    "                        'Top_Predictor_2_Coef': round(best_model_3.coef_[0][1], 4),\n",
    "                        'Top_Predictor_2_OR': round(np.exp(best_model_3.coef_[0][1]), 4),\n",
    "                        'Top_Predictor_3': best_combo_3[2],\n",
    "                        'Top_Predictor_3_Coef': round(best_model_3.coef_[0][2], 4),\n",
    "                        'Top_Predictor_3_OR': round(np.exp(best_model_3.coef_[0][2]), 4)\n",
    "                    }\n",
    "                    \n",
    "                    multi_results.append(result_entry)\n",
    "                    print(f\"    Best 3-predictor combo AUC: {best_auc_3:.4f} ({best_combo_3})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error in 3-predictor analysis: {str(e)}\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    if multi_results:\n",
    "        results_df = pd.DataFrame(multi_results)\n",
    "        print(f\"\\n=== Analysis Complete ===\")\n",
    "        print(f\"Generated {len(results_df)} model results\")\n",
    "        return results_df\n",
    "    else:\n",
    "        print(f\"\\n=== Analysis Complete ===\")\n",
    "        print(\"No valid models generated\")\n",
    "        # Return empty DataFrame with expected columns\n",
    "        empty_df = pd.DataFrame(columns=[\n",
    "            'Analysis', 'Outcome', 'Model_Type', 'Num_Predictors', 'Predictor_List', 'Sample_Size',\n",
    "            'AUC_Score', 'CV_AUC_Mean', 'CV_AUC_Std', 'CV_Accuracy_Mean', 'Model_Performance',\n",
    "            'Top_Predictor_1', 'Top_Predictor_1_Coef', 'Top_Predictor_1_OR',\n",
    "            'Top_Predictor_2', 'Top_Predictor_2_Coef', 'Top_Predictor_2_OR',\n",
    "            'Top_Predictor_3', 'Top_Predictor_3_Coef', 'Top_Predictor_3_OR'\n",
    "        ])\n",
    "        return empty_df\n",
    "\n",
    "# Now run the rewritten analysis\n",
    "print(\"=== RUNNING REWRITTEN MULTIPLE PREDICTOR ANALYSIS ===\")\n",
    "\n",
    "# Run multiple predictor analysis\n",
    "multiple_pred_results = multiple_predictor_analysis(longitudinal_clean, long_predictors, long_outcomes, \"Longitudinal_Multiple_Predictors\")\n",
    "\n",
    "# Check results\n",
    "print(f\"\\nResults DataFrame shape: {multiple_pred_results.shape}\")\n",
    "print(f\"Results DataFrame columns: {list(multiple_pred_results.columns)}\")\n",
    "\n",
    "# Filter and display results\n",
    "if len(multiple_pred_results) > 0:\n",
    "    \n",
    "    # Filter for good performing models (AUC > 0.6)\n",
    "    good_models = multiple_pred_results[multiple_pred_results['AUC_Score'] > 0.6].copy()\n",
    "    good_models = good_models.sort_values('AUC_Score', ascending=False)\n",
    "    \n",
    "    print(f\"\\n=== MULTIPLE PREDICTOR ANALYSIS RESULTS ===\")\n",
    "    print(f\"Total models tested: {len(multiple_pred_results)}\")\n",
    "    print(f\"Good performing models (AUC > 0.6): {len(good_models)}\")\n",
    "    \n",
    "    # Display all results first\n",
    "    print(f\"\\n=== ALL MODEL RESULTS ===\")\n",
    "    all_display_cols = ['Outcome', 'Model_Type', 'Num_Predictors', 'AUC_Score', 'CV_AUC_Mean', 'Model_Performance', 'Sample_Size']\n",
    "    print(multiple_pred_results[all_display_cols].round(3))\n",
    "    \n",
    "    if len(good_models) > 0:\n",
    "        multi_display_cols = ['Outcome', 'Model_Type', 'Num_Predictors', 'AUC_Score', 'Model_Performance',\n",
    "                             'Top_Predictor_1', 'Top_Predictor_1_OR', 'Top_Predictor_2', 'Top_Predictor_2_OR', 'Sample_Size']\n",
    "        print(f\"\\n=== GOOD PERFORMING MODELS (AUC > 0.6) ===\")\n",
    "        print(good_models[multi_display_cols].round(3))\n",
    "        \n",
    "        # Summary of best models by outcome\n",
    "        print(f\"\\n=== BEST MODEL PER OUTCOME ===\")\n",
    "        best_by_outcome = good_models.loc[good_models.groupby('Outcome')['AUC_Score'].idxmax()]\n",
    "        if len(best_by_outcome) > 0:\n",
    "            summary_cols = ['Outcome', 'Model_Type', 'AUC_Score', 'Predictor_List', 'Model_Performance']\n",
    "            print(best_by_outcome[summary_cols].round(3))\n",
    "    else:\n",
    "        print(\"\\nNo models achieved AUC > 0.6\")\n",
    "        print(\"Best performing models:\")\n",
    "        if len(multiple_pred_results) > 0:\n",
    "            best_models = multiple_pred_results.nlargest(5, 'AUC_Score')\n",
    "            print(best_models[all_display_cols].round(3))\n",
    "    \n",
    "    # Save results\n",
    "    multiple_pred_results.to_csv('multiple_predictor_analysis.csv', index=False)\n",
    "    if len(good_models) > 0:\n",
    "        good_models.to_csv('good_multiple_predictor_models.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to CSV files\")\n",
    "    \n",
    "else:\n",
    "    print(\"No results generated - check data compatibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d022cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: DIAGNOSTIC - Investigate Data Issues\n",
    "def diagnose_longitudinal_data_issues(longitudinal_clean, long_predictors, long_outcomes):\n",
    "    \"\"\"\n",
    "    Diagnose why we have insufficient data for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== LONGITUDINAL DATA DIAGNOSTIC ===\")\n",
    "    print(f\"Longitudinal dataset shape: {longitudinal_clean.shape}\")\n",
    "    print(f\"Predictors to analyze: {len(long_predictors)}\")\n",
    "    print(f\"Outcomes to analyze: {len(long_outcomes)}\")\n",
    "    \n",
    "    # Check if predictors and outcomes exist in the data\n",
    "    print(f\"\\n=== COLUMN AVAILABILITY CHECK ===\")\n",
    "    missing_predictors = [p for p in long_predictors if p not in longitudinal_clean.columns]\n",
    "    missing_outcomes = [o for o in long_outcomes if o not in longitudinal_clean.columns]\n",
    "    \n",
    "    print(f\"Missing predictors: {missing_predictors}\")\n",
    "    print(f\"Missing outcomes: {missing_outcomes}\")\n",
    "    \n",
    "    # Check available columns\n",
    "    available_predictors = [p for p in long_predictors if p in longitudinal_clean.columns]\n",
    "    available_outcomes = [o for o in long_outcomes if o in longitudinal_clean.columns]\n",
    "    \n",
    "    print(f\"Available predictors ({len(available_predictors)}): {available_predictors}\")\n",
    "    print(f\"Available outcomes ({len(available_outcomes)}): {available_outcomes}\")\n",
    "    \n",
    "    # Check missing value patterns\n",
    "    print(f\"\\n=== MISSING VALUE ANALYSIS ===\")\n",
    "    \n",
    "    if available_predictors:\n",
    "        print(\"Predictor missing value counts:\")\n",
    "        for pred in available_predictors:\n",
    "            missing_count = longitudinal_clean[pred].isna().sum()\n",
    "            missing_pct = (missing_count / len(longitudinal_clean)) * 100\n",
    "            print(f\"  {pred}: {missing_count} missing ({missing_pct:.1f}%)\")\n",
    "    \n",
    "    if available_outcomes:\n",
    "        print(\"\\nOutcome missing value counts:\")\n",
    "        for outcome in available_outcomes:\n",
    "            missing_count = longitudinal_clean[outcome].isna().sum()\n",
    "            missing_pct = (missing_count / len(longitudinal_clean)) * 100\n",
    "            print(f\"  {outcome}: {missing_count} missing ({missing_pct:.1f}%)\")\n",
    "    \n",
    "    # Check complete cases for each outcome\n",
    "    print(f\"\\n=== COMPLETE CASES ANALYSIS ===\")\n",
    "    \n",
    "    for outcome in available_outcomes:\n",
    "        print(f\"\\nAnalyzing complete cases for outcome: {outcome}\")\n",
    "        \n",
    "        # Required columns for this outcome\n",
    "        required_cols = available_predictors + [outcome]\n",
    "        \n",
    "        # Check data availability step by step\n",
    "        print(f\"  Total rows in dataset: {len(longitudinal_clean)}\")\n",
    "        \n",
    "        # Check each column individually\n",
    "        individual_counts = {}\n",
    "        for col in required_cols:\n",
    "            non_missing = longitudinal_clean[col].notna().sum()\n",
    "            individual_counts[col] = non_missing\n",
    "            print(f\"  {col}: {non_missing} non-missing values\")\n",
    "        \n",
    "        # Check complete cases\n",
    "        complete_cases = longitudinal_clean[required_cols].dropna()\n",
    "        print(f\"  Complete cases (all columns): {len(complete_cases)}\")\n",
    "        \n",
    "        if len(complete_cases) > 0:\n",
    "            print(f\"  Sample of complete cases:\")\n",
    "            print(complete_cases.head())\n",
    "        else:\n",
    "            print(\"  No complete cases found!\")\n",
    "            \n",
    "            # Find which combinations have the most data\n",
    "            print(\"  Checking pairwise combinations...\")\n",
    "            for pred in available_predictors[:5]:  # Check first 5 predictors\n",
    "                pair_complete = longitudinal_clean[[pred, outcome]].dropna()\n",
    "                print(f\"    {pred} + {outcome}: {len(pair_complete)} complete cases\")\n",
    "\n",
    "# Run diagnostic\n",
    "diagnose_longitudinal_data_issues(longitudinal_clean, long_predictors, long_outcomes)\n",
    "\n",
    "# Let's also check the original longitudinal_data before preprocessing\n",
    "print(f\"\\n=== ORIGINAL LONGITUDINAL DATA CHECK ===\")\n",
    "print(f\"Original longitudinal_data shape: {longitudinal_data.shape}\")\n",
    "print(f\"Original columns: {list(longitudinal_data.columns)}\")\n",
    "\n",
    "# Check if the preprocessing step removed too much data\n",
    "print(f\"\\n=== PREPROCESSING IMPACT ===\")\n",
    "print(\"Original data missing values:\")\n",
    "for col in longitudinal_data.columns:\n",
    "    if col in long_predictors + long_outcomes:\n",
    "        missing_count = longitudinal_data[col].isna().sum()\n",
    "        missing_pct = (missing_count / len(longitudinal_data)) * 100\n",
    "        print(f\"  {col}: {missing_count} missing ({missing_pct:.1f}%)\")\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\n=== DATA TYPES CHECK ===\")\n",
    "for col in longitudinal_data.columns:\n",
    "    if col in long_predictors + long_outcomes:\n",
    "        dtype = longitudinal_data[col].dtype\n",
    "        unique_vals = longitudinal_data[col].dropna().unique()\n",
    "        print(f\"  {col}: {dtype}, unique values: {unique_vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: SIMPLIFIED ANALYSIS - Work with Available Data\n",
    "def simplified_longitudinal_analysis(longitudinal_data, predictors, outcomes):\n",
    "    \"\"\"\n",
    "    Simplified analysis that works with whatever data we have\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== SIMPLIFIED LONGITUDINAL ANALYSIS ===\")\n",
    "    \n",
    "    # Use original data instead of preprocessed\n",
    "    data = longitudinal_data.copy()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for outcome in outcomes:\n",
    "        if outcome not in data.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nAnalyzing outcome: {outcome}\")\n",
    "        \n",
    "        for predictor in predictors:\n",
    "            if predictor not in data.columns:\n",
    "                continue\n",
    "                \n",
    "            # Simple pairwise analysis\n",
    "            pair_data = data[[predictor, outcome]].dropna()\n",
    "            \n",
    "            if len(pair_data) < 5:\n",
    "                continue\n",
    "                \n",
    "            print(f\"  {predictor} -> {outcome}: {len(pair_data)} cases\")\n",
    "            \n",
    "            # Check if both variables are binary\n",
    "            pred_unique = set(pair_data[predictor].unique())\n",
    "            out_unique = set(pair_data[outcome].unique())\n",
    "            \n",
    "            if pred_unique.issubset({0, 1}) and out_unique.issubset({0, 1}):\n",
    "                # Binary analysis\n",
    "                crosstab = pd.crosstab(pair_data[predictor], pair_data[outcome])\n",
    "                print(f\"    Crosstab:\\n{crosstab}\")\n",
    "                \n",
    "                # Chi-square test\n",
    "                from scipy.stats import chi2_contingency\n",
    "                if crosstab.shape == (2, 2):\n",
    "                    chi2, p_val, dof, expected = chi2_contingency(crosstab)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Predictor': predictor,\n",
    "                        'Outcome': outcome,\n",
    "                        'Sample_Size': len(pair_data),\n",
    "                        'Chi2_Stat': round(chi2, 4),\n",
    "                        'P_Value': round(p_val, 4),\n",
    "                        'Significant': p_val < 0.05,\n",
    "                        'Crosstab': crosstab.to_dict()\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"    Chi2: {chi2:.4f}, p-value: {p_val:.4f}\")\n",
    "    \n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        significant_results = results_df[results_df['Significant'] == True]\n",
    "        \n",
    "        print(f\"\\n=== SIMPLIFIED RESULTS ===\")\n",
    "        print(f\"Total tests: {len(results_df)}\")\n",
    "        print(f\"Significant results: {len(significant_results)}\")\n",
    "        \n",
    "        if len(significant_results) > 0:\n",
    "            display_cols = ['Predictor', 'Outcome', 'Sample_Size', 'Chi2_Stat', 'P_Value']\n",
    "            print(\"\\nSignificant relationships:\")\n",
    "            print(significant_results[display_cols])\n",
    "            \n",
    "        return results_df\n",
    "    else:\n",
    "        print(\"No valid pairwise comparisons found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Run simplified analysis\n",
    "simplified_results = simplified_longitudinal_analysis(longitudinal_data, long_predictors, long_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feaaa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: CHECK DATA MERGE PROCESS\n",
    "print(\"=== CHECKING DATA MERGE PROCESS ===\")\n",
    "\n",
    "# Let's examine the original wave data\n",
    "print(f\"Wave1 shape: {wave1.shape}\")\n",
    "print(f\"Wave23 shape: {wave23.shape}\")\n",
    "\n",
    "# Check StFCID availability\n",
    "print(f\"\\nStFCID in wave1: {'StFCID' in wave1.columns}\")\n",
    "print(f\"StFCID in wave23: {'StFCID' in wave23.columns}\")\n",
    "\n",
    "if 'StFCID' in wave1.columns and 'StFCID' in wave23.columns:\n",
    "    wave1_ids = set(wave1['StFCID'].dropna())\n",
    "    wave23_ids = set(wave23['StFCID'].dropna())\n",
    "    \n",
    "    print(f\"Unique IDs in wave1: {len(wave1_ids)}\")\n",
    "    print(f\"Unique IDs in wave23: {len(wave23_ids)}\")\n",
    "    print(f\"Overlapping IDs: {len(wave1_ids.intersection(wave23_ids))}\")\n",
    "    \n",
    "    # Check if the merge is working\n",
    "    if len(wave1_ids.intersection(wave23_ids)) == 0:\n",
    "        print(\"WARNING: No overlapping IDs - merge will fail!\")\n",
    "        print(\"Sample wave1 IDs:\", list(wave1_ids)[:5])\n",
    "        print(\"Sample wave23 IDs:\", list(wave23_ids)[:5])\n",
    "\n",
    "# Check predictor and outcome availability in original data\n",
    "print(f\"\\n=== ORIGINAL DATA COLUMN CHECK ===\")\n",
    "print(\"PREDICTORS availability:\")\n",
    "for pred in PREDICTORS:\n",
    "    in_wave1 = pred in wave1.columns\n",
    "    in_wave23 = pred in wave23.columns\n",
    "    print(f\"  {pred}: Wave1={in_wave1}, Wave23={in_wave23}\")\n",
    "\n",
    "print(\"\\nOUTCOMES availability:\")\n",
    "for outcome in OUTCOMES:\n",
    "    # Check for outcome with suffixes\n",
    "    w23_col = outcome + '_w23'\n",
    "    \n",
    "    in_wave23_w23 = w23_col in wave23.columns\n",
    "    \n",
    "    print(f\"  {outcome}: w23={in_wave23_w23}\")\n",
    "\n",
    "# Try a manual merge to see what's happening\n",
    "print(f\"\\n=== MANUAL MERGE TEST ===\")\n",
    "test_merge = pd.merge(wave1[['StFCID']], wave23[['StFCID']], on='StFCID', how='inner')\n",
    "print(f\"Test merge result: {len(test_merge)} rows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
