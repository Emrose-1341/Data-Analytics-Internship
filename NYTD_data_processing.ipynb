{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Libraries and S3 client initialized!\n",
      "üìÖ Setup date: 2025-06-26 11:53:47\n",
      "üîë AWS credentials loaded from ~/.aws/credentials\n",
      "‚úÖ Ready to access S3 buckets!\n"
     ]
    }
   ],
   "source": [
    "# S3 Cell 1: Imports and S3 Setup\n",
    "\"\"\"\n",
    "NYTD S3 Data Integration - Cell 1: Imports and S3 Configuration\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import io\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "print(\"üîß Libraries and S3 client initialized!\")\n",
    "print(\"üìÖ Setup date:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print(\"üîë AWS credentials loaded from ~/.aws/credentials\")\n",
    "print(\"‚úÖ Ready to access S3 buckets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ S3 helper functions defined!\n",
      "üìÅ Can now load: CSV, Excel (.xlsx/.xls), and compressed (.tab.gz) files from S3\n"
     ]
    }
   ],
   "source": [
    "# S3 Cell 2: S3 Helper Functions\n",
    "\"\"\"\n",
    "NYTD S3 Data Integration - Cell 2: S3 Data Access Functions\n",
    "\"\"\"\n",
    "\n",
    "def list_s3_files(bucket, prefix=\"\", suffix=None):\n",
    "    \"\"\"List files in S3 bucket\"\"\"\n",
    "    try:\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        keys = []\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            for obj in page.get('Contents', []):\n",
    "                key = obj['Key']\n",
    "                if not suffix or key.lower().endswith(suffix.lower()):\n",
    "                    keys.append(key)\n",
    "        print(f\"Found {len(keys)} files in s3://{bucket}/{prefix}\")\n",
    "        return keys\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing files: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_csv_from_s3(bucket, key):\n",
    "    \"\"\"Load CSV file from S3\"\"\"\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "        df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "        print(f\"Loaded {df.shape[0]} rows, {df.shape[1]} columns from {key}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {key}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_excel_from_s3(bucket, key, sheet_name=None):\n",
    "    \"\"\"Load Excel file from S3\"\"\"\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "        excel_data = io.BytesIO(obj['Body'].read())\n",
    "        \n",
    "        if sheet_name:\n",
    "            df = pd.read_excel(excel_data, sheet_name=sheet_name)\n",
    "        else:\n",
    "            # Read all sheets and return as dict\n",
    "            df = pd.read_excel(excel_data, sheet_name=None)\n",
    "        \n",
    "        if isinstance(df, dict):\n",
    "            total_rows = sum(sheet_df.shape[0] for sheet_df in df.values())\n",
    "            print(f\"Loaded Excel file {key} with {len(df)} sheets, {total_rows} total rows\")\n",
    "        else:\n",
    "            print(f\"Loaded {df.shape[0]} rows, {df.shape[1]} columns from {key}\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {key}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_compressed_from_s3(bucket, key):\n",
    "    \"\"\"Load compressed .tab.gz file from S3\"\"\"\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "        compressed = io.BytesIO(obj['Body'].read())\n",
    "        df = pd.read_csv(compressed, sep='\\t', compression='gzip', dtype=str)\n",
    "        print(f\"Loaded {df.shape[0]} rows, {df.shape[1]} columns from {key}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {key}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ S3 helper functions defined!\")\n",
    "print(\"üìÅ Can now load: CSV, Excel (.xlsx/.xls), and compressed (.tab.gz) files from S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NYTDS3Integrator class defined!\n",
      "üîß Ready to discover and load NYTD data from S3\n"
     ]
    }
   ],
   "source": [
    "# S3 Cell 3: NYTD S3 Integrator Class\n",
    "\"\"\"\n",
    "NYTD S3 Data Integration - Cell 3: Main Class Definition\n",
    "\"\"\"\n",
    "\n",
    "class NYTDS3Integrator:\n",
    "    \"\"\"NYTD data integration directly from S3 sources\"\"\"\n",
    "    \n",
    "    def __init__(self, curated_bucket='bdc-public-curated', raw_bucket='bdc-public-raw'):\n",
    "        self.curated_bucket = curated_bucket\n",
    "        self.raw_bucket = raw_bucket\n",
    "        self.start_time = datetime.now()\n",
    "        self.log = []\n",
    "        self.quality_checks = {}\n",
    "        \n",
    "        # Cohort definitions\n",
    "        self.cohorts = {\n",
    "            '202': {'year': 2011, 'label': 'FY11'},\n",
    "            '228': {'year': 2014, 'label': 'FY14'}, \n",
    "            '266': {'year': 2017, 'label': 'FY17'},\n",
    "            '297': {'year': 2020, 'label': 'FY20'}\n",
    "        }\n",
    "        \n",
    "        # Variables to pivot from long to wide format\n",
    "        self.outcome_vars = [\n",
    "            'CurrFTE', 'CurrPTE', 'EmplySklls', 'SocSecrty', 'EducAid', \n",
    "            'PubFinAs', 'PubFoodAs', 'PubHousAs', 'OthrFinAs', 'HighEdCert', \n",
    "            'CurrenRoll', 'CnctAdult', 'Homeless', 'SubAbuse', 'Incarc', \n",
    "            'Children', 'Marriage', 'Medicaid', 'OthrHlthIn', 'MedicalIn', \n",
    "            'MentlHlthIn', 'PrescripIn', 'OutcmRpt', 'OutcmFCS'\n",
    "        ]\n",
    "        \n",
    "        # Demographic vars (one per person, don't pivot)\n",
    "        self.demo_vars = [\n",
    "            'StFCID', 'StFIPS', 'St', 'RecNumbr', 'DOB', 'Sex',\n",
    "            'AmIAKN', 'Asian', 'BlkAfrAm', 'HawaiiPI', 'White', \n",
    "            'RaceUnkn', 'RaceDcln', 'HisOrgin', 'Baseline', \n",
    "            'Elig19', 'Elig21', 'SampleState', 'InSample'\n",
    "        ]\n",
    "        \n",
    "        print(\"üöÄ NYTD S3 Integration System Initialized\")\n",
    "        print(f\"ü™£ Curated Bucket: {curated_bucket}\")\n",
    "        print(f\"ü™£ Raw Bucket: {raw_bucket}\")\n",
    "        print(f\"üéØ Target Cohorts: {list(self.cohorts.keys())}\")\n",
    "        print(f\"üìä Outcome Variables to Pivot: {len(self.outcome_vars)}\")\n",
    "        print(f\"üë• Demographic Variables: {len(self.demo_vars)}\")\n",
    "    \n",
    "    def _log(self, message, level=\"INFO\"):\n",
    "        \"\"\"Add timestamped log entry\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        log_entry = f\"[{timestamp}] {level}: {message}\"\n",
    "        self.log.append(log_entry)\n",
    "        print(f\"  {log_entry}\")\n",
    "\n",
    "print(\"‚úÖ NYTDS3Integrator class defined!\")\n",
    "print(\"üîß Ready to discover and load NYTD data from S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ S3 data discovery functions added!\n",
      "üéØ Can now intelligently discover and categorize NYTD files in S3\n"
     ]
    }
   ],
   "source": [
    "# S3 Cell 4: S3 Data Discovery Functions\n",
    "\"\"\"\n",
    "NYTD S3 Data Integration - Cell 4: Smart S3 File Discovery\n",
    "\"\"\"\n",
    "\n",
    "def discover_nytd_files(self):\n",
    "    \"\"\"Discover NYTD files in S3 buckets\"\"\"\n",
    "    self._log(\"üîç Discovering NYTD files in S3...\")\n",
    "    \n",
    "    discovered_files = {\n",
    "        'curated': {},\n",
    "        'raw': {}\n",
    "    }\n",
    "    \n",
    "    # Search curated bucket\n",
    "    curated_prefixes = ['nytd/', 'ndacan/nytd/', 'NYTD/', '']\n",
    "    for prefix in curated_prefixes:\n",
    "        self._log(f\"üîç Searching curated bucket: s3://{self.curated_bucket}/{prefix}\")\n",
    "        files = list_s3_files(self.curated_bucket, prefix)\n",
    "        if files:\n",
    "            discovered_files['curated'][prefix] = files\n",
    "    \n",
    "    # Search raw bucket  \n",
    "    raw_prefixes = ['ndacan/nytd/', 'nytd/', '']\n",
    "    for prefix in raw_prefixes:\n",
    "        self._log(f\"üîç Searching raw bucket: s3://{self.raw_bucket}/{prefix}\")\n",
    "        files = list_s3_files(self.raw_bucket, prefix)\n",
    "        if files:\n",
    "            discovered_files['raw'][prefix] = files\n",
    "    \n",
    "    # Categorize files by type\n",
    "    file_inventory = {\n",
    "        'excel_files': [],\n",
    "        'csv_files': [],\n",
    "        'tab_gz_files': [],\n",
    "        'other_files': []\n",
    "    }\n",
    "    \n",
    "    for bucket_type, prefixes in discovered_files.items():\n",
    "        for prefix, files in prefixes.items():\n",
    "            for file in files:\n",
    "                file_info = {\n",
    "                    'bucket': self.curated_bucket if bucket_type == 'curated' else self.raw_bucket,\n",
    "                    'key': file,\n",
    "                    'type': bucket_type,\n",
    "                    'prefix': prefix\n",
    "                }\n",
    "                \n",
    "                if file.lower().endswith(('.xlsx', '.xls')):\n",
    "                    file_inventory['excel_files'].append(file_info)\n",
    "                elif file.lower().endswith('.csv'):\n",
    "                    file_inventory['csv_files'].append(file_info)\n",
    "                elif file.lower().endswith('.tab.gz'):\n",
    "                    file_inventory['tab_gz_files'].append(file_info)\n",
    "                else:\n",
    "                    file_inventory['other_files'].append(file_info)\n",
    "    \n",
    "    # Log discovery results\n",
    "    self._log(f\"üìä File Discovery Results:\")\n",
    "    for file_type, files in file_inventory.items():\n",
    "        self._log(f\"   {file_type}: {len(files)} files\")\n",
    "        for file_info in files[:3]:  # Show first 3 of each type\n",
    "            self._log(f\"     ‚Ä¢ {file_info['key']}\")\n",
    "        if len(files) > 3:\n",
    "            self._log(f\"     ... and {len(files)-3} more\")\n",
    "    \n",
    "    return file_inventory\n",
    "\n",
    "def identify_cohort_from_filename(self, filename):\n",
    "    \"\"\"Smart cohort identification from filename\"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    \n",
    "    # Strategy 1: Direct cohort number match\n",
    "    for cohort in self.cohorts.keys():\n",
    "        if cohort in filename_lower:\n",
    "            return cohort\n",
    "    \n",
    "    # Strategy 2: Fiscal year match\n",
    "    for cohort, info in self.cohorts.items():\n",
    "        fy_patterns = [\n",
    "            f\"fy{info['year']}\",\n",
    "            f\"fy{str(info['year'])[-2:]}\",\n",
    "            f\"{info['year']}\",\n",
    "            f\"c{str(info['year'])[-2:]}\"  # C14, C17, etc.\n",
    "        ]\n",
    "        \n",
    "        for pattern in fy_patterns:\n",
    "            if pattern in filename_lower.replace('-','').replace('_',''):\n",
    "                return cohort\n",
    "    \n",
    "    # Strategy 3: Pattern matching for known file patterns\n",
    "    patterns = {\n",
    "        'outcomes_c14': '202',  # FY11 cohort uses C14\n",
    "        'outcomes_c17': '266',  # FY17 cohort uses C17\n",
    "        'outcomes20': '297'     # FY20 cohort\n",
    "    }\n",
    "    \n",
    "    for pattern, cohort in patterns.items():\n",
    "        if pattern in filename_lower:\n",
    "            return cohort\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Add methods to class\n",
    "NYTDS3Integrator.discover_nytd_files = discover_nytd_files\n",
    "NYTDS3Integrator.identify_cohort_from_filename = identify_cohort_from_filename\n",
    "\n",
    "print(\"‚úÖ S3 data discovery functions added!\")\n",
    "print(\"üéØ Can now intelligently discover and categorize NYTD files in S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ S3 data loading functions added!\n",
      "üéØ Can now intelligently load NYTD data from various S3 file formats\n"
     ]
    }
   ],
   "source": [
    "# S3 Cell 5: S3 Data Loading Functions\n",
    "\"\"\"\n",
    "NYTD S3 Data Integration - Cell 5: Intelligent Data Loading from S3\n",
    "\"\"\"\n",
    "\n",
    "def load_datasets_from_s3(self):\n",
    "    \"\"\"Load NYTD datasets from S3 with intelligent file prioritization\"\"\"\n",
    "    self._log(\"üîÑ Loading NYTD datasets from S3...\")\n",
    "    \n",
    "    # First discover what's available\n",
    "    file_inventory = self.discover_nytd_files()\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    # Strategy 1: Look for curated Excel files first (highest priority)\n",
    "    excel_files = file_inventory['excel_files']\n",
    "    if excel_files:\n",
    "        self._log(f\"üìä Found {len(excel_files)} Excel files, processing...\")\n",
    "        \n",
    "        for file_info in excel_files:\n",
    "            filename = os.path.basename(file_info['key'])\n",
    "            cohort_found = self.identify_cohort_from_filename(filename)\n",
    "            \n",
    "            if cohort_found and cohort_found not in datasets:\n",
    "                self._log(f\"üéØ Loading cohort {cohort_found} from Excel: {file_info['key']}\")\n",
    "                df = load_excel_from_s3(file_info['bucket'], file_info['key'])\n",
    "                \n",
    "                if isinstance(df, dict):\n",
    "                    # Multiple sheets - combine or pick the largest\n",
    "                    largest_sheet = max(df.keys(), key=lambda x: df[x].shape[0])\n",
    "                    self._log(f\"   Using largest sheet: {largest_sheet}\")\n",
    "                    datasets[cohort_found] = df[largest_sheet]\n",
    "                elif df is not None:\n",
    "                    datasets[cohort_found] = df\n",
    "            elif cohort_found:\n",
    "                self._log(f\"   Skipping {file_info['key']} - cohort {cohort_found} already loaded\")\n",
    "            else:\n",
    "                self._log(f\"‚ùì Could not identify cohort for {file_info['key']}\")\n",
    "    \n",
    "    # Strategy 2: Look for CSV files for missing cohorts\n",
    "    if len(datasets) < 4:  # We expect 4 cohorts\n",
    "        csv_files = file_inventory['csv_files']\n",
    "        if csv_files:\n",
    "            self._log(f\"üìÑ Checking {len(csv_files)} CSV files for missing cohorts...\")\n",
    "            \n",
    "            for file_info in csv_files:\n",
    "                filename = os.path.basename(file_info['key'])\n",
    "                cohort_found = self.identify_cohort_from_filename(filename)\n",
    "                \n",
    "                if cohort_found and cohort_found not in datasets:\n",
    "                    self._log(f\"üéØ Loading cohort {cohort_found} from CSV: {file_info['key']}\")\n",
    "                    df = load_csv_from_s3(file_info['bucket'], file_info['key'])\n",
    "                    if df is not None:\n",
    "                        datasets[cohort_found] = df\n",
    "    \n",
    "    # Strategy 3: Fall back to compressed files from raw bucket\n",
    "    if len(datasets) < 4:\n",
    "        tab_gz_files = file_inventory['tab_gz_files']\n",
    "        if tab_gz_files:\n",
    "            self._log(f\"üì¶ Checking {len(tab_gz_files)} compressed files for missing cohorts...\")\n",
    "            \n",
    "            # Known file patterns for raw data\n",
    "            file_patterns = {\n",
    "                '202': 'outcomes_C14.tab.gz',\n",
    "                '228': 'outcomes_C14.tab.gz', \n",
    "                '266': 'outcomes_C17.tab.gz',\n",
    "                '297': 'Outcomes20_w3.tab.gz'\n",
    "            }\n",
    "            \n",
    "            for cohort, pattern in file_patterns.items():\n",
    "                if cohort not in datasets:\n",
    "                    matching_files = [f for f in tab_gz_files \n",
    "                                    if pattern in f['key'] and f\"/{cohort}/\" in f['key']]\n",
    "                    \n",
    "                    if matching_files:\n",
    "                        file_info = matching_files[0]  # Take first match\n",
    "                        self._log(f\"üéØ Loading cohort {cohort} from compressed: {file_info['key']}\")\n",
    "                        df = load_compressed_from_s3(file_info['bucket'], file_info['key'])\n",
    "                        if df is not None:\n",
    "                            datasets[cohort] = df\n",
    "    \n",
    "    # Strategy 4: Flexible pattern matching for any remaining files\n",
    "    if len(datasets) < 4:\n",
    "        self._log(\"üîç Attempting flexible pattern matching for remaining files...\")\n",
    "        \n",
    "        all_files = (file_inventory['excel_files'] + \n",
    "                    file_inventory['csv_files'] + \n",
    "                    file_inventory['tab_gz_files'])\n",
    "        \n",
    "        for file_info in all_files:\n",
    "            filename = os.path.basename(file_info['key'])\n",
    "            \n",
    "            # Try multiple identification strategies\n",
    "            for cohort in self.cohorts.keys():\n",
    "                if cohort not in datasets:\n",
    "                    # Check for any mention of cohort year or number\n",
    "                    year = self.cohorts[cohort]['year']\n",
    "                    if (str(year) in filename or \n",
    "                        cohort in filename or\n",
    "                        f\"fy{str(year)[-2:]}\" in filename.lower()):\n",
    "                        \n",
    "                        self._log(f\"üéØ Flexible match: Loading cohort {cohort} from {file_info['key']}\")\n",
    "                        \n",
    "                        if file_info['key'].endswith('.xlsx') or file_info['key'].endswith('.xls'):\n",
    "                            df = load_excel_from_s3(file_info['bucket'], file_info['key'])\n",
    "                        elif file_info['key'].endswith('.csv'):\n",
    "                            df = load_csv_from_s3(file_info['bucket'], file_info['key'])\n",
    "                        elif file_info['key'].endswith('.tab.gz'):\n",
    "                            df = load_compressed_from_s3(file_info['bucket'], file_info['key'])\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "                        if isinstance(df, dict):\n",
    "                            largest_sheet = max(df.keys(), key=lambda x: df[x].shape[0])\n",
    "                            df = df[largest_sheet]\n",
    "                        \n",
    "                        if df is not None:\n",
    "                            datasets[cohort] = df\n",
    "                            break\n",
    "    \n",
    "    if not datasets:\n",
    "        self._log(\"‚ùå No NYTD datasets could be loaded from S3!\", \"ERROR\")\n",
    "        return None\n",
    "    \n",
    "    # Log final results\n",
    "    self._log(f\"‚úÖ Successfully loaded {len(datasets)} cohorts from S3:\")\n",
    "    for cohort, df in datasets.items():\n",
    "        cohort_year = self.cohorts[cohort]['year']\n",
    "        self._log(f\"   ‚Ä¢ Cohort {cohort} (FY{cohort_year}): {df.shape[0]:,} rows √ó {df.shape[1]} cols\")\n",
    "    \n",
    "    missing_cohorts = [c for c in self.cohorts.keys() if c not in datasets]\n",
    "    if missing_cohorts:\n",
    "        self._log(f\"‚ö†Ô∏è  Missing cohorts: {missing_cohorts}\", \"WARNING\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Add method to class\n",
    "NYTDS3Integrator.load_datasets_from_s3 = load_datasets_from_s3\n",
    "\n",
    "print(\"‚úÖ S3 data loading functions added!\")\n",
    "print(\"üéØ Can now intelligently load NYTD data from various S3 file formats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data processing functions added!\n",
      "üîß Can now standardize, combine, and pivot S3 data into wide format\n"
     ]
    }
   ],
   "source": [
    "# S3 Cell 6: Data Processing Functions\n",
    "\"\"\"\n",
    "NYTD S3 Data Integration - Cell 6: Data Standardization and Processing\n",
    "\"\"\"\n",
    "\n",
    "def standardize_s3_datasets(self, datasets):\n",
    "    \"\"\"Standardize datasets loaded from S3\"\"\"\n",
    "    self._log(\"üîß Standardizing S3 datasets...\")\n",
    "    \n",
    "    standardized = {}\n",
    "    \n",
    "    # Get all unique columns across datasets\n",
    "    all_columns = set()\n",
    "    for df in datasets.values():\n",
    "        all_columns.update(df.columns)\n",
    "    \n",
    "    self._log(f\"   Found {len(all_columns)} unique columns across all datasets\")\n",
    "    \n",
    "    for cohort, df in datasets.items():\n",
    "        df_std = df.copy()\n",
    "        \n",
    "        # Add cohort identifiers\n",
    "        df_std['cohort_id'] = cohort\n",
    "        df_std['cohort_year'] = self.cohorts[cohort]['year']\n",
    "        df_std['cohort_label'] = self.cohorts[cohort]['label']\n",
    "        df_std['data_source'] = 'S3'\n",
    "        \n",
    "        # Add missing columns with NaN\n",
    "        missing_cols = all_columns - set(df.columns)\n",
    "        for col in missing_cols:\n",
    "            if col not in ['cohort_id', 'cohort_year', 'cohort_label', 'data_source']:\n",
    "                df_std[col] = np.nan\n",
    "        \n",
    "        # Standardize data types\n",
    "        if 'Wave' in df_std.columns:\n",
    "            df_std['Wave'] = pd.to_numeric(df_std['Wave'], errors='coerce')\n",
    "        \n",
    "        # Convert dates with S3-specific handling\n",
    "        date_cols = ['RepDate', 'DOB', 'OutcmDte']\n",
    "        for col in date_cols:\n",
    "            if col in df_std.columns:\n",
    "                if col == 'RepDate':\n",
    "                    # Handle YYYYMM.0 format common in S3 data\n",
    "                    def fix_repdate(date_val):\n",
    "                        if pd.isna(date_val):\n",
    "                            return pd.NaT\n",
    "                        try:\n",
    "                            date_str = str(date_val).replace('.0', '')\n",
    "                            if len(date_str) == 6 and date_str.isdigit():  # YYYYMM format\n",
    "                                year = date_str[:4]\n",
    "                                month = date_str[4:6]\n",
    "                                return pd.to_datetime(f\"{year}-{month}-01\")\n",
    "                            return pd.to_datetime(date_val, errors='coerce')\n",
    "                        except:\n",
    "                            return pd.NaT\n",
    "                    \n",
    "                    df_std[col] = df_std[col].apply(fix_repdate)\n",
    "                else:\n",
    "                    df_std[col] = pd.to_datetime(df_std[col], errors='coerce')\n",
    "        \n",
    "        # Create unique person ID for linking\n",
    "        if 'StFCID' in df_std.columns:\n",
    "            df_std['person_id'] = df_std['StFCID'].astype(str)\n",
    "        else:\n",
    "            self._log(f\"‚ö†Ô∏è  No StFCID column found in cohort {cohort}\", \"WARNING\")\n",
    "            df_std['person_id'] = df_std.index.astype(str)\n",
    "        \n",
    "        # Clean up outcome variables\n",
    "        for var in self.outcome_vars:\n",
    "            if var in df_std.columns:\n",
    "                # Convert to numeric where possible\n",
    "                df_std[var] = pd.to_numeric(df_std[var], errors='coerce')\n",
    "        \n",
    "        standardized[cohort] = df_std\n",
    "        self._log(f\"‚úÖ Standardized {cohort}: {df_std.shape}\")\n",
    "    \n",
    "    return standardized\n",
    "\n",
    "def combine_cohorts(self, datasets):\n",
    "    \"\"\"Combine all cohorts into single long-format dataset\"\"\"\n",
    "    self._log(\"üîó Combining cohorts into integrated dataset...\")\n",
    "    \n",
    "    # Stack all datasets\n",
    "    combined_dfs = []\n",
    "    for cohort, df in datasets.items():\n",
    "        # Add a marker for which cohort this data came from\n",
    "        df_marked = df.copy()\n",
    "        df_marked['original_cohort'] = cohort\n",
    "        combined_dfs.append(df_marked)\n",
    "    \n",
    "    integrated = pd.concat(combined_dfs, ignore_index=True, sort=False)\n",
    "    \n",
    "    # Sort by person and wave\n",
    "    if 'person_id' in integrated.columns and 'Wave' in integrated.columns:\n",
    "        integrated = integrated.sort_values(['person_id', 'Wave']).reset_index(drop=True)\n",
    "    \n",
    "    self._log(f\"‚úÖ Integrated dataset: {integrated.shape[0]:,} rows √ó {integrated.shape[1]} cols\")\n",
    "    self._log(f\"   Total individuals: {integrated['person_id'].nunique():,}\")\n",
    "    self._log(f\"   Cohorts: {integrated['cohort_id'].nunique()}\")\n",
    "    \n",
    "    if 'Wave' in integrated.columns:\n",
    "        waves = sorted([w for w in integrated['Wave'].unique() if pd.notna(w)])\n",
    "        self._log(f\"   Waves: {waves}\")\n",
    "    \n",
    "    return integrated\n",
    "\n",
    "def convert_to_wide_format(self, long_df):\n",
    "    \"\"\"Convert from long format to wide format (pivot waves)\"\"\"\n",
    "    self._log(\"üîÑ Converting to wide format...\")\n",
    "    \n",
    "    # Identify variables present in the data\n",
    "    available_outcome_vars = [var for var in self.outcome_vars if var in long_df.columns]\n",
    "    available_demo_vars = [var for var in self.demo_vars if var in long_df.columns]\n",
    "    \n",
    "    self._log(f\"   Pivoting {len(available_outcome_vars)} outcome variables\")\n",
    "    self._log(f\"   Keeping {len(available_demo_vars)} demographic variables\")\n",
    "    \n",
    "    if 'Wave' not in long_df.columns:\n",
    "        self._log(\"‚ö†Ô∏è  No Wave column found - cannot pivot by wave\", \"WARNING\")\n",
    "        return long_df\n",
    "    \n",
    "    # Get demographic info (one record per person)\n",
    "    # Use Wave 1 for demographics since they shouldn't change\n",
    "    demo_data = long_df[long_df['Wave'] == 1].copy()\n",
    "    \n",
    "    if demo_data.empty:\n",
    "        # If no Wave 1 data, use first available wave\n",
    "        first_wave = long_df['Wave'].min()\n",
    "        demo_data = long_df[long_df['Wave'] == first_wave].copy()\n",
    "        self._log(f\"   Using Wave {first_wave} for demographics (Wave 1 not available)\")\n",
    "    \n",
    "    # Keep demographic variables + identifiers\n",
    "    demo_cols = (['person_id', 'cohort_id', 'cohort_year', 'cohort_label', 'data_source', 'original_cohort'] + \n",
    "                 available_demo_vars)\n",
    "    demo_cols = [col for col in demo_cols if col in demo_data.columns]\n",
    "    demographics = demo_data[demo_cols].copy()\n",
    "    \n",
    "    # Remove duplicates (in case of multiple records per person in Wave 1)\n",
    "    demographics = demographics.drop_duplicates(subset=['person_id'], keep='first')\n",
    "    \n",
    "    # Pivot outcome variables by wave\n",
    "    pivot_data = []\n",
    "    \n",
    "    available_waves = sorted([w for w in long_df['Wave'].unique() if pd.notna(w)])\n",
    "    \n",
    "    for wave in available_waves:\n",
    "        wave_data = long_df[long_df['Wave'] == wave].copy()\n",
    "        \n",
    "        # Select outcome variables for this wave\n",
    "        wave_cols = ['person_id'] + available_outcome_vars\n",
    "        wave_cols = [col for col in wave_cols if col in wave_data.columns]\n",
    "        wave_subset = wave_data[wave_cols].copy()\n",
    "        \n",
    "        # Remove duplicates (keep first occurrence per person)\n",
    "        wave_subset = wave_subset.drop_duplicates(subset=['person_id'], keep='first')\n",
    "        \n",
    "        # Rename outcome variables with wave suffix\n",
    "        rename_dict = {}\n",
    "        for var in available_outcome_vars:\n",
    "            if var in wave_subset.columns:\n",
    "                rename_dict[var] = f\"{var}_{int(wave)}\"\n",
    "        \n",
    "        wave_subset = wave_subset.rename(columns=rename_dict)\n",
    "        pivot_data.append(wave_subset)\n",
    "    \n",
    "    # Merge all waves together\n",
    "    wide_df = demographics.copy()\n",
    "    \n",
    "    for wave_df in pivot_data:\n",
    "        wide_df = wide_df.merge(wave_df, on='person_id', how='left')\n",
    "    \n",
    "    self._log(f\"‚úÖ Wide format: {wide_df.shape[0]:,} rows √ó {wide_df.shape[1]} cols\")\n",
    "    \n",
    "    return wide_df\n",
    "\n",
    "# Add methods to class\n",
    "NYTDS3Integrator.standardize_s3_datasets = standardize_s3_datasets\n",
    "NYTDS3Integrator.combine_cohorts = combine_cohorts\n",
    "NYTDS3Integrator.convert_to_wide_format = convert_to_wide_format\n",
    "\n",
    "print(\"‚úÖ Data processing functions added!\")\n",
    "print(\"üîß Can now standardize, combine, and pivot S3 data into wide format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Main S3 integration workflow function added!\n",
      "üéØ Ready to execute complete S3 integration pipeline\n"
     ]
    }
   ],
   "source": [
    "# S3 Cell 7: Main S3 Integration Function\n",
    "\"\"\"\n",
    "NYTD S3 Data Integration - Cell 7: Complete Integration Workflow\n",
    "\"\"\"\n",
    "\n",
    "def run_s3_integration(self):\n",
    "    \"\"\"Execute complete S3 integration workflow\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ NYTD S3 DATA INTEGRATION & WIDE FORMAT CONVERSION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load datasets from S3\n",
    "        self._log(\"üéØ Step 1: Loading datasets from S3...\")\n",
    "        datasets = self.load_datasets_from_s3()\n",
    "        if not datasets:\n",
    "            self._log(\"‚ùå No datasets loaded from S3!\", \"ERROR\")\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Standardize datasets\n",
    "        self._log(\"üéØ Step 2: Standardizing datasets...\")\n",
    "        standardized = self.standardize_s3_datasets(datasets)\n",
    "        \n",
    "        # Step 3: Combine cohorts\n",
    "        self._log(\"üéØ Step 3: Combining cohorts...\")\n",
    "        integrated_long = self.combine_cohorts(standardized)\n",
    "        \n",
    "        # Step 4: Convert to wide format\n",
    "        self._log(\"üéØ Step 4: Converting to wide format...\")\n",
    "        analytical_wide = self.convert_to_wide_format(integrated_long)\n",
    "        \n",
    "        # Step 5: Quality checks\n",
    "        self._log(\"üéØ Step 5: Running quality checks...\")\n",
    "        qa_results = self.run_quality_checks(integrated_long, analytical_wide)\n",
    "        \n",
    "        # Step 6: Save results\n",
    "        self._log(\"üéØ Step 6: Saving results...\")\n",
    "        saved_files = self.save_s3_results(integrated_long, analytical_wide, qa_results)\n",
    "        \n",
    "        # Final summary\n",
    "        duration = (datetime.now() - self.start_time).total_seconds() / 60\n",
    "        print(f\"\\nüéâ S3 INTEGRATION COMPLETE!\")\n",
    "        print(f\"‚è±Ô∏è  Duration: {duration:.1f} minutes\")\n",
    "        print(f\"üìä Analytical Dataset: {len(analytical_wide):,} individuals √ó {len(analytical_wide.columns)} variables\")\n",
    "        print(f\"üìÅ Files Created:\")\n",
    "        for filename in saved_files.values():\n",
    "            if filename:\n",
    "                print(f\"   ‚Ä¢ {filename}\")\n",
    "        \n",
    "        return {\n",
    "            'long_format': integrated_long,\n",
    "            'wide_format': analytical_wide,\n",
    "            'quality_assurance': qa_results,\n",
    "            'files': saved_files,\n",
    "            'processing_log': self.log\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        self._log(f\"‚ùå S3 Integration failed: {e}\", \"ERROR\")\n",
    "        import traceback\n",
    "        self._log(f\"   Full error: {traceback.format_exc()}\", \"ERROR\")\n",
    "        raise e\n",
    "\n",
    "def run_quality_checks(self, long_df, wide_df):\n",
    "    \"\"\"Run quality assurance checks on the integrated data\"\"\"\n",
    "    self._log(\"üîç Running quality assurance checks...\")\n",
    "    \n",
    "    qa_results = {\n",
    "        'record_counts': {\n",
    "            'long_format_records': len(long_df),\n",
    "            'wide_format_records': len(wide_df),\n",
    "            'unique_individuals_long': long_df['person_id'].nunique() if 'person_id' in long_df.columns else 0,\n",
    "            'unique_individuals_wide': wide_df['person_id'].nunique() if 'person_id' in wide_df.columns else 0\n",
    "        },\n",
    "        'data_integrity': {},\n",
    "        'coverage_analysis': {},\n",
    "        'wave_analysis': {}\n",
    "    }\n",
    "    \n",
    "    # Check if wide format has one record per person\n",
    "    if 'person_id' in wide_df.columns:\n",
    "        qa_results['data_integrity']['one_record_per_person'] = len(wide_df) == wide_df['person_id'].nunique()\n",
    "    \n",
    "    # Check wave coverage\n",
    "    for wave in [1, 2, 3]:\n",
    "        wave_vars = [col for col in wide_df.columns if col.endswith(f'_{wave}')]\n",
    "        if wave_vars:\n",
    "            non_null_counts = {}\n",
    "            for var in wave_vars[:10]:  # Check first 10 variables\n",
    "                non_null_counts[var] = wide_df[var].notna().sum()\n",
    "            qa_results['wave_analysis'][f'wave_{wave}'] = {\n",
    "                'variables': len(wave_vars),\n",
    "                'sample_coverage': non_null_counts\n",
    "            }\n",
    "    \n",
    "    # Cohort distribution\n",
    "    if 'cohort_id' in wide_df.columns:\n",
    "        cohort_dist = wide_df['cohort_id'].value_counts().to_dict()\n",
    "        qa_results['coverage_analysis']['cohort_distribution'] = cohort_dist\n",
    "    \n",
    "    # Missing data analysis for key variables\n",
    "    key_vars = ['Homeless_1', 'Homeless_2', 'Homeless_3', 'CurrFTE_1', 'CurrFTE_2', 'CurrFTE_3']\n",
    "    missing_analysis = {}\n",
    "    for var in key_vars:\n",
    "        if var in wide_df.columns:\n",
    "            missing_pct = (wide_df[var].isnull().sum() / len(wide_df)) * 100\n",
    "            missing_analysis[var] = round(missing_pct, 1)\n",
    "    qa_results['coverage_analysis']['key_variables_missing_pct'] = missing_analysis\n",
    "    \n",
    "    self._log(\"‚úÖ Quality assurance complete\")\n",
    "    return qa_results\n",
    "\n",
    "def save_s3_results(self, long_df, wide_df, qa_results):\n",
    "    \"\"\"Save final datasets and documentation\"\"\"\n",
    "    self._log(\"üíæ Saving S3 integration results...\")\n",
    "    \n",
    "    saved_files = {}\n",
    "    \n",
    "    try:\n",
    "        # Save long format (integrated)\n",
    "        long_filename = 'NYTD_S3_Integrated_Long_Format.csv'\n",
    "        long_df.to_csv(long_filename, index=False)\n",
    "        saved_files['long_format'] = long_filename\n",
    "        self._log(f\"‚úÖ Saved: {long_filename} ({len(long_df):,} rows)\")\n",
    "    except Exception as e:\n",
    "        self._log(f\"‚ùå Error saving long format: {e}\", \"ERROR\")\n",
    "        saved_files['long_format'] = None\n",
    "    \n",
    "    try:\n",
    "        # Save wide format (analytical dataset)\n",
    "        wide_filename = 'NYTD_S3_Analytical_Wide_Format.csv'\n",
    "        wide_df.to_csv(wide_filename, index=False)\n",
    "        saved_files['wide_format'] = wide_filename\n",
    "        self._log(f\"‚úÖ Saved: {wide_filename} ({len(wide_df):,} rows)\")\n",
    "    except Exception as e:\n",
    "        self._log(f\"‚ùå Error saving wide format: {e}\", \"ERROR\")\n",
    "        saved_files['wide_format'] = None\n",
    "    \n",
    "    try:\n",
    "        # Create and save summary\n",
    "        summary = pd.DataFrame({\n",
    "            'Dataset': ['Long Format', 'Wide Format'],\n",
    "            'Records': [len(long_df), len(wide_df)],\n",
    "            'Variables': [len(long_df.columns), len(wide_df.columns)],\n",
    "            'Individuals': [\n",
    "                long_df['person_id'].nunique() if 'person_id' in long_df.columns else 0,\n",
    "                wide_df['person_id'].nunique() if 'person_id' in wide_df.columns else 0\n",
    "            ],\n",
    "            'Filename': [saved_files.get('long_format', ''), saved_files.get('wide_format', '')]\n",
    "        })\n",
    "        \n",
    "        summary_filename = 'NYTD_S3_Dataset_Summary.csv'\n",
    "        summary.to_csv(summary_filename, index=False)\n",
    "        saved_files['summary'] = summary_filename\n",
    "        self._log(f\"‚úÖ Saved: {summary_filename}\")\n",
    "    except Exception as e:\n",
    "        self._log(f\"‚ùå Error saving summary: {e}\", \"ERROR\")\n",
    "        saved_files['summary'] = None\n",
    "    \n",
    "    try:\n",
    "        # Save comprehensive documentation\n",
    "        documentation = {\n",
    "            'integration_metadata': {\n",
    "                'processing_date': datetime.now().isoformat(),\n",
    "                'data_source': 'S3 bdc-public-curated and bdc-public-raw',\n",
    "                'cohorts_processed': list(self.cohorts.keys()),\n",
    "                'total_individuals': wide_df['person_id'].nunique() if 'person_id' in wide_df.columns else 0,\n",
    "                'total_variables': len(wide_df.columns)\n",
    "            },\n",
    "            'quality_assurance': qa_results,\n",
    "            'processing_log': self.log,\n",
    "            'variable_catalog': {\n",
    "                'demographic_variables': [col for col in wide_df.columns if not any(col.endswith(f'_{w}') for w in [1,2,3])],\n",
    "                'wave_1_variables': [col for col in wide_df.columns if col.endswith('_1')],\n",
    "                'wave_2_variables': [col for col in wide_df.columns if col.endswith('_2')],\n",
    "                'wave_3_variables': [col for col in wide_df.columns if col.endswith('_3')]\n",
    "            },\n",
    "            'file_inventory': saved_files\n",
    "        }\n",
    "        \n",
    "        doc_filename = 'NYTD_S3_Integration_Documentation.json'\n",
    "        with open(doc_filename, 'w') as f:\n",
    "            json.dump(documentation, f, indent=2, default=str)\n",
    "        saved_files['documentation'] = doc_filename\n",
    "        self._log(f\"‚úÖ Saved: {doc_filename}\")\n",
    "    except Exception as e:\n",
    "        self._log(f\"‚ùå Error saving documentation: {e}\", \"ERROR\")\n",
    "        saved_files['documentation'] = None\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "# Add methods to class\n",
    "NYTDS3Integrator.run_s3_integration = run_s3_integration\n",
    "NYTDS3Integrator.run_quality_checks = run_quality_checks\n",
    "NYTDS3Integrator.save_s3_results = save_s3_results\n",
    "\n",
    "print(\"‚úÖ Main S3 integration workflow function added!\")\n",
    "print(\"üéØ Ready to execute complete S3 integration pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing NYTD S3 Integration...\n",
      "üöÄ NYTD S3 Integration System Initialized\n",
      "ü™£ Curated Bucket: bdc-public-curated\n",
      "ü™£ Raw Bucket: bdc-public-raw\n",
      "üéØ Target Cohorts: ['202', '228', '266', '297']\n",
      "üìä Outcome Variables to Pivot: 24\n",
      "üë• Demographic Variables: 19\n",
      "\n",
      "üîÑ Starting S3 integration process...\n",
      "\n",
      "============================================================\n",
      "üöÄ NYTD S3 DATA INTEGRATION & WIDE FORMAT CONVERSION\n",
      "============================================================\n",
      "  [11:53:47] INFO: üéØ Step 1: Loading datasets from S3...\n",
      "  [11:53:47] INFO: üîÑ Loading NYTD datasets from S3...\n",
      "  [11:53:47] INFO: üîç Discovering NYTD files in S3...\n",
      "  [11:53:47] INFO: üîç Searching curated bucket: s3://bdc-public-curated/nytd/\n",
      "Found 1 files in s3://bdc-public-curated/nytd/\n",
      "  [11:53:48] INFO: üîç Searching curated bucket: s3://bdc-public-curated/ndacan/nytd/\n",
      "Found 11 files in s3://bdc-public-curated/ndacan/nytd/\n",
      "  [11:53:48] INFO: üîç Searching curated bucket: s3://bdc-public-curated/NYTD/\n",
      "Found 0 files in s3://bdc-public-curated/NYTD/\n",
      "  [11:53:48] INFO: üîç Searching curated bucket: s3://bdc-public-curated/\n",
      "Found 2409 files in s3://bdc-public-curated/\n",
      "  [11:53:49] INFO: üîç Searching raw bucket: s3://bdc-public-raw/ndacan/nytd/\n",
      "Found 13 files in s3://bdc-public-raw/ndacan/nytd/\n",
      "  [11:53:49] INFO: üîç Searching raw bucket: s3://bdc-public-raw/nytd/\n",
      "Found 0 files in s3://bdc-public-raw/nytd/\n",
      "  [11:53:49] INFO: üîç Searching raw bucket: s3://bdc-public-raw/\n",
      "Found 24626 files in s3://bdc-public-raw/\n",
      "  [11:54:06] INFO: üìä File Discovery Results:\n",
      "  [11:54:06] INFO:    excel_files: 26 files\n",
      "  [11:54:06] INFO:      ‚Ä¢ state_specific/alabama/alabama-public-health/2021/modified_disabled.xlsx\n",
      "  [11:54:06] INFO:      ‚Ä¢ state_specific/alabama/alabama-public-health/2021/modified_food_help_snaps.xlsx\n",
      "  [11:54:06] INFO:      ‚Ä¢ state_specific/alabama/alabama-public-health/2021/modified_geriatric.xlsx\n",
      "  [11:54:06] INFO:      ... and 23 more\n",
      "  [11:54:06] INFO:    csv_files: 2622 files\n",
      "  [11:54:06] INFO:      ‚Ä¢ ndacan/nytd/outcomes/cleaned/nytd_outcomes_202_cleaned.csv\n",
      "  [11:54:06] INFO:      ‚Ä¢ ndacan/nytd/outcomes/cleaned/nytd_outcomes_228_cleaned.csv\n",
      "  [11:54:06] INFO:      ‚Ä¢ ndacan/nytd/outcomes/cleaned/nytd_outcomes_266_cleaned.csv\n",
      "  [11:54:06] INFO:      ... and 2619 more\n",
      "  [11:54:06] INFO:    tab_gz_files: 20 files\n",
      "  [11:54:06] INFO:      ‚Ä¢ ndacan/nytd/outcomes/202/outcomes_C14.tab.gz\n",
      "  [11:54:06] INFO:      ‚Ä¢ ndacan/nytd/outcomes/228/outcomes_C14.tab.gz\n",
      "  [11:54:06] INFO:      ‚Ä¢ ndacan/nytd/outcomes/266/outcomes_C17.tab.gz\n",
      "  [11:54:06] INFO:      ... and 17 more\n",
      "  [11:54:06] INFO:    other_files: 24392 files\n",
      "  [11:54:06] INFO:      ‚Ä¢ nytd/\n",
      "  [11:54:06] INFO:      ‚Ä¢ ndacan/nytd/\n",
      "  [11:54:06] INFO:      ‚Ä¢ ndacan/nytd/outcomes/\n",
      "  [11:54:06] INFO:      ... and 24389 more\n",
      "  [11:54:06] INFO: üìä Found 26 Excel files, processing...\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/alabama-public-health/2021/modified_disabled.xlsx\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/alabama-public-health/2021/modified_food_help_snaps.xlsx\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/alabama-public-health/2021/modified_geriatric.xlsx\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/alabama-public-health/2021/modified_highschool.xlsx\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/alabama-public-health/2021/modified_income_percapita.xlsx\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/alabama-public-health/2021/modified_no_car.xlsx\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/alabama-public-health/2021/modified_unemployment.xlsx\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/alabama-public-health/2021/modified_uninsured.xlsx\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/alabama-public-health/2021/modified_violence_mortality_rate.xlsx\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/kids_count_data_center/2000-2023/abuse_neglect.xlsx\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/kids_count_data_center/2000-2023/modified_chronic_absenteeism_kcdc.xlsx\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/kids_count_data_center/2000-2023/modified_foster_care_kcdc.xlsx\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/kids_count_data_center/2000-2023/modified_high_school_dropout_rate_kcdc.xlsx\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/kids_count_data_center/2000-2023/modified_juvenile_violent_crime_court_petition_rate_kcdc.xlsx\n",
      "  [11:54:06] INFO: ‚ùì Could not identify cohort for state_specific/alabama/office_of_refugee_resettlement/2015-2025/modified_unaccompanied_children_released.xlsx\n",
      "  [11:54:06] INFO: üéØ Loading cohort 202 from Excel: feeding_america/2024/feeding_america_2024_original.xlsx\n",
      "Loaded Excel file feeding_america/2024/feeding_america_2024_original.xlsx with 4 sheets, 14674 total rows\n",
      "  [11:54:10] INFO:    Using largest sheet: county\n",
      "  [11:54:10] INFO:    Skipping feeding_america/archive/MMG2024_2019-2022_Data_ToShare_v3.xlsx - cohort 202 already loaded\n",
      "  [11:54:10] INFO:    Skipping state_specific/alabama/acs/ACS_2023_variables.xlsx - cohort 202 already loaded\n",
      "  [11:54:10] INFO: ‚ùì Could not identify cohort for state_specific/alabama/hud-cocs/Alabama_CoCs.xlsx\n",
      "  [11:54:10] INFO: ‚ùì Could not identify cohort for state_specific/alabama/kids_count_data_center/2000-2023/Chronic_Absenteeism_KCDC.xlsx\n",
      "  [11:54:10] INFO: ‚ùì Could not identify cohort for state_specific/alabama/kids_count_data_center/2000-2023/Foster_care_KCDC.xlsx\n",
      "  [11:54:10] INFO: ‚ùì Could not identify cohort for state_specific/alabama/kids_count_data_center/2000-2023/High_School_Dropout_Rate_KCDC.xlsx\n",
      "  [11:54:10] INFO: ‚ùì Could not identify cohort for state_specific/alabama/kids_count_data_center/2000-2023/juvenile_violent_crime_court_petition_rate_KCDC.xlsx\n",
      "  [11:54:10] INFO: ‚ùì Could not identify cohort for state_specific/alabama/office_of_juvenile_justice_and_delinquency_prevention/2020/juvenile_court_case_counts.xlsx\n",
      "  [11:54:10] INFO: ‚ùì Could not identify cohort for state_specific/alabama/office_of_juvenile_justice_and_delinquency_prevention/2021/juvenile_court_case_count.xlsx\n",
      "  [11:54:10] INFO: ‚ùì Could not identify cohort for state_specific/alabama/office_of_refugee_resettlement/2015-2025/unaccompanied_children_released.xlsx\n",
      "  [11:54:10] INFO: üìÑ Checking 2622 CSV files for missing cohorts...\n",
      "  [11:54:10] INFO: üéØ Loading cohort 228 from CSV: ndacan/nytd/outcomes/cleaned/nytd_outcomes_228_cleaned.csv\n",
      "Loaded 52199 rows, 50 columns from ndacan/nytd/outcomes/cleaned/nytd_outcomes_228_cleaned.csv\n",
      "  [11:54:13] INFO: üéØ Loading cohort 266 from CSV: ndacan/nytd/outcomes/cleaned/nytd_outcomes_266_cleaned.csv\n",
      "Loaded 0 rows, 52 columns from ndacan/nytd/outcomes/cleaned/nytd_outcomes_266_cleaned.csv\n",
      "  [11:54:13] INFO: üéØ Loading cohort 297 from CSV: ndacan/nytd/outcomes/cleaned/nytd_outcomes_297_cleaned.csv\n",
      "Loaded 0 rows, 52 columns from ndacan/nytd/outcomes/cleaned/nytd_outcomes_297_cleaned.csv\n",
      "  [11:54:14] INFO: ‚úÖ Successfully loaded 4 cohorts from S3:\n",
      "  [11:54:14] INFO:    ‚Ä¢ Cohort 202 (FY2011): 12,572 rows √ó 19 cols\n",
      "  [11:54:14] INFO:    ‚Ä¢ Cohort 228 (FY2014): 52,199 rows √ó 50 cols\n",
      "  [11:54:14] INFO:    ‚Ä¢ Cohort 266 (FY2017): 0 rows √ó 52 cols\n",
      "  [11:54:14] INFO:    ‚Ä¢ Cohort 297 (FY2020): 0 rows √ó 52 cols\n",
      "  [11:54:14] INFO: üéØ Step 2: Standardizing datasets...\n",
      "  [11:54:14] INFO: üîß Standardizing S3 datasets...\n",
      "  [11:54:14] INFO:    Found 74 unique columns across all datasets\n",
      "  [11:54:14] INFO: ‚úÖ Standardized 202: (12572, 79)\n",
      "  [11:54:55] INFO: ‚úÖ Standardized 228: (52199, 79)\n",
      "  [11:54:55] INFO: ‚úÖ Standardized 266: (0, 79)\n",
      "  [11:54:55] INFO: ‚úÖ Standardized 297: (0, 79)\n",
      "  [11:54:55] INFO: üéØ Step 3: Combining cohorts...\n",
      "  [11:54:55] INFO: üîó Combining cohorts into integrated dataset...\n",
      "  [11:54:56] INFO: ‚úÖ Integrated dataset: 64,771 rows √ó 80 cols\n",
      "  [11:54:56] INFO:    Total individuals: 23,615\n",
      "  [11:54:56] INFO:    Cohorts: 2\n",
      "  [11:54:56] INFO:    Waves: [np.float64(1.0), np.float64(2.0), np.float64(3.0)]\n",
      "  [11:54:56] INFO: üéØ Step 4: Converting to wide format...\n",
      "  [11:54:56] INFO: üîÑ Converting to wide format...\n",
      "  [11:54:56] INFO:    Pivoting 24 outcome variables\n",
      "  [11:54:56] INFO:    Keeping 19 demographic variables\n",
      "  [11:54:56] INFO: ‚úÖ Wide format: 23,614 rows √ó 97 cols\n",
      "  [11:54:56] INFO: üéØ Step 5: Running quality checks...\n",
      "  [11:54:56] INFO: üîç Running quality assurance checks...\n",
      "  [11:54:56] INFO: ‚úÖ Quality assurance complete\n",
      "  [11:54:56] INFO: üéØ Step 6: Saving results...\n",
      "  [11:54:56] INFO: üíæ Saving S3 integration results...\n",
      "  [11:55:01] INFO: ‚úÖ Saved: NYTD_S3_Integrated_Long_Format.csv (64,771 rows)\n",
      "  [11:55:04] INFO: ‚úÖ Saved: NYTD_S3_Analytical_Wide_Format.csv (23,614 rows)\n",
      "  [11:55:04] INFO: ‚úÖ Saved: NYTD_S3_Dataset_Summary.csv\n",
      "  [11:55:04] INFO: ‚úÖ Saved: NYTD_S3_Integration_Documentation.json\n",
      "\n",
      "üéâ S3 INTEGRATION COMPLETE!\n",
      "‚è±Ô∏è  Duration: 1.3 minutes\n",
      "üìä Analytical Dataset: 23,614 individuals √ó 97 variables\n",
      "üìÅ Files Created:\n",
      "   ‚Ä¢ NYTD_S3_Integrated_Long_Format.csv\n",
      "   ‚Ä¢ NYTD_S3_Analytical_Wide_Format.csv\n",
      "   ‚Ä¢ NYTD_S3_Dataset_Summary.csv\n",
      "   ‚Ä¢ NYTD_S3_Integration_Documentation.json\n",
      "\n",
      "============================================================\n",
      "üìä S3 INTEGRATION RESULTS PREVIEW\n",
      "============================================================\n",
      "\n",
      "üéØ ANALYTICAL DATASET (Wide Format)\n",
      "üìÅ Source: S3 bdc-public-curated\n",
      "üìä Shape: 23,614 rows √ó 97 columns\n",
      "üë• Each row = 1 individual across all survey waves\n",
      "\n",
      "üìã FIRST 5 RECORDS:\n",
      "        person_id cohort_id  cohort_year cohort_label data_source  \\\n",
      "0  AK450292595028       228         2014         FY14          S3   \n",
      "1  AK450564097743       228         2014         FY14          S3   \n",
      "2  AK450862600729       228         2014         FY14          S3   \n",
      "3  AK450876000863       228         2014         FY14          S3   \n",
      "4  AK451045802561       228         2014         FY14          S3   \n",
      "\n",
      "  original_cohort          StFCID  StFIPS  St      RecNumbr  ... Incarc_3  \\\n",
      "0             228  AK450292595028     2.0  AK  450292595028  ...      NaN   \n",
      "1             228  AK450564097743     2.0  AK  450564097743  ...      NaN   \n",
      "2             228  AK450862600729     2.0  AK  450862600729  ...      0.0   \n",
      "3             228  AK450876000863     2.0  AK  450876000863  ...      0.0   \n",
      "4             228  AK451045802561     2.0  AK  451045802561  ...      0.0   \n",
      "\n",
      "  Children_3 Marriage_3 Medicaid_3 OthrHlthIn_3 MedicalIn_3 MentlHlthIn_3  \\\n",
      "0        NaN        NaN        NaN          NaN         NaN           NaN   \n",
      "1        NaN        NaN        NaN          NaN         NaN           NaN   \n",
      "2        1.0        0.0        1.0          0.0        88.0          88.0   \n",
      "3        0.0       88.0        1.0          3.0        88.0          88.0   \n",
      "4        0.0       88.0        1.0          0.0        88.0          88.0   \n",
      "\n",
      "  PrescripIn_3 OutcmRpt_3 OutcmFCS_3  \n",
      "0          NaN        NaN        NaN  \n",
      "1          NaN        NaN        NaN  \n",
      "2         88.0        1.0        0.0  \n",
      "3         88.0        1.0        0.0  \n",
      "4         88.0        1.0        0.0  \n",
      "\n",
      "[5 rows x 97 columns]\n",
      "\n",
      "üè∑Ô∏è COLUMN CATEGORIES:\n",
      "\n",
      "üë• Demographic Variables (25):\n",
      "    ['person_id', 'cohort_id', 'cohort_year', 'cohort_label', 'data_source', 'original_cohort', 'StFCID', 'StFIPS', 'St', 'RecNumbr', 'DOB', 'Sex']\n",
      "   ... and 13 more\n",
      "\n",
      "üìä Wave 1 Outcome Variables (24):\n",
      "    ['CurrFTE_1', 'CurrPTE_1', 'EmplySklls_1', 'SocSecrty_1', 'EducAid_1', 'PubFinAs_1', 'PubFoodAs_1', 'PubHousAs_1', 'OthrFinAs_1', 'HighEdCert_1']\n",
      "   ... and 14 more\n",
      "\n",
      "üìä Wave 2 Outcome Variables (24):\n",
      "    ['CurrFTE_2', 'CurrPTE_2', 'EmplySklls_2', 'SocSecrty_2', 'EducAid_2', 'PubFinAs_2', 'PubFoodAs_2', 'PubHousAs_2', 'OthrFinAs_2', 'HighEdCert_2']\n",
      "   ... and 14 more\n",
      "\n",
      "üìä Wave 3 Outcome Variables (24):\n",
      "    ['CurrFTE_3', 'CurrPTE_3', 'EmplySklls_3', 'SocSecrty_3', 'EducAid_3', 'PubFinAs_3', 'PubFoodAs_3', 'PubHousAs_3', 'OthrFinAs_3', 'HighEdCert_3']\n",
      "   ... and 14 more\n",
      "\n",
      "üìà SAMPLE WAVE COMPARISON:\n",
      "First 3 individuals showing wave progression:\n",
      "        person_id cohort_id  Homeless_1  Homeless_2  Homeless_3  CurrFTE_1  \\\n",
      "0  AK450292595028       228         0.0         NaN         NaN        0.0   \n",
      "1  AK450564097743       228         0.0         NaN         NaN        0.0   \n",
      "2  AK450862600729       228         1.0         0.0         0.0        0.0   \n",
      "\n",
      "   CurrFTE_2  CurrFTE_3  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        0.0        0.0  \n",
      "\n",
      "üìä COHORT DISTRIBUTION:\n",
      "   ‚Ä¢ Cohort 228 (FY2014): 23,614 individuals\n",
      "\n",
      "üìÅ FILES CREATED:\n",
      "   ‚Ä¢ NYTD_S3_Integrated_Long_Format.csv (18,371,487 bytes)\n",
      "   ‚Ä¢ NYTD_S3_Analytical_Wide_Format.csv (8,925,067 bytes)\n",
      "   ‚Ä¢ NYTD_S3_Dataset_Summary.csv (174 bytes)\n",
      "   ‚Ä¢ NYTD_S3_Integration_Documentation.json (13,579 bytes)\n",
      "\n",
      "üéâ S3 INTEGRATION SUCCESSFUL!\n",
      "üìÑ Main analytical file: NYTD_S3_Analytical_Wide_Format.csv\n",
      "üìö Documentation: NYTD_S3_Integration_Documentation.json\n",
      "\n",
      "‚úÖ QUALITY CHECKS:\n",
      "   ‚Ä¢ Wide format has one record per person: True\n",
      "   ‚Ä¢ Total individuals: 23,614\n",
      "   ‚Ä¢ Total observations: 64,771\n",
      "\n",
      "üìÇ LOCAL FILES CREATED:\n",
      "   ‚úÖ NYTD_S3_Analytical_Wide_Format.csv (8,925,067 bytes)\n",
      "   ‚úÖ NYTD_S3_Dataset_Summary.csv (174 bytes)\n",
      "   ‚úÖ NYTD_S3_Integrated_Long_Format.csv (18,371,487 bytes)\n",
      "   ‚úÖ NYTD_S3_Integration_Documentation.json (13,579 bytes)\n"
     ]
    }
   ],
   "source": [
    "# S3 Cell 8: Usage and Data Preview\n",
    "\"\"\"\n",
    "NYTD S3 Data Integration - Cell 8: Execute Integration and Preview Results\n",
    "Run this cell AFTER running all previous cells (1-7)\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the S3 integrator\n",
    "print(\"üöÄ Initializing NYTD S3 Integration...\")\n",
    "s3_integrator = NYTDS3Integrator()\n",
    "\n",
    "# Run the complete S3 integration\n",
    "print(\"\\nüîÑ Starting S3 integration process...\")\n",
    "results = s3_integrator.run_s3_integration()\n",
    "\n",
    "# Preview the results\n",
    "if results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä S3 INTEGRATION RESULTS PREVIEW\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    wide_df = results['wide_format']\n",
    "    long_df = results['long_format']\n",
    "    \n",
    "    print(f\"\\nüéØ ANALYTICAL DATASET (Wide Format)\")\n",
    "    print(f\"üìÅ Source: S3 bdc-public-curated\")\n",
    "    print(f\"üìä Shape: {wide_df.shape[0]:,} rows √ó {wide_df.shape[1]} columns\")\n",
    "    print(f\"üë• Each row = 1 individual across all survey waves\")\n",
    "    \n",
    "    print(f\"\\nüìã FIRST 5 RECORDS:\")\n",
    "    print(wide_df.head())\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è COLUMN CATEGORIES:\")\n",
    "    \n",
    "    # Show demographic columns\n",
    "    demo_cols = [col for col in wide_df.columns \n",
    "                 if not any(col.endswith(f'_{w}') for w in [1,2,3])]\n",
    "    print(f\"\\nüë• Demographic Variables ({len(demo_cols)}):\")\n",
    "    print(\"   \", demo_cols[:12])\n",
    "    if len(demo_cols) > 12:\n",
    "        print(f\"   ... and {len(demo_cols)-12} more\")\n",
    "    \n",
    "    # Show wave-specific columns\n",
    "    for wave in [1, 2, 3]:\n",
    "        wave_cols = [col for col in wide_df.columns if col.endswith(f'_{wave}')]\n",
    "        if wave_cols:\n",
    "            print(f\"\\nüìä Wave {wave} Outcome Variables ({len(wave_cols)}):\")\n",
    "            print(\"   \", wave_cols[:10])\n",
    "            if len(wave_cols) > 10:\n",
    "                print(f\"   ... and {len(wave_cols)-10} more\")\n",
    "    \n",
    "    print(f\"\\nüìà SAMPLE WAVE COMPARISON:\")\n",
    "    sample_vars = ['person_id', 'cohort_id', 'Homeless_1', 'Homeless_2', 'Homeless_3', \n",
    "                   'CurrFTE_1', 'CurrFTE_2', 'CurrFTE_3']\n",
    "    available_vars = [var for var in sample_vars if var in wide_df.columns]\n",
    "    if available_vars:\n",
    "        print(\"First 3 individuals showing wave progression:\")\n",
    "        print(wide_df[available_vars].head(3))\n",
    "    \n",
    "    print(f\"\\nüìä COHORT DISTRIBUTION:\")\n",
    "    if 'cohort_id' in wide_df.columns:\n",
    "        cohort_counts = wide_df['cohort_id'].value_counts().sort_index()\n",
    "        for cohort, count in cohort_counts.items():\n",
    "            year = s3_integrator.cohorts.get(cohort, {}).get('year', 'Unknown')\n",
    "            print(f\"   ‚Ä¢ Cohort {cohort} (FY{year}): {count:,} individuals\")\n",
    "    \n",
    "    print(f\"\\nüìÅ FILES CREATED:\")\n",
    "    files = results.get('files', {})\n",
    "    for file_type, filename in files.items():\n",
    "        if filename:\n",
    "            size = os.path.getsize(filename) if os.path.exists(filename) else 0\n",
    "            print(f\"   ‚Ä¢ {filename} ({size:,} bytes)\")\n",
    "    \n",
    "    print(f\"\\nüéâ S3 INTEGRATION SUCCESSFUL!\")\n",
    "    print(f\"üìÑ Main analytical file: {files.get('wide_format', 'Not saved')}\")\n",
    "    print(f\"üìö Documentation: {files.get('documentation', 'Not saved')}\")\n",
    "    \n",
    "    # Quick data quality summary\n",
    "    qa = results.get('quality_assurance', {})\n",
    "    if qa:\n",
    "        print(f\"\\n‚úÖ QUALITY CHECKS:\")\n",
    "        record_counts = qa.get('record_counts', {})\n",
    "        print(f\"   ‚Ä¢ Wide format has one record per person: {qa.get('data_integrity', {}).get('one_record_per_person', 'Unknown')}\")\n",
    "        print(f\"   ‚Ä¢ Total individuals: {record_counts.get('unique_individuals_wide', 'Unknown'):,}\")\n",
    "        print(f\"   ‚Ä¢ Total observations: {record_counts.get('long_format_records', 'Unknown'):,}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå S3 Integration failed!\")\n",
    "    print(\"üìã Check the error messages above for troubleshooting.\")\n",
    "    print(\"\\nüîß Troubleshooting tips:\")\n",
    "    print(\"   1. Verify AWS credentials are configured\")\n",
    "    print(\"   2. Check S3 bucket permissions\")\n",
    "    print(\"   3. Confirm NYTD files exist in the specified buckets\")\n",
    "    print(\"   4. Run individual cells to identify the specific issue\")\n",
    "\n",
    "# Optional: Quick file verification\n",
    "print(f\"\\nüìÇ LOCAL FILES CREATED:\")\n",
    "local_files = [f for f in os.listdir('.') if f.startswith('NYTD_S3_')]\n",
    "for file in local_files:\n",
    "    size = os.path.getsize(file)\n",
    "    print(f\"   ‚úÖ {file} ({size:,} bytes)\")\n",
    "\n",
    "if not local_files:\n",
    "    print(\"   ‚ùå No S3 integration files found locally\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
